{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae73f39-3b8d-4beb-b1fb-7582a5c41df9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735c2ce-4834-4a43-af2f-adf7f6a5fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "from scipy.stats import pearsonr\n",
    "import copy\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.interpolate import splprep, splev\n",
    "from scipy.signal import convolve2d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functools import reduce\n",
    "import operator\n",
    "import time\n",
    "import numbers\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.optimize import curve_fit\n",
    "import math\n",
    "from scipy._lib._util import _contains_nan\n",
    "import scipy.special\n",
    "import random\n",
    "import warnings\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import norm\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397d70c-41fb-4d6d-a6c9-64372c97a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adds to the trial info dataset the time of the reach (also adds the target position)\n",
    "'''\n",
    "def add_reachTime(dataset):\n",
    "    actual_dataset = dataset.make_trial_data(start_field='move_onset_time')\n",
    "    \n",
    "    cursor_pos = np.asarray(actual_dataset['cursor_pos'])\n",
    "    trials_id = np.asarray(actual_dataset['trial_id'])\n",
    "    mask = np.concatenate(([True], trials_id[1:] != trials_id[:-1]))\n",
    "    targets = np.asarray(dataset.trial_info['target_pos'])\n",
    "    active_id = np.asarray(dataset.trial_info['active_target'])\n",
    "    target_pos = [target[active_id[i]] for i, target in enumerate(targets)]\n",
    "    target_pos = np.array(target_pos, dtype='int64')\n",
    "    started_time = np.array(actual_dataset['align_time'])\n",
    "    move_time = np.array(dataset.trial_info['move_onset_time'])\n",
    "    \n",
    "    diffs = []\n",
    "    for i in range(len(cursor_pos)):\n",
    "        diffs += [np.sqrt(np.power(target_pos[trials_id[i]][0] - cursor_pos[i][0], 2)+\n",
    "                          np.power(target_pos[trials_id[i]][1] - cursor_pos[i][1], 2))]\n",
    "    \n",
    "    diffs = np.split(diffs, np.where(mask)[0])[1:]\n",
    "    started_time = np.split(started_time, np.where(mask)[0])[1:]\n",
    "    reach_time = []\n",
    "    \n",
    "    for i in range(len(target_pos)):\n",
    "        reach_time += [started_time[i][np.argmin(diffs[i])]+move_time[i]]\n",
    "        \n",
    "    dataset.trial_info['reach_time'] = reach_time\n",
    "    dataset.trial_info['active_pos_x'] = target_pos[:,0]\n",
    "    dataset.trial_info['active_pos_y'] = target_pos[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678851df-cd97-4fa6-b7dc-cad00c77199f",
   "metadata": {},
   "source": [
    "Standard dataset path: /Users/guilhermec.f/000128/sub-Jenkins\n",
    "<br>\n",
    "It takes 93-133 sec to run\n",
    "\n",
    "Large dataset path: /Users/guilhermec.f/000138/sub-Jenkins\n",
    "<br>\n",
    "It takes 6 sec to run\n",
    "\n",
    "Medium dataset path: /Users/guilhermec.f/000139/sub-Jenkins\n",
    "<br>\n",
    "It takes 3 sec to run\n",
    "\n",
    "Small dataset path: /Users/guilhermec.f/000140/sub-Jenkins\n",
    "<br>\n",
    "It takes 1 sec to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012e85a-7856-4583-ad64-61180c4fd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardPath = 'your path'\n",
    "standardDS = NWBDataset(standardPath, \"*train\", split_heldout=False)\n",
    "standardDS.resample(5) #resizes bins to 5ms (only once needed)\n",
    "standard_conds = standardDS.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "standard_neurons = standardDS.data['spikes'].columns.tolist() #Gets the name of all neurons\n",
    "add_reachTime(standardDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2967c-e219-4fcd-b689-5bb51da3ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "largePath = 'your path'\n",
    "largeDS = NWBDataset(largePath, \"*train\", split_heldout=False)\n",
    "largeDS.resample(5) #resizes bins to 5ms (only once needed)\n",
    "large_conds = largeDS.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "large_neurons = largeDS.data['spikes'].columns.tolist() #Gets the name of all neurons\n",
    "add_reachTime(largeDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0af0df-6416-4d88-baaf-a30fb8c70ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediumPath = 'your path'\n",
    "mediumDS = NWBDataset(mediumPath, \"*train\", split_heldout=False)\n",
    "mediumDS.resample(5)\n",
    "medium_conds = mediumDS.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "medium_neurons = mediumDS.data['spikes'].columns.tolist() #Gets the name of all neurons\n",
    "add_reachTime(mediumDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4503de-79f9-47cc-88d2-ad8c0a2102d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallPath = 'your path'\n",
    "smallDS = NWBDataset(smallPath, \"*train\", split_heldout=False)\n",
    "smallDS.resample(5) #resizes bins to 5ms (only once needed)\n",
    "conds = smallDS.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "small_neurons = smallDS.data['spikes'].columns.tolist() #Gets the name of all neurons\n",
    "add_reachTime(smallDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c352e-5b5d-41d0-8260-1ec303f11d6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions from the Demo Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ebbdc-9be4-40e6-a6fe-bb11ba79d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the PSTH with STDG for a given condition for a given neuron\n",
    "def plot_givenPSTHs(dataset, condition, neuron = -1, seed=2468):\n",
    "    \n",
    "    # Seed generator for consistent plots\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Check if 'spikes_smth_50' column already exists\n",
    "    if 'spikes_smth_50' not in dataset.data.columns:\n",
    "        # Smooth spikes with 50 ms std Gaussian\n",
    "        dataset.smooth_spk(50, name='smth_50')\n",
    "    \n",
    "    if(neuron == -1):\n",
    "        neur_num = np.random.choice(dataset.data.spikes.columns)\n",
    "    else:\n",
    "        neur_num = neuron\n",
    "\n",
    "    print(neur_num)\n",
    "    \n",
    "    # Find unique conditions\n",
    "    conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "    print(\"Number of conditions\", len(conds))\n",
    "    \n",
    "    if(type(condition) == int):\n",
    "        n_conds = condition\n",
    "        test = np.random.choice(len(conds), size=n_conds, replace=False)\n",
    "\n",
    "    # Plot random subset of conditions\n",
    "        for i in test:\n",
    "            if(type(condition) == int): cond = conds[i]\n",
    "            else: cond = i\n",
    "            # Find trials in condition\n",
    "            mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
    "            # Extract trial data\n",
    "            trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
    "            # Average hand position across trials\n",
    "            psth = trial_data.groupby('align_time')[[('spikes_smth_50', neur_num)]].mean().to_numpy() / dataset.bin_width * 1000\n",
    "            # Color PSTHs by reach angle\n",
    "            active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
    "            reach_angle = np.arctan2(*active_target[::-1])\n",
    "            # Plot reach\n",
    "            plt.plot(np.arange(-50, 450, dataset.bin_width), psth, label=cond, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
    "    else:\n",
    "        # Plot random subset of conditions\n",
    "        for i in condition:\n",
    "            cond = i\n",
    "            # Find trials in condition\n",
    "            mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
    "            # Extract trial data\n",
    "            trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
    "            # Average hand position across trials\n",
    "            psth = trial_data.groupby('align_time')[[('spikes_smth_50', neur_num)]].mean().to_numpy() / dataset.bin_width * 1000\n",
    "            # Color PSTHs by reach angle\n",
    "            active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
    "            reach_angle = np.arctan2(*active_target[::-1])\n",
    "            # Plot reach\n",
    "            plt.plot(np.arange(-50, 450, dataset.bin_width), psth, label=cond, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
    "    \n",
    "    # Add labels\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel('Time after movement onset (ms)')\n",
    "    plt.ylabel('Firing rate (spk/s)')\n",
    "    plt.title(f'Neur {neur_num} PSTH')\n",
    "    plt.legend(title='condition', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c373642-77d1-4520-a7f7-cb77a874a753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots path for all conditions\n",
    "def plot_path(dataset, conditions=-1):\n",
    "    label_check = False\n",
    "    if conditions == -1:\n",
    "        conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "    else:\n",
    "        conds = conditions\n",
    "        label_check = True\n",
    "    \n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "    \n",
    "    for i, cond in enumerate(conds):\n",
    "        mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
    "        trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
    "        traj = trial_data.groupby('align_time')[[('hand_pos', 'x'), ('hand_pos', 'y')]].mean().to_numpy()\n",
    "        active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
    "        reach_angle = np.arctan2(*active_target[::-1])\n",
    "        \n",
    "        if(label_check): ax.plot(traj[:, 0], traj[:, 1], linewidth=0.7, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5), label=f'{conds[i]}')\n",
    "        else: ax.plot(traj[:, 0], traj[:, 1], linewidth=0.7, color=plt.cm.hsv(reach_angle / (2*np.pi) + 0.5)) \n",
    "    \n",
    "    if(label_check): plt.legend(title='Condition', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df5bc2-5a29-494a-9663-5bf9fdd44ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the data to a ridge regression, compares the results with R2 and spits out a DS for analysing the predictions\n",
    "def kinematic_decoding_eval (dataset):\n",
    "    # Extract neural data and lagged hand velocity\n",
    "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-130, 370))\n",
    "    lagged_trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450))\n",
    "    rates = trial_data.spikes_smth_50.to_numpy()\n",
    "    vel = lagged_trial_data.hand_vel.to_numpy()\n",
    "    \n",
    "    # Fit and evaluate decoder\n",
    "    gscv = GridSearchCV(Ridge(), {'alpha': np.logspace(-4, 0, 5)})\n",
    "    gscv.fit(rates, vel)\n",
    "    pred_vel = gscv.predict(rates)\n",
    "    print(f\"Decoding R2: {gscv.best_score_}\")\n",
    "    print(\"Best alpha value:\", gscv.best_params_['alpha'])\n",
    "\n",
    "    #Saves the extracted predictions in a new DS\n",
    "    predictedDS = copy.deepcopy(dataset)\n",
    "    \n",
    "    # Merge predictions back to continuous data\n",
    "    pred_vel_df = pd.DataFrame(pred_vel, index=lagged_trial_data.clock_time, columns=pd.MultiIndex.from_tuples([('pred_vel', 'x'), ('pred_vel', 'y')]))\n",
    "    predictedDS.data = pd.concat([predictedDS.data, pred_vel_df], axis=1)\n",
    "\n",
    "    return predictedDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe0794-71e6-413b-bb4c-ca096902b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted vs true kinematics\n",
    "def plot_comparison(dataset, condition):\n",
    "    conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "    cond = conds[condition]\n",
    "    \n",
    "    # Find trials in condition and extract data\n",
    "    mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
    "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
    "    \n",
    "    # Initialize figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 4))\n",
    "    t = np.arange(-50, 450, dataset.bin_width)\n",
    "    \n",
    "    # Loop through trials in condition\n",
    "    for _, trial in trial_data.groupby('trial_id'):\n",
    "        # True and predicted x velocity\n",
    "        axs[0][0].plot(t, trial.hand_vel.x, linewidth=0.7, color='black')\n",
    "        axs[1][0].plot(t, trial.pred_vel.x, linewidth=0.7, color='blue')\n",
    "        # True and predicted y velocity\n",
    "        axs[0][1].plot(t, trial.hand_vel.y, linewidth=0.7, color='black')\n",
    "        axs[1][1].plot(t, trial.pred_vel.y, linewidth=0.7, color='blue')\n",
    "        # True and predicted trajectories\n",
    "        true_traj = np.cumsum(trial.hand_vel.to_numpy(), axis=0) * dataset.bin_width / 1000\n",
    "        pred_traj = np.cumsum(trial.pred_vel.to_numpy(), axis=0) * dataset.bin_width / 1000\n",
    "        axs[0][2].plot(true_traj[:, 0], true_traj[:, 1], linewidth=0.7, color='black')\n",
    "        axs[1][2].plot(pred_traj[:, 0], pred_traj[:, 1], linewidth=0.7, color='blue')\n",
    "    \n",
    "    # Set up shared axes\n",
    "    for i in range(2):\n",
    "        axs[i][0].set_xlim(-50, 450)\n",
    "        axs[i][1].set_xlim(-50, 450)\n",
    "        axs[i][2].set_xlim(-180, 180)\n",
    "        axs[i][2].set_ylim(-130, 130)\n",
    "    \n",
    "    # Add labels\n",
    "    axs[0][0].set_title('X velocity (mm/s)')\n",
    "    axs[0][1].set_title('Y velocity (mm/s)')\n",
    "    axs[0][2].set_title('Reach trajectory')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0bdec-b9ef-4ddc-88cc-842b7fc3fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neural trajectories for subset of conditions\n",
    "def plot_neuralTrajectories(dataset, seed=2021):\n",
    "    # Seed generator for consistent plots\n",
    "    np.random.seed(seed)\n",
    "    n_conds = 27 # number of conditions to plot\n",
    "    \n",
    "    # Get unique conditions\n",
    "    conds = dataset.trial_info.set_index(['trial_type', 'trial_version']).index.unique().tolist()\n",
    "    \n",
    "    # Loop through conditions\n",
    "    rates = []\n",
    "    colors = []\n",
    "    for i in np.random.choice(len(conds), n_conds):\n",
    "        cond = conds[i]\n",
    "        # Find trials in condition\n",
    "        mask = np.all(dataset.trial_info[['trial_type', 'trial_version']] == cond, axis=1)\n",
    "        # Extract trial data\n",
    "        trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450), ignored_trials=(~mask))\n",
    "        # Append averaged smoothed spikes for condition\n",
    "        rates.append(trial_data.groupby('align_time')[trial_data[['spikes_smth_50']].columns].mean().to_numpy())\n",
    "        # Append reach angle-based color for condition\n",
    "        active_target = dataset.trial_info[mask].target_pos.iloc[0][dataset.trial_info[mask].active_target.iloc[0]]\n",
    "        reach_angle = np.arctan2(*active_target[::-1])\n",
    "        colors.append(plt.cm.hsv(reach_angle / (2*np.pi) + 0.5))\n",
    "    \n",
    "    # Stack data and apply PCA\n",
    "    rate_stack = np.vstack(rates)\n",
    "    rate_scaled = StandardScaler().fit_transform(rate_stack)\n",
    "    pca = PCA(n_components=3)\n",
    "    traj_stack = pca.fit_transform(rate_scaled)\n",
    "    traj_arr = traj_stack.reshape((n_conds, len(rates[0]), -1))\n",
    "    \n",
    "    # Loop through trajectories and plot\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for traj, col in zip(traj_arr, colors):\n",
    "        ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=col)\n",
    "        ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2], color=col) \n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479009f-65db-4f6b-9517-503dc67d7ff8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Final Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca6f6b-8e2b-43c1-a128-954f893aa612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Makes Datasets per Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf6205-5d45-474c-ba80-76fa06552ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the dataset for only one trial\n",
    "'''\n",
    "def trial_datasetMaker(dataset, trials):\n",
    "    if(len(trials) == 1):\n",
    "        trials_possible = trials_present(dataset)\n",
    "        check_mask = np.isin(trials, trials_possible)[0]\n",
    "    \n",
    "        if(check_mask):\n",
    "            columns = dataset.columns\n",
    "            matrix = np.asarray(dataset)\n",
    "            \n",
    "            trial_ids = np.array(dataset['trial_id'])\n",
    "            mask = trial_ids == trials[0]\n",
    "            matrix = matrix[mask]\n",
    "            trial_dataset = pd.DataFrame(matrix, columns=columns)\n",
    "            return trial_dataset\n",
    "        else:\n",
    "            print('Trial ', trials[0], ' not in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67abb5-8317-449c-86c7-d7cb19567657",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04368c49-0166-4ba0-a3ae-40905dcab50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divides the original training set into train/val/test sets with a fixed seed for reproducibility.\n",
    "'''\n",
    "def generate_sets(dataset, ratio, folds, unique=True, start_time='move_onset_time', end_time='reach_time', seed=2003):\n",
    "    \n",
    "    def generate_subset(dataset, subset_name, start_time='move_onset_time', end_time='reach_time'):\n",
    "        mask = np.all(dataset.trial_info[['split']] == subset_name, axis=1)\n",
    "        return dataset.make_trial_data(start_field=start_time, end_field=end_time, ignored_trials=~mask)\n",
    "    \n",
    "    # Set the random seed if provided\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    \n",
    "    # Data Unpacking\n",
    "    ref_dataset = generate_subset(standardDS, 'train')\n",
    "    \n",
    "    trials = trials_present(dataset.trial_info)\n",
    "    selectable_trials = trials_present(ref_dataset)\n",
    "    full_train_trials = selectable_trials\n",
    "    val_length = int(len(trials_present(ref_dataset)) * 0.1)\n",
    "    \n",
    "    # Train and Val set making\n",
    "    train_sets = []\n",
    "    val_sets = []\n",
    "    val_selectable_trials = selectable_trials\n",
    "    for k in range(folds):\n",
    "        val_trials = np.sort(np.random.choice(selectable_trials, val_length, replace=False))\n",
    "        train_trials = np.setdiff1d(full_train_trials, val_trials)\n",
    "        \n",
    "        val_mask = np.isin(trials, val_trials)\n",
    "        train_mask = np.isin(trials, train_trials)\n",
    "        val_dataset = dataset.make_trial_data(start_field=start_time, end_field=end_time, ignored_trials=(~val_mask))\n",
    "        train_dataset = dataset.make_trial_data(start_field=start_time, end_field=end_time, ignored_trials=(~train_mask))\n",
    "        if unique:\n",
    "            selectable_trials = train_trials\n",
    "        val_sets.append(val_dataset)\n",
    "        train_sets.append(train_dataset)\n",
    "    \n",
    "    # Test set making\n",
    "    test_dataset = generate_subset(dataset, 'val')\n",
    "\n",
    "    return train_sets, val_sets, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3d945-5aa4-491f-8257-d9fa4fb463bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gets the Firing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4301d-bd9b-4325-81c2-e78c5390db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the firing rate of a dataset's spikes with a specific window size ------ OTIMIZATION NEEDED\n",
    "'''\n",
    "def firing_rate(dataset, window_size, bin_time=5, panda=True):\n",
    "    size = len(dataset)\n",
    "    neurons_names = dataset['spikes'].columns\n",
    "    nr_neurons = len(neurons_names)\n",
    "    spikes_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    firing_matrix = []\n",
    "    trials = np.unique(trial_ids)\n",
    "    kernel = (np.ones((window_size, 1)) / window_size)\n",
    "    \n",
    "    for trial in trials:\n",
    "        mask = trial_ids == trial\n",
    "        matrix = spikes_data[mask]\n",
    "        convolved = (convolve2d(matrix, kernel, mode='same')/5)*1000\n",
    "        firing_matrix.append(convolved)\n",
    "        \n",
    "    firing_matrix = np.concatenate(firing_matrix, axis=0)\n",
    "    if(panda): \n",
    "        #dataset['spikes'] = pd.DataFrame(firing_matrix, columns=np.asarray(neurons_names, dtype='int'))\n",
    "        dataset['spikes'] = firing_matrix\n",
    "    else: return firing_matrix, trial_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7004c-192a-45fc-8b7c-03d1f5b81103",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gets the Trials Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3900cf6c-f00a-4aae-84e7-f87b8659a3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the trials present in one dataset\n",
    "'''\n",
    "def trials_present(dataset):\n",
    "    trial_dataset = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    return np.unique(trial_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595f2dd-2006-4d84-b352-c12d9b75c38e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plots all Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3385a1-ecb7-441e-abb7-f8f37db94499",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots all trajectories separate (with trial duration in ms)\n",
    "'''\n",
    "def plot_allTrajectoriesS(dataset, original_dataset, nr_cols=4, save=False, save_name='plot.png'):\n",
    "    #Gets the hand positions (trua and pred)\n",
    "    cursor_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    true_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    predicted_x = np.asarray(dataset['pred_X'], dtype='float64')\n",
    "    predicted_y = np.asarray(dataset['pred_Y'], dtype='float64')\n",
    "    true_x = true_pos[:,0]\n",
    "    true_y = true_pos[:,1]\n",
    "    cursor_x = cursor_pos[:,0]\n",
    "    cursor_y = cursor_pos[:,1]\n",
    "    \n",
    "    #Gets the trials and time\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    mask = np.concatenate(([True], trial_ids[1:] != trial_ids[:-1]))\n",
    "    split_indices = np.where(mask)[0]\n",
    "    align_time = np.asarray(dataset['align_time'])\n",
    "    \n",
    "    #Gets the target positions per trial\n",
    "    trials = np.unique(trial_ids)\n",
    "    target_pos = np.asarray(original_dataset.trial_info[['active_pos_x', 'active_pos_y']], dtype='int64')[trials]\n",
    "    barrier_pos = np.asarray(original_dataset.trial_info['barrier_pos'])[trials]\n",
    "    barrier_lengths = np.array([len(inner_array) for inner_array in barrier_pos])\n",
    "    \n",
    "    \n",
    "    #Splits all arrays acording to the trials\n",
    "    predicted_x = np.split(predicted_x, split_indices)[1:]\n",
    "    predicted_y = np.split(predicted_y, split_indices)[1:]\n",
    "    align_time = np.split(align_time, split_indices)[1:]\n",
    "    true_x = np.split(true_x, split_indices)[1:]\n",
    "    true_y = np.split(true_y, split_indices)[1:]\n",
    "    cursor_x = np.split(cursor_x, split_indices)[1:]\n",
    "    cursor_y = np.split(cursor_y, split_indices)[1:]\n",
    "    \n",
    "    nr_trials = len(trials)\n",
    "    #The ploting starts\n",
    "    if(nr_trials%nr_cols == 0): nr_rows = nr_trials//nr_cols\n",
    "    else: nr_rows = nr_trials//nr_cols + 1\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*4, nr_rows*4))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    #Plotting\n",
    "    for i in range(nr_trials):\n",
    "        ax = axs[i]\n",
    "        x_coords = true_x[i]\n",
    "        y_coords = true_y[i]\n",
    "        x_pred = predicted_x[i]\n",
    "        y_pred = predicted_y[i]\n",
    "        x_cursor = cursor_x[i]\n",
    "        y_cursor = cursor_y[i]\n",
    "        target_x = target_pos[i,0]\n",
    "        target_y = target_pos[i,1]\n",
    "        duration = int(align_time[i][-1])/1000000000\n",
    "        distance = round(np.sqrt(np.power(x_pred[-1]-x_coords[-1],2) + np.power(y_pred[-1]-y_coords[-1],2)),3)\n",
    "        barriers = barrier_pos[i]\n",
    "    \n",
    "        #Gets the limit\n",
    "        x_lim = np.max([np.max(x_coords), np.max(x_pred), np.max(x_cursor), np.abs(np.min(x_coords)), np.abs(np.min(x_pred)), \n",
    "                        np.abs(np.min(x_cursor))])\n",
    "        y_lim = np.max([np.max(y_coords), np.max(y_pred), np.max(y_cursor), np.abs(np.min(y_coords)), np.abs(np.min(y_pred)), \n",
    "                        np.abs(np.min(y_cursor))])\n",
    "        lim_value = np.max([x_lim,y_lim])+15\n",
    "        \n",
    "        #Plots the barriers\n",
    "        for j in range(barrier_lengths[i]):\n",
    "            x, y, half_width, half_height = barrier_pos[i][j]\n",
    "            left = x - half_width\n",
    "            bottom = y - half_height\n",
    "            width = 2 * half_width\n",
    "            height = 2 * half_height\n",
    "            ax.add_patch(plt.Rectangle((left, bottom), width, height, facecolor='gray'))\n",
    "    \n",
    "        ax.plot(x_cursor, y_cursor, label='Cursor Path')\n",
    "        ax.plot(x_pred, y_pred, label='Predicted Path')\n",
    "        ax.plot(x_coords, y_coords, label='True Path')\n",
    "        ax.scatter(target_x, target_y, label='Target Position', color='red')\n",
    "        ax.scatter(x_coords[0], y_coords[0], label='Start Point', color='black')\n",
    "        ax.set_title(f'Trial {trials[i]}\\nLasted {duration} seconds\\nDistance Final: {distance}')\n",
    "        ax.set_xlim(-lim_value, lim_value)\n",
    "        ax.set_ylim(-lim_value, lim_value)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Trial Trajectories', y=1, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994174d9-bb1b-4a5e-b767-ac9219924104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plots the Cross Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c528cae-9c94-41fc-b468-01b32c4e4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the correlation for each trial in a dataset in the same plot.\n",
    "\n",
    "Parameteres\n",
    "-----------\n",
    "dataset - pandas.Dataframe\n",
    "    The required dataset with at least two columns named and with the specified metrics\n",
    "trials - array_like\n",
    "    The list of trials to be plotted (must be present in the dataset)\n",
    "metric - array_like of strings\n",
    "    The list with the column name of actual metric and the predicted metric\n",
    "window_size - integer, optional\n",
    "    The size of the visualization window in the x axis (-window_size+1, window_size)\n",
    "correlation - string, optional\n",
    "    Define the correlation metric applied\n",
    "    - 'cross' (default)\n",
    "        Uses the normalized cross correlation\n",
    "    - 'pearson'\n",
    "        Uses the pearson correlation\n",
    "bin_size - float, optional\n",
    "    Specifies what is the size of each bin in ms\n",
    "focus_trial - {None, int, string}, optional\n",
    "    Defines a plotted line that will recieve emphasis\n",
    "    - ''None'' (default)\n",
    "        No trial recieves emphasis\n",
    "    - ''int''\n",
    "        The specified trial recieves emphasis (needs to be in the trials array)\n",
    "    - 'max'\n",
    "        The trial with higher correlation recieves emphasis\n",
    "    - 'min'\n",
    "        The trial with lower correlation recieves emphasis\n",
    "    - 'best'\n",
    "        The trial with the best correlation recieves emphasis (positive or negative)\n",
    "    - 'worst'\n",
    "        The trial with the worst correlation recieves emphasis (closer to 0)\n",
    "    - 'best_lag'\n",
    "        The trial with the lowest lag for the highest correlation recieves emphasis (closer to 0)\n",
    "    - 'worst_lag'\n",
    "        The trial with the highest lag for the highest correlation recieves emphasis (positive or negative)\n",
    "    - 'mean'\n",
    "        A new plot line with the mean and the standard error shaded will recieve emphasis with is maximum correlation \n",
    "        for a given lag pointed out\n",
    "compare_axis - bool, optional\n",
    "    If true the y axis will have the full correlation scope (-1 - margin, 1 + margin) \n",
    "color_template - string, optional\n",
    "    Defines the plt.colormaps used for the plot lines color (uses the full spectrum)\n",
    "save - bool, optional\n",
    "    If true the plot will be saved in the same folder\n",
    "save_name - string, optional\n",
    "    If save true will define the file name and type\n",
    "    \n",
    "'''\n",
    "def plot_ccTogether(dataset, trials, metric, window_size=150, correlation='pearson', bin_size=5, focus_trial=None, compare_axis=True, \n",
    "                    color_template='Greys', save=False, save_name='all_corrs.png'):\n",
    "    #Data Extraction\n",
    "    nr_trials = len(trials)\n",
    "    color_array = np.round(np.linspace(0, 255, nr_trials))\n",
    "    bin_window = window_size//bin_size\n",
    "    dataset_matrix = np.asarray(dataset)\n",
    "    columns = np.array(dataset.columns)\n",
    "    pred_index = next(i for i, col in enumerate(columns) if col[0] == metric[1])\n",
    "    obs_index = next(i for i, col in enumerate(columns) if col[0] == metric[0])\n",
    "    trial_ids = np.array(dataset['trial_id'])\n",
    "    trials_possible = trials_present(dataset)\n",
    "    lags = np.arange(-window_size + 1, window_size, bin_size)\n",
    "    \n",
    "    #Check if any trial is not there\n",
    "    check_mask = np.isin(trials, trials_possible)\n",
    "    if(not check_mask.all()):\n",
    "        print('Some trial not in the dataset')\n",
    "    \n",
    "    #Initializes the arrays\n",
    "    corr_matrix = []\n",
    "    max_matrix = []\n",
    "    max_correlations = []\n",
    "    max_correlationsAbs = []\n",
    "    max_lags = []\n",
    "    max_lagsAbs = []\n",
    "    \n",
    "    #Get the trial_datasets\n",
    "    trial_datasets = np.asarray([dataset_matrix[trial_ids == trial] for trial in trials], dtype='object')\n",
    "    sep_preds = [trial[:, pred_index] for trial in trial_datasets]\n",
    "    sep_obs = [trial[:, obs_index] for trial in trial_datasets]\n",
    "    \n",
    "    #Computes the crosss and lags\n",
    "    for i in range(nr_trials): \n",
    "        obs = np.array(sep_obs[i], dtype='float64')\n",
    "        pred = np.array(sep_preds[i], dtype='float64')\n",
    "        if(correlation == 'pearson'):\n",
    "            cross_corr = cross_pearson(obs, pred)\n",
    "        elif(correlation == 'cross'):\n",
    "            k = np.sqrt((np.sum(np.power(obs,2)))*(np.sum(np.power(pred,2))))\n",
    "            cross_corr = np.correlate(obs, pred, mode='full')/k\n",
    "        cross_corr = cross_corr[len(cross_corr)//2-bin_window:len(cross_corr)//2+bin_window]\n",
    "    \n",
    "        #Stores the information\n",
    "        max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "        max_correlations += [max_corr]\n",
    "        max_correlationsAbs += [np.abs(max_corr)]\n",
    "        max_lags += [max_corr]\n",
    "        max_lagsAbs += [np.abs(max_lag)]\n",
    "        corr_matrix += [[cross_corr, lags]]\n",
    "        max_matrix += [[max_corr, max_lag]]\n",
    "    corr_matrix = np.asarray(corr_matrix, dtype='float64')\n",
    "    \n",
    "    #Defines the focus_trial\n",
    "    if(focus_trial == 'max'):\n",
    "        focus_trial = trials[np.argmax(max_correlations)]\n",
    "    elif(focus_trial == 'min'):\n",
    "        focus_trial = trials[np.argmin(max_correlations)]\n",
    "    elif(focus_trial == 'best'):\n",
    "        focus_trial = trials[np.argmax(max_correlationsAbs)]\n",
    "    elif(focus_trial == 'worst'):\n",
    "        focus_trial = trials[np.argmin(max_correlationsAbs)]\n",
    "    elif(focus_trial == 'best_lag'):\n",
    "        focus_trial = trials[np.argmin(max_lagsAbs)]\n",
    "    elif(focus_trial == 'worst_lag'):\n",
    "        focus_trial = trials[np.argmax(max_lagsAbs)]\n",
    "    \n",
    "    #Plot function\n",
    "    plt.figure(figsize=(15,10))\n",
    "    if(nr_trials > 30): no_label = True\n",
    "    else: no_label = False\n",
    "    \n",
    "    for i in range(nr_trials):\n",
    "        cross_corr = corr_matrix[i][0]\n",
    "        lags = corr_matrix[i][1]\n",
    "        max_corr = max_matrix[i][0]\n",
    "        max_lag = max_matrix[i][1]\n",
    "        if(focus_trial == None):\n",
    "            if(not no_label): plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), \n",
    "                                       label=f'Trial {trials[i]}', alpha=1)\n",
    "            else: plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=1)\n",
    "        elif(focus_trial == trials[i]):\n",
    "            plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), label=f'Trial {trials[i]}', \n",
    "                     alpha=1)\n",
    "            plt.scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "        else:\n",
    "            if(not no_label): plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), \n",
    "                                       label=f'Trial {trials[i]}', alpha=0.25)\n",
    "            else: plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=0.1)\n",
    "\n",
    "    #In case the focus trial is the mean, its the last to be applied\n",
    "    if(focus_trial == 'mean'):\n",
    "        mean_vector = np.mean(corr_matrix, axis=0)\n",
    "        ste_vector = np.std(corr_matrix, axis=0)\n",
    "        ste_values = ste_vector[0] / np.sqrt(nr_trials)\n",
    "        cross_corr = mean_vector[0]\n",
    "        lags = mean_vector[1]\n",
    "        max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "        plt.plot(lags, cross_corr, color='black', label='Mean Correlation', alpha=1)\n",
    "        plt.fill_between(lags, cross_corr-ste_values, cross_corr+ste_values, color='black', label='Standard Error', alpha=0.25)\n",
    "        plt.scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "        plt.scatter(0, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "    #Labeling and Formating\n",
    "    plt.xlabel('Lag (ms)')\n",
    "    if(correlation == 'cross'):\n",
    "        plt.ylabel('Cross-Correlation')\n",
    "        plt.title(f'Cross-Correlation of All Trials ({metric[0]})')\n",
    "    if(correlation == 'pearson'):\n",
    "        plt.ylabel('Pearson\\'s Correlation')\n",
    "        plt.title(f'Pearson\\'s Correlation of All Trials ({metric[0]})')\n",
    "    if(compare_axis): plt.ylim(-1.1,1.1)\n",
    "    plt.grid(True)\n",
    "    if(no_label): plt.legend(loc='lower left')\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a320bf-63f0-4cdb-96e5-51b3d33e2733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gets the Cross Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed53492-ff9d-42a3-a187-bc478e68f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the cross-correlation with pearson correlation values\n",
    "'''\n",
    "def cross_pearson(array1, array2):\n",
    "    array1 = np.asarray(array1, dtype='float64')\n",
    "    array2 = np.asarray(array2, dtype='float64')\n",
    "    \n",
    "    corr_p = np.zeros(len(array2) + len(array1) - 1)\n",
    "    size = len(array2) - 1\n",
    "    #left pad\n",
    "    for index, i in enumerate(range(size, 0, -1)):\n",
    "        x = np.pad(array2,(i), mode='constant')\n",
    "        x = x[i*2:]\n",
    "        corr_p[index] = np.corrcoef(array1,x)[0,1]\n",
    "    #righ pad\n",
    "    for index, i in enumerate(range(0, size+1)):\n",
    "        index = index+size\n",
    "        x = np.pad(array2,(i), mode='constant')\n",
    "        x = x[:len(x)-i*2]\n",
    "        if(i == 0):\n",
    "            if(np.all(x == x[0])):\n",
    "                noise = np.random.uniform(low=-0.0001, high=0.0001, size=x.shape)\n",
    "                x = x + noise\n",
    "        corr_p[index] = np.corrcoef(array1,x)[0,1]\n",
    "    return corr_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33bf65-d34c-40f9-b865-ff8d937e7748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Maximizes Correlation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9753636f-ab65-4162-b486-b25b8314940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximaze_corrParameters(cross_corr, lags):\n",
    "    max_corr_index = np.argmax(cross_corr)\n",
    "    min_corr_index = np.argmin(cross_corr)\n",
    "    max_corr_lag = lags[max_corr_index]\n",
    "    min_corr_lag = lags[min_corr_index]\n",
    "    max_corr = round(cross_corr[max_corr_index],2)\n",
    "    min_corr = round(cross_corr[min_corr_index],2)\n",
    "    if(abs(max_corr)<abs(min_corr)): \n",
    "        max_corr = min_corr\n",
    "        max_corr_lag = min_corr_lag\n",
    "    return max_corr, max_corr_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe039f8f-d75b-45eb-a03b-045ccfa967da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gets the Mean Pearson Correlation of a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9854b-65f3-4d32-914a-a6dacaae3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computes the mean pearson correlation of all trials (and standard deviation) at 0\n",
    "'''\n",
    "def get_pearson(dataset, metric, extra_metrics=False):\n",
    "    #Data Extraction\n",
    "    trial_ids = np.array(dataset['trial_id'])\n",
    "    trials = np.unique(trial_ids)\n",
    "    nr_trials = len(trials)\n",
    "    color_array = np.round(np.linspace(0, 255, nr_trials))\n",
    "    \n",
    "    dataset_matrix = np.asarray(dataset)\n",
    "    columns = np.array(dataset.columns)\n",
    "    pred_index = next(i for i, col in enumerate(columns) if col[0] == metric[1])\n",
    "    obs_index = next(i for i, col in enumerate(columns) if col[0] == metric[0])\n",
    "    \n",
    "    #Get the trial_datasets\n",
    "    trial_datasets = np.asarray([dataset_matrix[trial_ids == trial] for trial in trials], dtype='object')\n",
    "    sep_preds = [trial[:, pred_index] for trial in trial_datasets]\n",
    "    sep_obs = [trial[:, obs_index] for trial in trial_datasets]\n",
    "    \n",
    "    #Initializes the arrays\n",
    "    pearson_matrix = []\n",
    "    \n",
    "    #Computes the crosss and lags\n",
    "    for i in range(nr_trials): \n",
    "        obs = np.array(sep_obs[i], dtype='float64')\n",
    "        pred = np.array(sep_preds[i], dtype='float64')\n",
    "        if(np.all(pred == pred[0])):\n",
    "            noise = np.random.uniform(low=-0.0001, high=0.0001, size=pred.shape)\n",
    "            pred = pred + noise\n",
    "            print(f'-> Noise added to trial {trials[i]}')\n",
    "        pearson = np.corrcoef(obs, pred)[0,1]\n",
    "        if(np.isnan(pearson).any()): \n",
    "            print(f'Trial {trials[i]} as NaN Pearson')\n",
    "            print(f'Observed has Nan? {np.isnan(obs).any()}')\n",
    "            print(obs)\n",
    "            print(f'Predicted has Nan? {np.isnan(pred).any()}')\n",
    "            print(pred)\n",
    "        pearson_matrix.append(pearson)\n",
    "    pearson_matrix = np.asarray(pearson_matrix, dtype='float64')\n",
    "    \n",
    "    #Gets the mean and the standard deviation\n",
    "    r2 = np.power(pearson_matrix, 2)\n",
    "    mean_pearson = np.mean(pearson_matrix)\n",
    "    std_pearson = np.std(pearson_matrix)\n",
    "    mean_r2 = np.mean(r2)\n",
    "    std_r2 = np.std(r2)\n",
    "\n",
    "    if(extra_metrics): return mean_pearson, std_pearson, mean_r2, std_r2, pearson_matrix\n",
    "    else: return mean_pearson, std_pearson, mean_r2, std_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97135d92-96c0-44b2-8028-bd6dd0e05bb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Removes Neurons for Population Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e6696-9dce-48a7-ab49-95806935d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes the neurons from the decoder and prepares the val_dataset for predictions\n",
    "'''\n",
    "def remove_neurons(predicted_matrix, observed_matrix, dataset, neurons, threshold, metric='depth', remove_fromDataset=True):\n",
    "    #Gets the values from the metric used (depth)\n",
    "    if(metric == 'depth'):\n",
    "        max_vector = np.max(predicted_matrix, axis=1)\n",
    "        min_vector = np.min(predicted_matrix, axis=1)\n",
    "        depth_vector = max_vector-min_vector\n",
    "        acepted_neurons = np.where(depth_vector>=threshold, neurons, None)\n",
    "        rejected_neurons = np.where(depth_vector<threshold, neurons, None)\n",
    "    elif(metric == 'difference'):\n",
    "        difference_vector = np.sqrt(np.sum(np.power(predicted_matrix - observed_matrix.T, 2), axis=1))\n",
    "        acepted_neurons = np.where(difference_vector<=threshold, neurons, None)\n",
    "        rejected_neurons = np.where(difference_vector>threshold, neurons, None)\n",
    "    \n",
    "    #Builds arrays with the rejected and acepted neurons\n",
    "    acepted_neurons = np.asarray(np.ma.masked_equal(acepted_neurons, None).compressed(), dtype='int64')\n",
    "    \n",
    "    #Removes from the decoder\n",
    "    new_decoder = decoder[rejected_neurons == None]\n",
    "    \n",
    "    #Removes those columns from the dataset\n",
    "    rejected_neurons = np.asarray(np.ma.masked_equal(rejected_neurons, None).compressed(), dtype='int64')\n",
    "    if(remove_fromDataset):\n",
    "        col_toRemove = np.empty((len(rejected_neurons)), dtype=tuple)\n",
    "        for i, neuron in enumerate(rejected_neurons):\n",
    "            col_toRemove[i] = ('spikes', neuron)\n",
    "        dataset.drop(columns=col_toRemove, inplace=True)\n",
    "    return new_decoder, acepted_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd03b18-850a-4e1f-8abf-ec88706204e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove Neurons for Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11baf3-ebc2-430f-9dd5-9966e1a56510",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes the neurons according to a certain threshold\n",
    "'''\n",
    "def nb_remove_neurons(datasets, neurons, predicted, observed, grids, method, threshold=None, remove_fromDataset=True):\n",
    "    #Finding the neurons\n",
    "    if(method == 'depth'):\n",
    "        amplitude = predicted[:,0]\n",
    "        mask = np.where(amplitude >= threshold, True, False)\n",
    "    elif(method == 'std'):\n",
    "        x_std = np.array(predicted[:,3])\n",
    "        y_std = np.array(predicted[:,4])\n",
    "        mean_std = (x_std + y_std)/2\n",
    "        mask = np.where(mean_std <= threshold, True, False)\n",
    "    elif(method == 'diff'):\n",
    "        # Convert grids to the appropriate format\n",
    "        grids = np.array(grids)\n",
    "        x_grid = grids[0]\n",
    "        y_grid = grids[1]\n",
    "        \n",
    "        # Initialize an array to store differences\n",
    "        difference_matrix = np.zeros((len(observed), x_grid.shape[0], y_grid.shape[1]))\n",
    "        mask = ~np.isnan(observed[0])\n",
    "        \n",
    "        # Loop over all observed instances\n",
    "        for i in range(len(observed)):\n",
    "            # Calculate the predicted Gaussian surface for the entire grid\n",
    "            pred = ex_twoD_Gaussian2((x_grid, y_grid), *predicted[i])\n",
    "        \n",
    "            # Calculate the difference where observed is not NaN\n",
    "            difference_matrix[i][mask] = pred[mask] - observed[i][mask]\n",
    "        \n",
    "        # Flatten the differences for each instance\n",
    "        difference_matrix = difference_matrix.reshape(len(observed), -1)\n",
    "        distance_matrix = np.sqrt(np.sum(np.power(difference_matrix, 2), axis=1))\n",
    "\n",
    "        mask = np.where(distance_matrix <= threshold, True, False)\n",
    "\n",
    "    #Removing the neurons\n",
    "    neurons = np.array(neurons)\n",
    "    removed_neurons = neurons[~mask]\n",
    "    acepted_neurons = neurons[mask]\n",
    "    acepted_predicted = predicted[mask]\n",
    "    \n",
    "    if(remove_fromDataset):\n",
    "        for j, dataset in enumerate(datasets):\n",
    "            col_toRemove = np.empty((len(removed_neurons)), dtype=tuple)\n",
    "            for i, neuron in enumerate(removed_neurons):\n",
    "                col_toRemove[i] = ('spikes', neuron)\n",
    "            dataset.drop(columns=col_toRemove, inplace=True)\n",
    "    return acepted_neurons, acepted_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7fb5b0-5c3a-4840-b52a-092ab9ed700d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Get the distance of Tuning Surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197aac4f-7535-4532-b235-c7090404280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert grids to the appropriate format\n",
    "def get_tune_distance(grids, observed, predicted):\n",
    "    grids = np.array(grids)\n",
    "    x_grid = grids[0]\n",
    "    y_grid = grids[1]\n",
    "    \n",
    "    # Initialize an array to store differences\n",
    "    difference_matrix = np.zeros((len(observed), x_grid.shape[0], y_grid.shape[1]))\n",
    "    mask = ~np.isnan(observed[0])\n",
    "    \n",
    "    # Loop over all observed instances\n",
    "    for i in range(len(observed)):\n",
    "        # Calculate the predicted Gaussian surface for the entire grid\n",
    "        pred = ex_twoD_Gaussian2((x_grid, y_grid), *predicted[i])\n",
    "    \n",
    "        # Calculate the difference where observed is not NaN\n",
    "        difference_matrix[i][mask] = pred[mask] - observed[i][mask]\n",
    "    \n",
    "    # Flatten the differences for each instance\n",
    "    difference_matrix = difference_matrix.reshape(len(observed), -1)\n",
    "    distance_matrix = np.sqrt(np.sum(np.power(difference_matrix, 2), axis=1))\n",
    "\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b2356-1f01-442f-b05b-8386ab1ba712",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compares Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04fa4b-f167-4a90-a48c-528a681b83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the different models pearson preformance\n",
    "'''\n",
    "def compare_models(parameters, all_entries, model_names, eval='Pearson', title_name='title', label_name='label', \n",
    "                   save=False, save_name='plot.png'):\n",
    "    # Data extraction\n",
    "    nr_trials = len(all_entries[0, 0])\n",
    "    if(eval == 'Pearson'):\n",
    "        x_std = parameters[:, 0, 1]\n",
    "        y_std = parameters[:, 1, 1]\n",
    "        x_mean = parameters[:, 0, 0]\n",
    "        y_mean = parameters[:, 1, 0]\n",
    "        label_x = 'Mean Pearson for Mx'\n",
    "        label_y = 'Mean Pearson for My'\n",
    "    elif(eval == 'R2'):\n",
    "        x_std = parameters[:, 0, 3]\n",
    "        y_std = parameters[:, 1, 3]\n",
    "        x_mean = parameters[:, 0, 2]\n",
    "        y_mean = parameters[:, 1, 2]\n",
    "        label_x = 'Mean R2 for Mx'\n",
    "        label_y = 'Mean R2 for My'\n",
    "    \n",
    "    results = {\n",
    "        label_x: x_mean,\n",
    "        label_y: y_mean\n",
    "    }\n",
    "    cov_results = {\n",
    "        'Coefficient of Variation for X': x_std / x_mean,\n",
    "        'Coefficient of Variation for Y': y_std / y_mean,\n",
    "        'Mean Coefficient of Variation': np.mean([x_std / x_mean, y_std / y_mean], axis=0)\n",
    "    }\n",
    "    \n",
    "    # Extra Variables\n",
    "    max_mean_x = np.max(x_mean)\n",
    "    max_mean_y = np.max(y_mean)\n",
    "    max_x_model = np.argmax(x_mean)\n",
    "    max_y_model = np.argmax(y_mean)\n",
    "    deviation = np.array([x_std, y_std])\n",
    "    error = deviation / np.sqrt(nr_trials)\n",
    "    \n",
    "    #Begin Plotting\n",
    "    fig, ax1 = plt.subplots(figsize=(15, 5), layout='constrained')\n",
    "    ax2 = ax1.twinx()\n",
    "    model_x = np.arange(len(model_names))  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "    colors = ['tomato', 'dodgerblue', 'tomato', 'dodgerblue', 'black']\n",
    "    \n",
    "    # Plot CoV on the right y-axis\n",
    "    for i, (attribute, measurement) in enumerate(cov_results.items()):\n",
    "        offset = width * (i + 1)\n",
    "        alpha_value = 0.15 if i != 2 else 0.3\n",
    "        ax2.plot(model_x + offset, measurement, color=colors[i + 2], alpha=alpha_value, label=attribute)\n",
    "    \n",
    "    # Plot Pearson values on the left y-axis\n",
    "    for i, (attribute, measurement) in enumerate(results.items()):\n",
    "        offset = width * (i + 1)\n",
    "        ax1.errorbar(model_x + offset, measurement, error[i], linewidth=width * 5, ls='none', capsize=5, label=attribute, color=colors[i])\n",
    "    \n",
    "    # Highlight preferred model\n",
    "    mean_cov = np.mean([x_std / x_mean, y_std / y_mean], axis=0)\n",
    "    min_cov_index = np.argmin(mean_cov)\n",
    "    t1_x = model_x[min_cov_index]\n",
    "    t2_x = t1_x + offset + width\n",
    "    ax1.vlines(t1_x, 0, 1, color='grey', linestyle='--', linewidth=0.5)\n",
    "    ax1.vlines(t2_x, 0, 1, color='grey', linestyle='--', linewidth=0.5)\n",
    "    ax1.fill_betweenx([0, 1], t1_x, t2_x, color='yellow', alpha=0.3, \n",
    "                      label=f'Best Parameter: {model_names[min_cov_index]} (x: {round(x_mean[min_cov_index],2)} y: {round(y_mean[min_cov_index],2)})')\n",
    "    \n",
    "    # Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "    ax1.set_ylabel(eval)\n",
    "    ax1.set_xlabel(label_name)\n",
    "    ax1.set_title(title_name)\n",
    "    ax1.set_xticks(model_x + width)\n",
    "    ax1.set_xticklabels(np.array(model_names, dtype=int), fontsize=8)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.legend(loc='upper left', ncols=1)\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Add label for the right y-axis\n",
    "    ax2.set_ylabel('Coefficient of Variation')\n",
    "    ax2.legend(loc='upper right', ncols=1)\n",
    "    \n",
    "    # Save figure if required\n",
    "    if save: plt.savefig(save_name)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfac98d-435e-4df5-a20f-a76322a1e217",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Programming Journey (Danger Zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b18fa4-a6bc-4935-802e-ce9054a9a1ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1º Meeting - Play with the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9d6ac-ffe7-435a-8664-b99aab14d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression with given loss function and given regularization\n",
    "def linear_regression(dataset, loss='sse', lambaRidge=2):\n",
    "    # Data preparation\n",
    "    trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-130, 370))\n",
    "    lagged_trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=(-50, 450))\n",
    "    rates = trial_data.spikes.to_numpy()\n",
    "    vel = lagged_trial_data.hand_vel.to_numpy()\n",
    "        \n",
    "    #Loss Function Selection\n",
    "    if(loss == 'sse'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        penrose = np.linalg.pinv(rates.astype(np.float32))\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "    elif(loss == 'ridge'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        ratesT = np.transpose(rates)\n",
    "        prod = np.matmul(ratesT,rates)\n",
    "        identity = np.identity(len(prod))\n",
    "        helper = prod + lambaRidge * identity\n",
    "        inverse = np.linalg.inv(helper)\n",
    "        penrose = np.matmul(inverse,ratesT)\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "\n",
    "    print(f\"Pearson for the X direction: {pearsonr(predicted[:,0], trial_data['hand_vel']['x'])[0]}\")\n",
    "    print(f\"Pearson for the Y direction: {pearsonr(predicted[:,1], trial_data['hand_vel']['y'])[0]}\")\n",
    "\n",
    "    predictedDS = copy.deepcopy(dataset)\n",
    "    pred_vel_df = pd.DataFrame(predicted, index=lagged_trial_data.clock_time, columns=pd.MultiIndex.from_tuples([('pred_vel', 'x'), ('pred_vel', 'y')]))\n",
    "    predictedDS.data = pd.concat([predictedDS.data, pred_vel_df], axis=1)\n",
    "\n",
    "    return predictedDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a189c-3ef6-4068-800a-473aecde8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDS = linear_regression(smallDS,'ridge',np.exp(-18))\n",
    "plot_comparison(testDS, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d80e5c-abad-4dc0-8e68-0db8d247c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDS = linear_regression(smallDS,'ridge',1)\n",
    "plot_comparison(testDS, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c213db-30d6-40a8-8577-87d1a5186220",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDS = linear_regression(smallDS)\n",
    "plot_comparison(testDS, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d6bb9-c528-4dcb-b8f2-837e7b7d8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = smallDS.make_trial_data(align_field='move_onset_time', align_range=(-130, 370))\n",
    "lagged_trial_data = smallDS.make_trial_data(align_field='move_onset_time', align_range=(-50, 450))\n",
    "vel = lagged_trial_data.hand_vel.to_numpy()\n",
    "\n",
    "rates = trial_data.spikes.to_numpy()\n",
    "rates = np.insert(rates, 0, 1, axis=1)\n",
    "ratesT = np.transpose(rates)\n",
    "prod = np.matmul(ratesT,rates)\n",
    "identity = np.identity(len(prod))\n",
    "helper = prod + 2 * identity\n",
    "inverse = np.linalg.inv(helper)\n",
    "penrose = np.matmul(inverse,ratesT)\n",
    "w = np.matmul(penrose,vel)\n",
    "predicted = np.matmul(rates,w)\n",
    "\n",
    "len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c431f-9dfc-40c5-bc38-307c891800f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2º Meeting - Rate Code and Tuning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836133e9-51c7-44b5-bd7d-c1b0aa196484",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aacf12-9002-4aa7-a660-0ba904513670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the data from the tunning curve into a graph\n",
    "def plot_tuning(tuning_data, prefered_angle):\n",
    "    \n",
    "    x_coords = [item[0] for item in tuning_data]\n",
    "    values = [item[1] for item in tuning_data]\n",
    "    deviation = [item[2] for item in tuning_data]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot the first subplot - tunning curve with the deviations\n",
    "    axes[0,0].errorbar(x_coords, values, deviation, fmt='o', linewidth=1, capsize=6)\n",
    "    axes[0,0].set_xlabel('Angles [-pi,pi]')\n",
    "    axes[0,0].set_ylabel('Firing Rates')\n",
    "    axes[0,0].set_title(f'Tuning Data from Neuron {neuron}')\n",
    "    axes[0,0].set_xlim(-np.pi, np.pi)\n",
    "    \n",
    "    #Plot the second subplot - tunning curves alone\n",
    "    axes[0,1].scatter(x_coords, values)\n",
    "    axes[0,1].set_xlabel('Angles [-pi,pi]')\n",
    "    axes[0,1].set_ylabel('Firing Rates')\n",
    "    axes[0,1].set_title(f'Tuning Data from Neuron {neuron}')\n",
    "    axes[0,1].set_xlim(-np.pi, np.pi)\n",
    "    \n",
    "    # Plot the third subplot - preferred angle\n",
    "    axes[1,0].quiver(0, 0, 1 * np.cos(prefered_angle), 1 * np.sin(prefered_angle), angles='xy', scale_units='xy', scale=1, color='b', \n",
    "                     width=0.005)\n",
    "    axes[1,0].set_xlim(-1, 1)\n",
    "    axes[1,0].set_ylim(-1, 1)\n",
    "    axes[1,0].set_title(f'Preferred Direction of {round(prefered_angle, 3)}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff4bfa-20ab-4445-b47d-f8a9f12d49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots all the vectors that a bunch of neurons code for\n",
    "def plot_vectorMap(vectors):\n",
    "    for pair in vectors:\n",
    "        plt.quiver(0, 0, 1 * np.cos(pair[1]), 1 * np.sin(pair[1]), angles='xy', scale_units='xy', scale=1, color='b', label=pair[0], \n",
    "                   width=0.003)\n",
    "    plt.xlim(-1, 1) \n",
    "    plt.ylim(-1, 1)\n",
    "    plt.title('All Direction (Mapping)') \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbecb64-d50e-4a79-9770-578f63860370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the actual angle vs the predicted one in the context of covariate 1\n",
    "def plot_decoding(trials_toCheck, results):\n",
    "    nr_trials = len(trials_toCheck)\n",
    "    nr_rows = nr_trials//3\n",
    "    if(nr_trials%3 == 0): extra_row = 0\n",
    "    else: extra_row = 1\n",
    "    fig, axes = plt.subplots(nr_rows+extra_row, 3, figsize=(12, (nr_rows+extra_row)*3))\n",
    "    for i in range(nr_rows+extra_row):\n",
    "        if((extra_row == 1 and i < nr_rows+extra_row-1) or extra_row == 0):\n",
    "            for j in range(3):\n",
    "                predicted_angle = results[i*3+j][1]\n",
    "                actual_angle = results[i*3+j][0]\n",
    "                axes[i,j].quiver(0, 0, 1 * np.cos(predicted_angle), 1 * np.sin(predicted_angle), angles='xy', scale_units='xy', scale=1, \n",
    "                                 color='r', width=0.005, label='Predicted Angle')\n",
    "                axes[i,j].quiver(0, 0, 1 * np.cos(actual_angle), 1 * np.sin(actual_angle), angles='xy', scale_units='xy', scale=1, \n",
    "                                 color='b', width=0.005, label='Actual Angle')\n",
    "                axes[i,j].set_xlim(-1, 1)\n",
    "                axes[i,j].set_ylim(-1, 1)\n",
    "                axes[i,j].legend()\n",
    "                axes[i,j].set_title(f'Diference of {round(np.abs(actual_angle-predicted_angle), 3)} rads')\n",
    "        elif(extra_row == 1 and i == nr_rows+extra_row-1):\n",
    "            for j in range(nr_trials%3):\n",
    "                predicted_angle = results[i*3+j][1]\n",
    "                actual_angle = results[i*3+j][0]\n",
    "                axes[i,j].quiver(0, 0, 1 * np.cos(predicted_angle), 1 * np.sin(predicted_angle), angles='xy', scale_units='xy', scale=1, \n",
    "                                 color='r', width=0.005, label='Predicted Angle')\n",
    "                axes[i,j].quiver(0, 0, 1 * np.cos(actual_angle), 1 * np.sin(actual_angle), angles='xy', scale_units='xy', scale=1, \n",
    "                                 color='b', width=0.005, label='Actual Angle')\n",
    "                axes[i,j].set_xlim(-1, 1)\n",
    "                axes[i,j].set_ylim(-1, 1)\n",
    "                axes[i,j].legend()\n",
    "                axes[i,j].set_title(f'Diference of {round(np.abs(actual_angle-predicted_angle), 3)} rads')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5d23d-2601-4636-aa9f-a0b405a878ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd906b-4506-47d8-9799-4e72122505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the trial_info subset data without/with trial exclusions\n",
    "def trial_infoExtractor (dataset, ignored_mask, settings):\n",
    "    if(len(ignored_mask) == 0): \n",
    "        target_data = np.asarray(dataset.trial_info[['target_pos','active_target']])\n",
    "        duration_data = np.asarray(dataset.trial_info[['end_time','move_onset_time']], dtype='float64')/1000000 #In ms\n",
    "    else: \n",
    "        target_data = np.asarray(dataset.trial_info[['target_pos','active_target']][ignored_mask])\n",
    "        duration_data = np.asarray(dataset.trial_info[['end_time','move_onset_time']][ignored_mask], dtype='float64')/1000000 #In ms\n",
    "    \n",
    "    target_position = []             #Saves the target position per trial\n",
    "    target_angle = []                #Saves the target angle with origin per trial\n",
    "    trial_duration = []              #Saves the duration per trial in ms\n",
    "    for i in range(len(target_data)):\n",
    "        position = target_data[i][0][target_data[i][1]]\n",
    "        target_position +=  [position]\n",
    "        if(settings == 'simple'): target_angle += [np.arctan2(position[1],position[0])]\n",
    "        elif(settings == 'double_cos'): target_angle += [[np.cos(np.arctan2(position[1],position[0])), \n",
    "                                                          np.cos(np.arctan2(position[0],position[1]))]]\n",
    "        trial_duration += [duration_data[i][0]-duration_data[i][1]]\n",
    "    target_position = np.asarray(target_position) \n",
    "    #print(target_angle)\n",
    "\n",
    "    return target_angle, trial_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28716752-f2de-4e3c-b283-2b33af9ab82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns all diferent angles present in the set\n",
    "def all_angles(sorted_tunning):\n",
    "    size = len(sorted_tunning)\n",
    "    checked_angles = []\n",
    "    for i in range(size):\n",
    "        if(sorted_tunning[i][0] not in checked_angles):\n",
    "            checked_angles += [sorted_tunning[i][0]]\n",
    "    return checked_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b5100-d8dd-4b1a-9750-fed434c16889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the mean firing rate from each angle\n",
    "def mean_firingRate(different_angles, sorted_tunning):\n",
    "    final_tune = []\n",
    "    size = len(different_angles)\n",
    "    for j in range(size):\n",
    "        store = []\n",
    "        for i in range(len(sorted_tunning)):\n",
    "            if(sorted_tunning[i][0] == different_angles[j]):\n",
    "                store += [sorted_tunning[i][1]]\n",
    "        final_tune += [[different_angles[j], np.mean(store), np.std(store)]]\n",
    "    return final_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97cac6-c5d9-422f-8f79-766fda7e4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the tunning curve and returns the preferred direction\n",
    "def prefered_direction(tunning_data):\n",
    "    size = len(tunning_data)\n",
    "    prefered_angle = None\n",
    "    biggest_rate = 0\n",
    "    for i in range(size):\n",
    "        if(tunning_data[i][1] > biggest_rate): \n",
    "            prefered_angle = tunning_data[i][0]\n",
    "            biggest_rate = tunning_data[i][1]\n",
    "    return prefered_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e615e9b-104f-45f7-9d7c-6e32a07963bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression with given loss function and given regularization that returns the weights\n",
    "def linear_regression_model(trainning_var, trainning_out, loss='sse', lambaRidge=2):\n",
    "    #Data preparation\n",
    "    rates = trainning_var\n",
    "    vel = trainning_out\n",
    "    \n",
    "    #Loss Function Selection\n",
    "    if(loss == 'sse'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        penrose = np.linalg.pinv(rates.astype(np.float32))\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "    elif(loss == 'ridge'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        ratesT = np.transpose(rates)\n",
    "        prod = np.matmul(ratesT,rates)\n",
    "        identity = np.identity(len(prod))\n",
    "        helper = prod + lambaRidge * identity\n",
    "        inverse = np.linalg.inv(helper)\n",
    "        penrose = np.matmul(inverse,ratesT)\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8955112-d1cb-4adf-ab5c-00bad8d01ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtains the desired spike data (+ trial ids) and the mask used to remove the ignored trials. If wanted can generate for only one trial\n",
    "def generate_spikeData(dataset, neuron, ignored_ids=None, trial=-1, extra_needed=False):\n",
    "    ignored_mask = []\n",
    "    if(trial != -1): \n",
    "        ignored_mask = np.all(dataset.trial_info[['trial_id']] != trial, axis=1)\n",
    "        trial_data = dataset.make_trial_data(start_field='move_onset_time', ignored_trials=ignored_mask)\n",
    "        if(extra_needed): \n",
    "            duration_data = np.asarray(dataset.trial_info[['end_time','move_onset_time']][~ignored_mask], dtype='float64')/1000000\n",
    "            target_data = np.asarray(dataset.trial_info[['target_pos','active_target']][~ignored_mask])\n",
    "    elif(ignored_ids == None): \n",
    "        trial_data = dataset.make_trial_data(start_field='move_onset_time')\n",
    "    else:\n",
    "        for ids in ignored_ids:\n",
    "            ignored_mask += [np.all(dataset.trial_info[['trial_id']] != ids, axis=1)]\n",
    "        ignored_mask = reduce(operator.and_, ignored_mask)\n",
    "        trial_data = dataset.make_trial_data(start_field='move_onset_time', ignored_trials=~ignored_mask)\n",
    "\n",
    "    neuron_preData = trial_data[['spikes','trial_id']]\n",
    "    neuron_data = neuron_preData[['trial_id']].copy()\n",
    "    neuron_data[neuron] = neuron_preData['spikes'][neuron].copy()\n",
    "    neuron_data = np.asarray(neuron_data)\n",
    "\n",
    "    if(extra_needed):\n",
    "        position = target_data[0][0][target_data[0][1]]\n",
    "        target_angle = np.arctan2(position[1],position[0])\n",
    "        trial_duration = duration_data[0][0]-duration_data[0][1]\n",
    "        return neuron_data, ignored_mask, trial_duration, target_angle\n",
    "    return neuron_data, ignored_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb6ff1-7feb-420a-8f68-61f7590e244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the firing rate of a given neuron in a given trial\n",
    "def neuron_firingRate(neuron_data, movement_duration, trial, debugger=False):\n",
    "    firing_time = movement_duration / 2\n",
    "    firing_bins = firing_time // 5\n",
    "    count = 0\n",
    "    bins_added = 0\n",
    "    j = 0\n",
    "    while j < len(neuron_data) and bins_added < firing_bins:\n",
    "        if(neuron_data[j][0] == trial): \n",
    "            count += neuron_data[j][1]\n",
    "            bins_added += 1\n",
    "        j += 1\n",
    "    firing_rate = count/firing_time\n",
    "    \n",
    "    if(debugger):\n",
    "        print('Counted: ', count)\n",
    "        print('Firing Rate: ', firing_rate)\n",
    "        print('----')\n",
    "        print('Firing Time: ', firing_time)\n",
    "        print('Firing Bins: ', firing_bins)\n",
    "        print('')\n",
    "\n",
    "    return firing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd243da2-0ba0-4db6-ad64-c47bbe78e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the cosines values to the angle\n",
    "def cos2angle(cosines):\n",
    "    if(cosines[1] >= 0): angle = np.arccos(cosines[0])\n",
    "    else: angle = -np.arccos(cosines[0])\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324c07c-cdcb-4fc6-b3dc-ba0071ee611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts an angle to its cosines\n",
    "def angle2cos(angle):\n",
    "    return np.cos(angle), np.sin(angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed44b9-2d93-423c-969e-1efdeb5f4ddf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba915ebc-4497-4c27-807f-deec8f1d68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allows for better visualization of the spikes of a specific trial\n",
    "def raster_plot (dataset, neurons=None, trial=0, align=None):\n",
    "    mask = np.all(dataset.trial_info[['trial_id']] == trial, axis=1)\n",
    "\n",
    "    if(align == None): \n",
    "        trial_data = dataset.make_trial_data(align_field='move_onset_time', ignored_trials=(~mask))\n",
    "    else:\n",
    "        trial_data = dataset.make_trial_data(align_field='move_onset_time', align_range=align, ignored_trials=(~mask))\n",
    "\n",
    "    \n",
    "    if (neurons == None):\n",
    "        neurons = trial_data['spikes'].columns.tolist() #Gets the name of all neurons\n",
    "    neuron_spikes = []\n",
    "    for neuron in neurons:\n",
    "        neuron_spikes = neuron_spikes + [trial_data['spikes'][neuron]]\n",
    "\n",
    "    #Initialize a list to store spike times for each neuron\n",
    "    spike_times = [[] for _ in range(len(neuron_spikes))]\n",
    "    \n",
    "    #Iterate through the spike data and extract spike times for each neuron\n",
    "    for neuron_idx, neuron in enumerate(neuron_spikes):\n",
    "        for time_idx, spike in enumerate(neuron):\n",
    "            if spike == 1:\n",
    "                spike_times[neuron_idx].append(time_idx)  # Assuming time_idx represents time in ms\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for i, spikes in enumerate(spike_times, start=1):\n",
    "        ax.eventplot(spikes, colors='k', lineoffsets=i, linelengths=0.5)\n",
    "    ax.set_xlabel('Time (bins)')\n",
    "    #if(align != None): ax.set_xlim(align[0]//5, align[1]//5)\n",
    "    ax.set_ylabel('Neuron')\n",
    "    ax.set_title('Raster Plot for Neuron Firing')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa4d0c-6663-4157-acec-43f43904998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recieves a dataset and will generate a tunning curve and, using a explicit rule, define the prefered direction of the \n",
    "given neurons. Will also return the b coeficient used in the decoding function\n",
    "Co-variate: Target location\n",
    "'''\n",
    "def tuning_curve(dataset, neuron, ignored_ids=None, settings='maximum', debugger=False, plot=True):\n",
    "\n",
    "    #Obtains the desired spike data (+ trial ids) and the mask used to remove the ignored trials\n",
    "    neuron_data, ignored_mask = generate_spikeData(dataset, neuron, ignored_ids)\n",
    "    \n",
    "    #Defines the settings according to some templates\n",
    "    if(settings == 'maximum'): angle_type = 'simple' #Angle is returned as the angle with vector (0,1)\n",
    "    elif(settings == 'cosines'): angle_type = 'double_cos' #Angle is returned as the two cosines with vector (0,1) and (1,0)\n",
    "\n",
    "    #Gets the angle that the activated target makes and the duration of movement for each trial \n",
    "    target_angle, trial_duration = trial_infoExtractor(dataset,ignored_mask, angle_type)\n",
    "\n",
    "    tunning_data = []\n",
    "    for i in range(len(trial_duration)):\n",
    "        firing_rate = neuron_firingRate(neuron_data, trial_duration[i], i, debugger)\n",
    "        tunning_data += [[target_angle[i], firing_rate]]\n",
    "\n",
    "    #Sorts data from angles (necessary if you want a plot with connected lines)\n",
    "    sorted_tunning = sorted(tunning_data, key=lambda x: x[0])     \n",
    "\n",
    "    \n",
    "    #Returns the direction with higher firing rate\n",
    "    if(settings == 'maximum'): \n",
    "        different_angles = all_angles(sorted_tunning)\n",
    "        results = mean_firingRate(different_angles,sorted_tunning)\n",
    "        angle = prefered_direction(results)\n",
    "        if(plot): plot_tuning(results, angle)\n",
    "        return angle\n",
    "        \n",
    "    #Returns the coded direction using the equation d(M) = b + bxmx + bymy\n",
    "    elif(settings == 'cosines'):\n",
    "        cosines = [item[0] for item in sorted_tunning]\n",
    "        freqs = [item[1] for item in sorted_tunning]\n",
    "        weights = linear_regression_model(cosines, freqs)\n",
    "        k = np.sqrt(pow(weights[1],2) + pow(weights[2],2))\n",
    "        vectorC = [weights[1]/k, weights[2]/k]\n",
    "        angle = cos2angle(vectorC)\n",
    "        \n",
    "        if(plot): \n",
    "            different_angles = all_angles(sorted_tunning)\n",
    "            results = mean_firingRate(different_angles,sorted_tunning)\n",
    "            for i in range(len(results)): results[i][0] = cos2angle(results[i][0])\n",
    "            plot_tuning(results, angle)\n",
    "\n",
    "        \n",
    "        return vectorC, weights[0], angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5eb44b-ac92-4f26-bdcc-1e10e2d7db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the prefered directions of all neurons\n",
    "def direction_mapping(dataset, neurons, ignored_trials=None, config='maximum', plot=True):\n",
    "    results = []\n",
    "    plot_data = []\n",
    "    for neuron in neurons:\n",
    "        tuning_data = tuning_curve(dataset,neuron, ignored_ids=ignored_trials, settings=config, plot=False)\n",
    "        results += [[neuron, tuning_data[0], tuning_data[1]]]\n",
    "        if(plot): plot_data += [[neuron,tuning_data[2]]]\n",
    "    if(plot): plot_vectorMap(plot_data)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0279a6-a726-4464-b290-54cf502667fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recieves a frequency and the trainned data and returns the predicted direction\n",
    "def population_prediction(rate, data):\n",
    "    nr_neurons = len(data)\n",
    "    P = 0\n",
    "    for i in range(nr_neurons):\n",
    "        P += (rate[i]-data[i][3])*np.array([data[i][1], data[i][2]])\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc009236-680c-40c8-a008-b38c6acf4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies the decoding for multiple trials\n",
    "def decode_trials(dataset, trials_toCheck, neurons, vector_mapping, plot = False):\n",
    "    results = []\n",
    "    for trial_check in trials_toCheck:\n",
    "        rates = []\n",
    "        for neuron in neurons:\n",
    "            test_spikes, mask, duration, actual_angle = generate_spikeData(dataset, neuron, trial=trial_check, extra_needed=True)\n",
    "            rates += [neuron_firingRate(test_spikes, duration, trial_check)]\n",
    "        predicted_angle = cos2angle(population_prediction(rates, vector_mapping))\n",
    "        results += [[actual_angle, predicted_angle]]\n",
    "    if(plot): plot_decoding(trials_toCheck, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1cca75-1498-437a-965c-35a39bb83ad4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Crafting Table - Covariate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb59b4-a4ce-4317-8ee5-b797e8a8f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with target covariate and cosines angles (paper)\n",
    "Time Required: 65 sec\n",
    "'''\n",
    "\n",
    "decoder = direction_mapping(smallDS, neurons, config='cosines', plot=False)\n",
    "decoder #Takes about 35sec\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 35, small_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots the cross-correlation set\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Visualizes all predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdb0d6-72e3-466e-b776-21bf7d528e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the turning curve of a given neuron\n",
    "neuron = neurons[1]\n",
    "tuning_curve(smallDS, neuron, ignored_ids=[0,1], settings='cosines', debugger=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85646e51-eabb-4f12-af24-35023374510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the predicted target direction and compares to the actual target direction\n",
    "trials_toCheck = [0,1,2,3,4,5,6,7,8]\n",
    "decode_trials(smallDS, trials_toCheck, neurons, vector_mapping, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950143bc-e1f1-4a86-b689-37767eac8b6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Crafting Table - Covariate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34315945-7dd5-48fa-ac0c-36a03f87eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes a dataset spike to a firing rate calculated with a specific window size\n",
    "def firing_rate(dataset, window_size, neurons, bin_time=5):\n",
    "    spikes_data = dataset['spikes'].copy()\n",
    "    size = len(spikes_data)\n",
    "    for neuron in neurons:\n",
    "        spikes_data[neuron] = np.convolve(spikes_data[neuron], np.ones(window_size)/window_size, mode='same')/bin_time\n",
    "    dataset['spikes'] = spikes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cec92-711f-4b51-b34b-c951881cfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates the Train/Val subset\n",
    "def generate_subset(dataset, subset_name, start_time='move_onset_time'):\n",
    "    mask = np.all(dataset.trial_info[['split']] == subset_name, axis=1)\n",
    "    return dataset.make_trial_data(start_field=start_time, ignored_trials=~mask), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6668377-991c-4e9f-9e70-bbd86fe4eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_decoder(train_dataset, window_size, neurons, delay=0, start_time='move_onset_time', bin_time=5):\n",
    "    #Change spikes to firing rates in ms (moving average)\n",
    "    firing_rate(train_dataset, window_size, bin_time=bin_time)\n",
    "    train_size = len(train_dataset)\n",
    "    \n",
    "    #Adds the column vectorM with the cosines components to the train_dataset\n",
    "    vectorM = train_dataset['hand_pos'].copy()\n",
    "    vectorM = vectorM.rename(columns={'x': 'mx'}) \n",
    "    vectorM = vectorM.rename(columns={'y': 'my'}) \n",
    "    for i in range(train_size):\n",
    "        if (i == 0):\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        elif(train_dataset.loc[i,'trial_id'][0] != train_dataset.loc[i-1,'trial_id'][0]): #Find ways to optimize this loop gives 10 extra seconds\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        else:\n",
    "            preX = vectorM.loc[i - 1, 'mx']\n",
    "            preY = vectorM.loc[i - 1, 'my']\n",
    "        x = vectorM.loc[i, 'mx']\n",
    "        y = vectorM.loc[i, 'my']\n",
    "        angle = np.arctan2((y - preY), (x - preX))\n",
    "        cosX, cosY = angle2cos(angle)  \n",
    "        vectorM.loc[i, 'mx'] = cosX\n",
    "        vectorM.loc[i, 'my'] = cosY\n",
    "        vectorM.loc[i, 'angle'] = angle\n",
    "    train_dataset['mx'] = vectorM['mx']\n",
    "    train_dataset['my'] = vectorM['my']\n",
    "    train_dataset['angle'] = vectorM['angle']\n",
    "    \n",
    "    #Calculate the coeficients for every neuron and generates the decoder\n",
    "    ms_array = np.array([np.array([x, y]) for x, y in zip(vectorM['mx'], vectorM['my'])])\n",
    "    \n",
    "    #Adds the delay (if there is any)\n",
    "    delayM_array = []\n",
    "    for i in range(train_size):\n",
    "        if(i+delay > train_size-1): delayM_array += [[0,0]]\n",
    "        else: delayM_array += [ms_array[i+delay]]\n",
    "    ms_array = delayM_array\n",
    "    \n",
    "    decoder = []\n",
    "    for neuron in neurons:\n",
    "        rates = train_dataset['spikes'][neuron].copy()\n",
    "        weights = linear_regression_model(ms_array, np.array(rates), loss='ridge')\n",
    "        k = np.sqrt(weights[1]**2 + weights[2]**2)\n",
    "        cx = weights[1]/k\n",
    "        cy = weights[2]/k\n",
    "        angle = cos2angle([cx,cy])\n",
    "        magnitude = np.sqrt(np.square(cx) + np.square(cy))\n",
    "        decoder += [[neuron, [cx, cy], weights[0], magnitude, angle]]\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb7cd6-607c-446c-8ea4-d36b2297cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds to the dataset predicted angle values\n",
    "def angle_predictor(val_dataset, decoder, window_size, neurons, removed_neurons=None ,distance_coef=7, bin_time=5, pred_pos=False):\n",
    "    #Prepares the data\n",
    "    firing_rate(val_dataset, window_size, bin_time=bin_time)\n",
    "    val_size = len(val_dataset)\n",
    "\n",
    "    #Predicts the direction\n",
    "    for i in range(val_size):\n",
    "        if(removed_neurons != None): \n",
    "            pred_angle = population_prediction(np.asarray(val_dataset['spikes'].drop(removed_neurons, axis=1).loc[i]),decoder) #HEREEEEE\n",
    "        else:\n",
    "            pred_angle = population_prediction(np.asarray(val_dataset.loc[i, 'spikes']),decoder)\n",
    "        val_dataset.loc[i, 'pred_mag'] = np.sqrt(np.square(pred_angle[0]) + np.square(pred_angle[1]))\n",
    "        val_dataset.loc[i, 'pred_cosX'] = pred_angle[0]\n",
    "        val_dataset.loc[i, 'pred_cosY'] = pred_angle[1]\n",
    "        val_dataset.loc[i, 'pred_angle'] = cos2angle(pred_angle)\n",
    "    \n",
    "    #Predicts the position\n",
    "        if(pred_pos):\n",
    "            if(i == 0): \n",
    "                val_dataset.loc[i, 'pred_X'] = val_dataset.loc[i, 'hand_pos']['x']\n",
    "                val_dataset.loc[i, 'pred_Y'] = val_dataset.loc[i, 'hand_pos']['y']\n",
    "            elif(val_dataset.loc[i,'trial_id'][0] != val_dataset.loc[i-1,'trial_id'][0]): \n",
    "                val_dataset.loc[i, 'pred_X'] = val_dataset.loc[i, 'hand_pos']['x']\n",
    "                val_dataset.loc[i, 'pred_Y'] = val_dataset.loc[i, 'hand_pos']['y']\n",
    "            else:\n",
    "                val_dataset.loc[i, 'pred_X'] = val_dataset.loc[i-1, 'pred_X'][0] + distance_coef * val_dataset.loc[i-1, 'pred_mag'][0] * val_dataset.loc[i-1, 'pred_cosX'][0]\n",
    "                val_dataset.loc[i, 'pred_Y'] = val_dataset.loc[i-1, 'pred_Y'][0] + distance_coef * val_dataset.loc[i-1, 'pred_mag'][0] * val_dataset.loc[i-1, 'pred_cosY'][0]\n",
    "    \n",
    "    #Builds the vectorM for comparition\n",
    "    vectorM = val_dataset['hand_pos'].copy()\n",
    "    vectorM = vectorM.rename(columns={'x': 'mx'}) \n",
    "    vectorM = vectorM.rename(columns={'y': 'my'}) \n",
    "    for i in range(val_size):\n",
    "        if (i == 0):\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        elif(val_dataset.loc[i,'trial_id'][0] != val_dataset.loc[i-1,'trial_id'][0]): #Find ways to optimize this loop gives 10 extra seconds\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        else:\n",
    "            preX = vectorM.loc[i - 1, 'mx']\n",
    "            preY = vectorM.loc[i - 1, 'my']\n",
    "        x = vectorM.loc[i, 'mx']\n",
    "        y = vectorM.loc[i, 'my']\n",
    "        angle = np.arctan2((y - preY), (x - preX))\n",
    "        cosX, cosY = angle2cos(angle)  \n",
    "        vectorM.loc[i, 'mx'] = cosX\n",
    "        vectorM.loc[i, 'my'] = cosY\n",
    "        vectorM.loc[i, 'angle'] = angle\n",
    "    val_dataset['mx'] = vectorM['mx']\n",
    "    val_dataset['my'] = vectorM['my']\n",
    "    val_dataset['angle'] = vectorM['angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75db522-dba0-46ab-9e9c-8489ca99cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots all trajectory comparitions between predicted trajectory and actual trajectory\n",
    "def plot_allPredictions(dataset, trials, nr_cols=3):\n",
    "    nr_trials = len(trials)\n",
    "    #Prepares subplot organization\n",
    "    nr_rows = nr_trials//nr_cols\n",
    "    if(nr_trials%nr_cols == 0): extra_row = 0\n",
    "    else: extra_row = 1\n",
    "    fig, axes = plt.subplots(nr_rows+extra_row, nr_cols, figsize=(12, (nr_rows+extra_row)*3))\n",
    "    print(nr_rows+extra_row)\n",
    "    #Plots the data\n",
    "    for i in range(nr_rows+extra_row):\n",
    "        if((extra_row == 1 and i < nr_rows+extra_row-1) or extra_row == 0):\n",
    "            for j in range(nr_cols):\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                actual_x = []\n",
    "                actual_y = []\n",
    "                desired_trial = trials[i*nr_cols+j]\n",
    "                for k in range(len(dataset)):\n",
    "                    if(dataset.loc[k, 'trial_id'][0] == desired_trial):\n",
    "                        x_coords += [dataset.loc[k, 'pred_X'][0]]\n",
    "                        y_coords += [dataset.loc[k, 'pred_Y'][0]]\n",
    "                        actual_x += [dataset.loc[k, 'hand_pos']['x']]\n",
    "                        actual_y += [dataset.loc[k, 'hand_pos']['y']]\n",
    "                axes[i,j].plot(x_coords, y_coords, label='Predicted')\n",
    "                axes[i,j].plot(actual_x, actual_y, label='Actual')\n",
    "                axes[i,j].set_xlabel('X')\n",
    "                axes[i,j].set_ylabel('Y')\n",
    "                #axes[i,j].legend()\n",
    "                axes[i,j].grid(True)\n",
    "                axes[i,j].set_title(f\"Hand Trajectory for Trial {desired_trial}\")\n",
    "        elif(extra_row == 1 and i == nr_rows+extra_row-1):\n",
    "            for j in range(nr_trials%nr_cols):\n",
    "                x_coords = []\n",
    "                y_coords = []\n",
    "                actual_x = []\n",
    "                actual_y = []\n",
    "                desired_trial = trials[i*nr_cols+j]\n",
    "                for k in range(len(dataset)):\n",
    "                    if(dataset.loc[k, 'trial_id'][0] == desired_trial):\n",
    "                        x_coords += [dataset.loc[k, 'pred_X'][0]]\n",
    "                        y_coords += [dataset.loc[k, 'pred_Y'][0]]\n",
    "                        actual_x += [dataset.loc[k, 'hand_pos']['x']]\n",
    "                        actual_y += [dataset.loc[k, 'hand_pos']['y']]\n",
    "                axes[i,j].plot(x_coords, y_coords, label='Predicted')\n",
    "                axes[i,j].plot(actual_x, actual_y, label='Actual')\n",
    "                axes[i,j].set_xlabel('X')\n",
    "                axes[i,j].set_ylabel('Y')\n",
    "                #axes[i,j].legend()\n",
    "                axes[i,j].grid(True)\n",
    "                axes[i,j].set_title(f\"Hand Trajectory for Trial {desired_trial}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_predictions.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15ab21c-aae4-4685-a560-18fa3934ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_crossCorrelation (dataset, trial, time_window=150, bin_size=5, plot=True):\n",
    "    bin_window = time_window//bin_size\n",
    "    if(not isinstance(trial, numbers.Integral)):\n",
    "        print('Please input only one trial as an integer')\n",
    "        return\n",
    "\n",
    "    trial_dataset = trial_datasetMaker(dataset, [trial])\n",
    "    predicted = np.asarray(trial_dataset['pred_angle'])\n",
    "    actual = np.asarray(trial_dataset['angle'])\n",
    "    k = np.sqrt((np.sum(actual*actual))*(np.sum(predicted*predicted)))\n",
    "    cross_corr = np.correlate(actual, predicted, mode='full')/k\n",
    "    lags = np.arange(-time_window + 1, time_window, bin_size)\n",
    "    cross_corr = cross_corr[len(actual)-bin_window:len(actual)+bin_window]\n",
    "\n",
    "    if(plot):\n",
    "        plt.plot(lags, cross_corr)\n",
    "        plt.xlabel('Lag (ms)')\n",
    "        plt.ylabel('Pearsons Coefficient')\n",
    "        plt.title(f'Cross-correlation of Trial {trial} Angle')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Find the lag at which cross-correlation is maximum\n",
    "        max_corr_index = np.argmax(cross_corr)\n",
    "        max_corr_lag = lags[max_corr_index]\n",
    "        max_corr = cross_corr[max_corr_index]\n",
    "        print(\"Maximum cross-correlation at lag: \", max_corr_lag)\n",
    "        print(\"Maximum cross-correlation of: \", max_corr)\n",
    "    else:\n",
    "        return cross_corr, lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cb012-d9eb-478b-8b9c-6cdc5708cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_desiredTrial(val_dataset, desired_trial):\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    actual_x = []\n",
    "    actual_y = []\n",
    "    for i in range(len(val_dataset)):\n",
    "        if(val_dataset.loc[i, 'trial_id'][0] == desired_trial):\n",
    "            x_coords += [val_dataset.loc[i, 'pred_X'][0]]\n",
    "            y_coords += [val_dataset.loc[i, 'pred_Y'][0]]\n",
    "            actual_x += [val_dataset.loc[i, 'hand_pos']['x']]\n",
    "            actual_y += [val_dataset.loc[i, 'hand_pos']['y']]\n",
    "    colors = np.random.rand(len(y_coords))\n",
    "    #plt.plot(x_coords, y_coords, label='Predicted')\n",
    "    plt.scatter(x_coords, y_coords, label='Predicted', c=colors, cmap='viridis')\n",
    "    plt.plot(actual_x, actual_y, label='Actual')\n",
    "    plt.title(f\"Cursor Trajectory for Trial {desired_trial}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc133ff0-20b9-4982-b482-0e72c1c65758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosines angles (paper)\n",
    "Moving Average Size: 35\n",
    "Time Required: 39 sec\n",
    "'''\n",
    "\n",
    "#Creates the training data\n",
    "train_dataset, mask = generate_subset(smallDS, 'train')\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 35, small_neurons)\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 35, small_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots Cross_Correlation\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Plots all trial predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53f37b-0cd7-4b26-98e0-97336cc5d478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosines angles (paper)\n",
    "Moving Average Size: 150\n",
    "Time Required: 39 sec\n",
    "'''\n",
    "\n",
    "#Creates the training data\n",
    "train_dataset, mask = generate_subset(smallDS, 'train')\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 150, small_neurons)\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 150, small_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots Cross_Correlation\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Plots all trial predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de40e9-3e0c-4e5c-b858-d23473730026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosines angles (paper)\n",
    "Moving Average Size: 10\n",
    "Time Required: 39 sec\n",
    "'''\n",
    "\n",
    "#Creates the training data\n",
    "train_dataset, mask = generate_subset(smallDS, 'train')\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 10, small_neurons)\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 10, small_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots Cross_Correlation\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Plots all trial predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f590a54-0a2a-453d-a940-50ad36a0d556",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and tuning curve angles removing the neurons that \n",
    "have not the desired depth\n",
    "Time Required: 37 sec\n",
    "'''\n",
    "\n",
    "#Creates the training data\n",
    "#train_dataset, mask = generate_subset(smallDS, 'train')\n",
    "#decoder = angle_decoder(train_dataset, 35, small_neurons)\n",
    "\n",
    "dataset = train_dataset\n",
    "neurons = small_neurons\n",
    "nr_directions = 25\n",
    "\n",
    "spike_data = dataset['spikes'].copy()\n",
    "angle_data = dataset['angle'].copy()\n",
    "\n",
    "target_angles = np.linspace(-np.pi, np.pi, nr_directions, endpoint=False)\n",
    "for i, angle in enumerate(angle_data):\n",
    "    closest_angle_idx = np.argmin(np.abs(target_angles - angle))\n",
    "    angle_data[i] = target_angles[closest_angle_idx]\n",
    "\n",
    "for neuron in neurons:\n",
    "    neuron_data = spike_data[neurons]\n",
    "\n",
    "mean_matrix = []\n",
    "for j in range(len(target_angles)):\n",
    "    store_matrix = []\n",
    "    for i in range(len(spike_data)):\n",
    "        if(angle_data[i] == target_angles[j]):\n",
    "            store_matrix += [spike_data.loc[i]]\n",
    "    mean_matrix += [[target_angles[j], np.mean(store_matrix, axis=0)]]\n",
    "\n",
    "#Visualize\n",
    "decoder = []\n",
    "rejected_neurons = []\n",
    "desired_neuron = 1011\n",
    "rates_perAngle = np.zeros_like(target_angles)\n",
    "for j, neuron in enumerate (neurons):\n",
    "    for i in range(len(rates_perAngle)):\n",
    "        rates_perAngle[i] = mean_matrix[i][1][j]\n",
    "\n",
    "    # Get the maximum and minimum values\n",
    "    max_value = max(rates_perAngle)\n",
    "    min_value = min(rates_perAngle)\n",
    "    depth = (max_value - min_value)/max_value\n",
    "    max_index = np.argmax(rates_perAngle)\n",
    "    prefered_angle = target_angles[max_index]\n",
    "    \n",
    "    if(neuron == desired_neuron):\n",
    "        plt.plot(target_angles, rates_perAngle, marker='o', linestyle='-')\n",
    "        plt.xlabel('Target Angles')\n",
    "        plt.ylabel('Firing Rate')\n",
    "        plt.title(f'Tuning Curve of Neuron {desired_neuron}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        print(\"Maximum value:\", max_value)\n",
    "        print(\"Minimum value:\", min_value)\n",
    "        print(\"Depth:\", depth)\n",
    "        \n",
    "    if(depth < 0.7):\n",
    "        rejected_neurons += [neuron]\n",
    "    else:\n",
    "        decoder += [[neuron, [np.cos(prefered_angle), np.sin(prefered_angle)], 0, prefered_angle]]\n",
    "\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 35, small_neurons, removed_neurons=rejected_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots Cross_Correlation\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Plots all trial predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5378511-bced-4913-8abd-6d6f280a05da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosine angles removing the neurons that \n",
    "have not the desired depth\n",
    "Time Required: 45 sec\n",
    "'''\n",
    "\n",
    "#Creates the training data\n",
    "train_dataset, mask = generate_subset(smallDS, 'train')\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 35, small_neurons)\n",
    "\n",
    "dataset = train_dataset\n",
    "neurons = small_neurons\n",
    "nr_directions = 25\n",
    "\n",
    "spike_data = dataset['spikes'].copy()\n",
    "angle_data = dataset['angle'].copy()\n",
    "\n",
    "target_angles = np.linspace(-np.pi, np.pi, nr_directions, endpoint=False)\n",
    "for i, angle in enumerate(angle_data):\n",
    "    closest_angle_idx = np.argmin(np.abs(target_angles - angle))\n",
    "    angle_data[i] = target_angles[closest_angle_idx]\n",
    "\n",
    "for neuron in neurons:\n",
    "    neuron_data = spike_data[neurons]\n",
    "\n",
    "mean_matrix = []\n",
    "for j in range(len(target_angles)):\n",
    "    store_matrix = []\n",
    "    for i in range(len(spike_data)):\n",
    "        if(angle_data[i] == target_angles[j]):\n",
    "            store_matrix += [spike_data.loc[i]]\n",
    "    mean_matrix += [[target_angles[j], np.mean(store_matrix, axis=0)]]\n",
    "\n",
    "#Visualize\n",
    "rejected_neurons = []\n",
    "rejected_index = []\n",
    "desired_neuron = 1011\n",
    "rates_perAngle = np.zeros_like(target_angles)\n",
    "for j, neuron in enumerate (neurons):\n",
    "    for i in range(len(rates_perAngle)):\n",
    "        rates_perAngle[i] = mean_matrix[i][1][j]\n",
    "\n",
    "    # Get the maximum and minimum values\n",
    "    max_value = max(rates_perAngle)\n",
    "    min_value = min(rates_perAngle)\n",
    "    depth = (max_value - min_value)/max_value\n",
    "    max_index = np.argmax(rates_perAngle)\n",
    "    prefered_angle = target_angles[max_index]\n",
    "    \n",
    "    if(neuron == desired_neuron):\n",
    "        plt.plot(target_angles, rates_perAngle, marker='o', linestyle='-')\n",
    "        plt.xlabel('Target Angles')\n",
    "        plt.ylabel('Firing Rate')\n",
    "        plt.title(f'Tuning Curve of Neuron {desired_neuron}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        print(\"Maximum value:\", max_value)\n",
    "        print(\"Minimum value:\", min_value)\n",
    "        print(\"Depth:\", depth)\n",
    "        \n",
    "    if(depth < 0.7):\n",
    "        rejected_neurons += [neuron]\n",
    "        rejected_index += [j]\n",
    "\n",
    "#Removes those neurons from the decoder\n",
    "old_decoder = decoder\n",
    "decoder = []\n",
    "for i in range(len(old_decoder)):\n",
    "    if(old_decoder[i][0] not in rejected_neurons): decoder += [old_decoder[i]]\n",
    "\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(smallDS, 'val')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset,decoder, 35, small_neurons, removed_neurons=rejected_neurons, distance_coef=50, pred_pos=True)\n",
    "\n",
    "#Plots Cross_Correlation\n",
    "plot_crossCor(val_dataset)\n",
    "\n",
    "#Plots all trial predictions\n",
    "repeated_trials = np.asarray(val_dataset['trial_id'])\n",
    "trials = []\n",
    "for i in range(len(val_dataset)):\n",
    "    if(repeated_trials[i] not in trials): trials += [repeated_trials[i]]\n",
    "\n",
    "plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b144291-714f-4925-9206-16ba4f8d38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = trial_crossCorrelation(val_dataset, 0, time_window=400, plot=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436068c1-2e26-4082-8eae-5614016301bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the dataset for only one trial\n",
    "def trial_datasetMaker(dataset, trials):\n",
    "    trials_possible = trials_present(dataset)\n",
    "    check_mask = np.isin(trials, trials_possible)\n",
    "\n",
    "    if(check_mask.all()):\n",
    "        size = len(dataset)\n",
    "        lines_toRemove = []\n",
    "        for i in range(size):\n",
    "            if(dataset.loc[i, 'trial_id'][0] not in trials): lines_toRemove += [i]\n",
    "        new_ds = dataset.drop(lines_toRemove)\n",
    "        return new_ds\n",
    "    else:\n",
    "        for i in range(len(check_mask)):\n",
    "            if(not check_mask[i]): print('Trial ', trials[i], ' not in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f5f66-5416-45eb-8dd1-62df5d1954d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#trial_datasetMaker(val_dataset, [3])\n",
    "trials_present(val_dataset)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time-start_time\n",
    "\n",
    "print(f'It took: {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108ff57-6803-4e43-a8df-828d14364fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the trials present in one dataset\n",
    "def trials_present(dataset):\n",
    "    size = len(dataset)\n",
    "    trials = []\n",
    "    for i in range(size):\n",
    "        if(dataset.loc[i, 'trial_id'][0] not in trials): trials += [dataset.loc[i, 'trial_id'][0]]\n",
    "    return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012bdbf5-4ccf-465c-9b85-0cc88f16996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = trials_present(val_dataset)\n",
    "nr_cols = 4\n",
    "save = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b1e69-1e8f-4b74-801d-1d5ddee98854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ccSeperate(val_dataset, trials_present(val_dataset), nr_cols=4, save=True, compare_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866132a-4c0d-47fe-8548-276ff140facf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3º Meeting - Improve Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e673b-b388-47af-b63b-8464b9f05d18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot-Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ac234-0e23-4f70-8616-706564e860f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the cross-correlation\n",
    "'''\n",
    "def plot_crossCorrelation(cross_corr, lags, trial, compare_axis=False):\n",
    "    plt.plot(lags, cross_corr)\n",
    "    plt.xlabel('Lag (ms)')\n",
    "    plt.ylabel('Pearsons Coefficient')\n",
    "    plt.title(f'Cross-correlation of Trial {trial} ({metric[0]})')\n",
    "    if(compare_axis): plt.ylim(-1.1,1.1)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the lag at which cross-correlation is maximum\n",
    "    max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "    print(\"Maximum cross-correlation at lag: \", max_lag)\n",
    "    print(\"Maximum cross-correlation of: \", max_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d62af8-8e05-43a2-a08c-5954e805cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the cross-correlation for each trial in a dataset in different subplots. It also shows the point of higher \n",
    "absolute pearson's value\n",
    "\n",
    "Parameteres\n",
    "-----------\n",
    "dataset - pandas.Dataframe\n",
    "    The required dataset with at least two columns named and with the specified metrics\n",
    "trials - array_like\n",
    "    The list of trials to be plotted (must be present in the dataset)\n",
    "metric - array_like of strings\n",
    "    The list with the column name of actual metric and the predicted metric\n",
    "correlation - string, optional\n",
    "    Define the correlation metric applied\n",
    "    - 'cross' (default)\n",
    "        Uses the normalized cross correlation\n",
    "    - 'pearson'\n",
    "        Uses the pearson correlation\n",
    "window_size - integer, optional\n",
    "    The size of the visualization window in the x axis (-window_size+1, window_size)\n",
    "bin_size - float, optional\n",
    "    Specifies what is the size of each bin in ms\n",
    "nr_cols - int, optional\n",
    "    Specifies the number of columns for the grid where all the plots will sit on\n",
    "compare_axis - bool, optional\n",
    "    If true the y axis will have the full correlation scope (-1 - margin, 1 + margin) \n",
    "save - bool, optional\n",
    "    If true the plot will be saved in the same folder\n",
    "save_name - string, optional\n",
    "    If save true will define the file name and type\n",
    "'''\n",
    "def plot_ccSeperate(dataset, listings, metric, correlation='cross', window_size=400, bin_size=5, nr_cols=4, compare_axis=False, save=False, \n",
    "                    save_name='all_corrs.png'):\n",
    "    #Prepares subplot organization\n",
    "    nr_plots = len(listings)\n",
    "    nr_rows = nr_plots//nr_cols\n",
    "    if(nr_plots%nr_cols == 0): extra_row = 0\n",
    "    else: extra_row = 1\n",
    "    \n",
    "    #Creates the subplots accordingly\n",
    "    fig, axes = plt.subplots(nr_rows+extra_row, nr_cols, figsize=(12, (nr_rows+extra_row)*3))\n",
    "        \n",
    "    #Plots the data\n",
    "    for i in range(nr_rows+extra_row):\n",
    "        if((extra_row == 1 and i < nr_rows+extra_row-1) or extra_row == 0):\n",
    "            for j in range(nr_cols):\n",
    "                cross_corr, lags = trial_crossCorrelation(dataset, listings[i*nr_cols + j], metric, correlation=correlation, \n",
    "                                                          time_window=window_size, plot=False)\n",
    "                max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "                \n",
    "                axes[i,j].plot(lags, cross_corr)\n",
    "                axes[i,j].scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "                axes[i,j].set_xlabel('Lag (ms)')\n",
    "                axes[i,j].grid(True)\n",
    "                if(correlation == 'pearson'): axes[i,j].set_ylabel('Pearson\\'s Correlation')   \n",
    "                elif(correlation == 'cross'): axes[i,j].set_ylabel('Cross-Correlation')\n",
    "                axes[i,j].set_title(f'Trial {listings[i*nr_cols + j]}', fontsize=10)\n",
    "                axes[i,j].legend(loc='lower left')\n",
    "                if(compare_axis): axes[i,j].set_ylim(-1.1, 1.1)\n",
    "        elif(extra_row == 1 and i == nr_rows+extra_row-1):\n",
    "            for j in range(nr_plots%nr_cols):\n",
    "                cross_corr, lags = trial_crossCorrelation(dataset, listings[i*nr_cols + j], metric, correlation=correlation, \n",
    "                                                          time_window=window_size, plot=False)\n",
    "                max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "            \n",
    "                axes[i,j].plot(lags, cross_corr)\n",
    "                axes[i,j].scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "                axes[i,j].set_xlabel('Lag (ms)')\n",
    "                if(correlation == 'pearson'): axes[i,j].set_ylabel('Pearson\\'s Correlation')   \n",
    "                elif(correlation == 'cross'): axes[i,j].set_ylabel('Cross-Correlation')\n",
    "                axes[i,j].set_title(f'Trial {listings[i*nr_cols + j]}', fontsize=10)\n",
    "                axes[i,j].grid(True)\n",
    "                axes[i,j].legend(loc='lower left')\n",
    "                if(compare_axis): axes[i,j].set_ylim(-1.1, 1.1)\n",
    "    if(correlation == 'pearson'): plt.suptitle(f'Pearson\\'s Correlation for All Trials ({metric[0]})', y=0.995)\n",
    "    elif(correlation == 'cross'): plt.suptitle(f'Cross-Correlation for All Trials ({metric[0]})', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105fee4-a5b0-4bfd-9275-d34699c4215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the correlation for each trial in a dataset in the same plot.\n",
    "\n",
    "Parameteres\n",
    "-----------\n",
    "dataset - pandas.Dataframe\n",
    "    The required dataset with at least two columns named and with the specified metrics\n",
    "trials - array_like\n",
    "    The list of trials to be plotted (must be present in the dataset)\n",
    "metric - array_like of strings\n",
    "    The list with the column name of actual metric and the predicted metric\n",
    "window_size - integer, optional\n",
    "    The size of the visualization window in the x axis (-window_size+1, window_size)\n",
    "correlation - string, optional\n",
    "    Define the correlation metric applied\n",
    "    - 'cross' (default)\n",
    "        Uses the normalized cross correlation\n",
    "    - 'pearson'\n",
    "        Uses the pearson correlation\n",
    "bin_size - float, optional\n",
    "    Specifies what is the size of each bin in ms\n",
    "focus_trial - {None, int, string}, optional\n",
    "    Defines a plotted line that will recieve emphasis\n",
    "    - ''None'' (default)\n",
    "        No trial recieves emphasis\n",
    "    - ''int''\n",
    "        The specified trial recieves emphasis (needs to be in the trials array)\n",
    "    - 'max'\n",
    "        The trial with higher correlation recieves emphasis\n",
    "    - 'min'\n",
    "        The trial with lower correlation recieves emphasis\n",
    "    - 'best'\n",
    "        The trial with the best correlation recieves emphasis (positive or negative)\n",
    "    - 'worst'\n",
    "        The trial with the worst correlation recieves emphasis (closer to 0)\n",
    "    - 'best_lag'\n",
    "        The trial with the lowest lag for the highest correlation recieves emphasis (closer to 0)\n",
    "    - 'worst_lag'\n",
    "        The trial with the highest lag for the highest correlation recieves emphasis (positive or negative)\n",
    "    - 'mean'\n",
    "        A new plot line with the mean and the standard error shaded will recieve emphasis with is maximum correlation \n",
    "        for a given lag pointed out\n",
    "compare_axis - bool, optional\n",
    "    If true the y axis will have the full correlation scope (-1 - margin, 1 + margin) \n",
    "color_template - string, optional\n",
    "    Defines the plt.colormaps used for the plot lines color (uses the full spectrum)\n",
    "save - bool, optional\n",
    "    If true the plot will be saved in the same folder\n",
    "save_name - string, optional\n",
    "    If save true will define the file name and type\n",
    "    \n",
    "'''\n",
    "def plot_ccTogether(dataset, trials, metric, window_size=400, correlation='cross', bin_size=5, focus_trial=None, compare_axis=False, \n",
    "                    color_template='winter', save=False, save_name='all_corrs.png'):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    nr_trials = len(trials)\n",
    "    color_array = np.round(np.linspace(0, 255, nr_trials))\n",
    "    if(nr_trials > 30): no_label = True\n",
    "    else: no_label = False\n",
    "\n",
    "    corr_matrix = []\n",
    "    max_matrix = []\n",
    "    max_correlations = []\n",
    "    max_correlationsAbs = []\n",
    "    max_lags = []\n",
    "    max_lagsAbs = []\n",
    "    for i in range(nr_trials):\n",
    "        cross_corr, lags = trial_crossCorrelation(dataset, trials[i], metric, correlation=correlation, time_window=window_size, \n",
    "                                                  bin_size=bin_size, plot=False)\n",
    "        max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "        max_correlations += [max_corr]\n",
    "        max_correlationsAbs += [np.abs(max_corr)]\n",
    "        max_lags += [max_corr]\n",
    "        max_lagsAbs += [np.abs(max_lag)]\n",
    "        corr_matrix += [[cross_corr, lags]]\n",
    "        max_matrix += [[max_corr, max_lag]]\n",
    "    corr_matrix = np.asarray(corr_matrix, dtype='float64')\n",
    "    \n",
    "    if(focus_trial == 'max'):\n",
    "        focus_trial = trials[np.argmax(max_correlations)]\n",
    "    elif(focus_trial == 'min'):\n",
    "        focus_trial = trials[np.argmin(max_correlations)]\n",
    "    elif(focus_trial == 'best'):\n",
    "        focus_trial = trials[np.argmax(max_correlationsAbs)]\n",
    "    elif(focus_trial == 'worst'):\n",
    "        focus_trial = trials[np.argmin(max_correlationsAbs)]\n",
    "    elif(focus_trial == 'best_lag'):\n",
    "        focus_trial = trials[np.argmin(max_lagsAbs)]\n",
    "    elif(focus_trial == 'worst_lag'):\n",
    "        focus_trial = trials[np.argmax(max_lagsAbs)]\n",
    "        \n",
    "    for i in range(nr_trials):\n",
    "        cross_corr = corr_matrix[i][0]\n",
    "        lags = corr_matrix[i][1]\n",
    "        max_corr = max_matrix[i][0]\n",
    "        max_lag = max_matrix[i][1]\n",
    "        if(focus_trial == None):\n",
    "            if(not no_label): plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), \n",
    "                                       label=f'Trial {trials[i]}', alpha=1)\n",
    "            else: plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=1)\n",
    "        elif(focus_trial == trials[i]):\n",
    "            plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), label=f'Trial {trials[i]}', \n",
    "                     alpha=1)\n",
    "            plt.scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "        else:\n",
    "            if(not no_label): plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), \n",
    "                                       label=f'Trial {trials[i]}', alpha=0.25)\n",
    "            else: plt.plot(lags, cross_corr, color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=0.1)\n",
    "\n",
    "    if(focus_trial == 'mean'):\n",
    "        mean_vector = np.mean(corr_matrix, axis=0)\n",
    "        ste_vector = np.std(corr_matrix, axis=0)\n",
    "        ste_values = ste_vector[0] / np.sqrt(len(ste_vector[0]))\n",
    "        cross_corr = mean_vector[0]\n",
    "        lags = mean_vector[1]\n",
    "        max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "        plt.plot(lags, cross_corr, color='black', label='Mean Corr', alpha=1)\n",
    "        plt.fill_between(lags, cross_corr-ste_values, cross_corr+ste_values, color='black', label='Stand Err', alpha=0.25)\n",
    "        plt.scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "\n",
    "    plt.xlabel('Lag (ms)')\n",
    "    if(correlation == 'cross'):\n",
    "        plt.ylabel('Cross-Correlation')\n",
    "        plt.title(f'Cross-Correlation of All Trials ({metric[0]})')\n",
    "    if(correlation == 'pearson'):\n",
    "        plt.ylabel('Pearson\\'s Correlation')\n",
    "        plt.title(f'Pearson\\'s Correlation of All Trials ({metric[0]})')\n",
    "    if(compare_axis): plt.ylim(-1.1,1.1)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower left')\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122216e-c770-4c73-952a-081f7bc023f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots in the same graph a predicetd metric and the actual metric value throughout time for a single trial\n",
    "'''\n",
    "def plot_metricComparition_single (dataset, trial, metric, bin_size = 5, compare_axis=True):\n",
    "    trial_dataset = trial_datasetMaker(dataset, [trial])\n",
    "    trial_dataset = np.asarray(trial_dataset.loc[:,[metric[0], metric[1]]], dtype='float64')\n",
    "    \n",
    "    actual_angle = trial_dataset[:,0]\n",
    "    pred_angle = trial_dataset[:,1]\n",
    "    trial_time = np.round(np.linspace(0, bin_size*len(actual_angle), len(actual_angle)))\n",
    "    \n",
    "    \n",
    "    plt.plot(trial_time, actual_angle)\n",
    "    plt.plot(trial_time, pred_angle)\n",
    "    plt.xlabel('Duration (ms)')\n",
    "    plt.ylabel('Angle (rads)')\n",
    "    plt.title(f'{metric[0]} and {metric[1]} of Trial {trial}')\n",
    "    if(compare_axis): plt.ylim(-np.pi-0.1,np.pi+0.1)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feba7ce-7893-4942-b78e-be2d39b027b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots for all the trials in listings a parameter and its predicted version (in this case, 'angle', 'cosx' or 'cosy')\n",
    "'''\n",
    "def plot_metricComparition_allS(dataset, listing, metric, bin_size = 5, nr_cols = 4, compare_axis=(-np.pi-1, np.pi +1), save = False, \n",
    "                                save_name = 'test.png'):\n",
    "    #Prepares subplot organization\n",
    "    nr_plots = len(listing)\n",
    "    nr_rows = nr_plots//nr_cols\n",
    "    if(nr_plots%nr_cols == 0): extra_row = 0\n",
    "    else: extra_row = 1\n",
    "    \n",
    "    #Creates the subplots accordingly\n",
    "    fig, axes = plt.subplots(nr_rows+extra_row, nr_cols, figsize=(12, (nr_rows+extra_row)*3))\n",
    "    \n",
    "    #Plots the data\n",
    "    for i in range(nr_rows+extra_row):\n",
    "        if((extra_row == 1 and i < nr_rows+extra_row-1) or extra_row == 0):\n",
    "            for j in range(nr_cols):\n",
    "                trial_dataset = trial_datasetMaker(dataset, [listing[i*nr_cols + j]])\n",
    "                trial_dataset = np.asarray(trial_dataset.loc[:,[metric[0], metric[1]]], dtype='float64')\n",
    "                \n",
    "                actual_angle = trial_dataset[:,0]\n",
    "                pred_angle = trial_dataset[:,1]\n",
    "                trial_time = np.round(np.linspace(0, bin_size*len(actual_angle), len(actual_angle)))\n",
    "                \n",
    "                axes[i,j].plot(trial_time, actual_angle, label=f'{metric[0]}')\n",
    "                axes[i,j].plot(trial_time, pred_angle, label=f'{metric[1]}')\n",
    "                axes[i,j].set_xlabel('Trial Duration (ms)')\n",
    "                axes[i,j].set_ylabel(f'{metric[0]} (rads)')\n",
    "                axes[i,j].grid(True)\n",
    "                k = np.sqrt(np.sum(actual_angle*actual_angle)*np.sum(pred_angle*pred_angle))\n",
    "                cross_corr = round(np.sum(actual_angle*pred_angle)/k,2)\n",
    "                axes[i,j].set_title(f'Trial: {listing[i*nr_cols + j]}, Corr={cross_corr}', fontsize=10)\n",
    "                axes[i,j].legend(loc='upper left')\n",
    "                if(compare_axis != None): axes[i,j].set_ylim(compare_axis)\n",
    "        elif(extra_row == 1 and i == nr_rows+extra_row-1):\n",
    "            for j in range(nr_plots%nr_cols):\n",
    "                trial_dataset = trial_datasetMaker(dataset, [listing[i*nr_cols + j]])\n",
    "                trial_dataset = np.asarray(trial_dataset.loc[:,[metric[0], metric[1]]], dtype='float64')\n",
    "                \n",
    "                actual_angle = trial_dataset[:,0]\n",
    "                pred_angle = trial_dataset[:,1]\n",
    "                trial_time = np.round(np.linspace(0, bin_size*len(actual_angle), len(actual_angle)))\n",
    "            \n",
    "                axes[i,j].plot(trial_time, actual_angle, label=f'{metric[0]}')\n",
    "                axes[i,j].plot(trial_time, pred_angle, label=f'{metric[1]}')\n",
    "                axes[i,j].set_xlabel('Trial Duration (ms)')\n",
    "                axes[i,j].set_ylabel(f'{metric[0]} (rads)')\n",
    "                axes[i,j].grid(True)\n",
    "                k = np.sqrt(np.sum(actual_angle*actual_angle)*np.sum(pred_angle*pred_angle))\n",
    "                cross_corr = round(np.sum(actual_angle*pred_angle)/k,2)\n",
    "                axes[i,j].set_title(f'Trial: {listing[i*nr_cols + j]}, Corr={cross_corr}', fontsize=10)\n",
    "                axes[i,j].legend(loc='upper left')\n",
    "                if(compare_axis != None): axes[i,j].set_ylim(compare_axis)\n",
    "    plt.title(f'{metric[0]} and {metric[1]} for Multiple Trials')\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ce2fd-86d8-4402-a9ab-048564a2df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the tuning curves\n",
    "'''\n",
    "def plot_tuning(dataset, decoder, nr_directions, neurons, nr_cols=5, threshold=0, save=False, save_name='plot.png'):\n",
    "    mean_matrix, predicted_matrix, target_angles = tune_neurons(dataset, decoder, nr_directions, neurons)\n",
    "    max_rate = np.max([np.max(mean_matrix), np.max(predicted_matrix)])+1\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(mean_matrix[0])//nr_cols\n",
    "    else: nr_rows = len(mean_matrix[0])//nr_cols + 1\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*3, nr_rows*3))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i in range(len(neurons)):\n",
    "        ax = axs[i]\n",
    "        observed = mean_matrix[:,i]\n",
    "        predicted = predicted_matrix[i]\n",
    "        height = round(np.max(predicted_matrix[i])-np.min(predicted_matrix[i]),3)\n",
    "        if(height > threshold):\n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve')\n",
    "            ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nHeight: {height}')\n",
    "            ax.set_title(f'Neuron {neurons[i]}', fontsize=10)\n",
    "        else: \n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve', alpha=0.3)\n",
    "            ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nHeight: {height}', alpha=0.3)\n",
    "            ax.set_title(f'Neuron {neurons[i]}', fontsize=10)\n",
    "        ax.set_ylim(0, max_rate)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "        \n",
    "    plt.suptitle(f'Tuning Curves\\n with {nr_directions} angles and threshold at {threshold}', y=1, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c2df6-3c11-43ce-a2ea-5302023db80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the directions of the desired trial (NOT OPTIMIZED)\n",
    "'''\n",
    "def plot_desiredTrajectory(val_dataset, desired_trial):\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    actual_x = []\n",
    "    actual_y = []\n",
    "    for i in range(len(val_dataset)):\n",
    "        if(val_dataset.loc[i, 'trial_id'][0] == desired_trial):\n",
    "            x_coords += [val_dataset.loc[i, 'pred_X'][0]]\n",
    "            y_coords += [val_dataset.loc[i, 'pred_Y'][0]]\n",
    "            actual_x += [val_dataset.loc[i, 'hand_pos']['x']]\n",
    "            actual_y += [val_dataset.loc[i, 'hand_pos']['y']]\n",
    "    colors = np.random.rand(len(y_coords))\n",
    "    plt.plot(x_coords, y_coords, label='Predicted')\n",
    "    #plt.scatter(x_coords, y_coords, label='Predicted', c=colors, cmap='viridis')\n",
    "    plt.plot(actual_x, actual_y, label='Actual')\n",
    "    plt.title(f\"Cursor Trajectory for Trial {desired_trial}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7e133-a124-4406-9154-aa70c2c1b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots all trajectories separate (with trial duration in ms)\n",
    "'''\n",
    "def plot_allTrajectoriesS(dataset, original_dataset, nr_cols=4, save=False, save_name='plot.png'):\n",
    "    #Gets the hand positions (trua and pred)\n",
    "    cursor_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    true_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    predicted_x = np.asarray(dataset['pred_X'], dtype='float64')\n",
    "    predicted_y = np.asarray(dataset['pred_Y'], dtype='float64')\n",
    "    true_x = true_pos[:,0]\n",
    "    true_y = true_pos[:,1]\n",
    "    cursor_x = cursor_pos[:,0]\n",
    "    cursor_y = cursor_pos[:,1]\n",
    "    \n",
    "    #Gets the trials and time\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    mask = np.concatenate(([True], trial_ids[1:] != trial_ids[:-1]))\n",
    "    split_indices = np.where(mask)[0]\n",
    "    align_time = np.asarray(dataset['align_time'])\n",
    "    \n",
    "    #Gets the target positions per trial\n",
    "    trials = np.unique(trial_ids)\n",
    "    target_pos = np.asarray(original_dataset.trial_info[['active_pos_x', 'active_pos_y']], dtype='int64')[trials]\n",
    "    barrier_pos = np.asarray(original_dataset.trial_info['barrier_pos'])[trials]\n",
    "    barrier_lengths = np.array([len(inner_array) for inner_array in barrier_pos])\n",
    "    \n",
    "    \n",
    "    #Splits all arrays acording to the trials\n",
    "    predicted_x = np.split(predicted_x, split_indices)[1:]\n",
    "    predicted_y = np.split(predicted_y, split_indices)[1:]\n",
    "    align_time = np.split(align_time, split_indices)[1:]\n",
    "    true_x = np.split(true_x, split_indices)[1:]\n",
    "    true_y = np.split(true_y, split_indices)[1:]\n",
    "    cursor_x = np.split(cursor_x, split_indices)[1:]\n",
    "    cursor_y = np.split(cursor_y, split_indices)[1:]\n",
    "    \n",
    "    nr_trials = len(trials)\n",
    "    #The ploting starts\n",
    "    if(nr_trials%nr_cols == 0): nr_rows = nr_trials//nr_cols\n",
    "    else: nr_rows = nr_trials//nr_cols + 1\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*4, nr_rows*4))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    #Plotting\n",
    "    for i in range(nr_trials):\n",
    "        ax = axs[i]\n",
    "        x_coords = true_x[i]\n",
    "        y_coords = true_y[i]\n",
    "        x_pred = predicted_x[i]\n",
    "        y_pred = predicted_y[i]\n",
    "        x_cursor = cursor_x[i]\n",
    "        y_cursor = cursor_y[i]\n",
    "        target_x = target_pos[i,0]\n",
    "        target_y = target_pos[i,1]\n",
    "        duration = int(align_time[i][-1])/1000000000\n",
    "        distance = round(np.sqrt(np.power(x_pred[-1]-x_coords[-1],2) + np.power(y_pred[-1]-y_coords[-1],2)),3)\n",
    "        barriers = barrier_pos[i]\n",
    "    \n",
    "        #Gets the limit\n",
    "        x_lim = np.max([np.max(x_coords), np.max(x_pred), np.max(x_cursor), np.abs(np.min(x_coords)), np.abs(np.min(x_pred)), \n",
    "                        np.abs(np.min(x_cursor))])\n",
    "        y_lim = np.max([np.max(y_coords), np.max(y_pred), np.max(y_cursor), np.abs(np.min(y_coords)), np.abs(np.min(y_pred)), \n",
    "                        np.abs(np.min(y_cursor))])\n",
    "        lim_value = np.max([x_lim,y_lim])+15\n",
    "        \n",
    "        #Plots the barriers\n",
    "        for j in range(barrier_lengths[i]):\n",
    "            x, y, half_width, half_height = barrier_pos[i][j]\n",
    "            left = x - half_width\n",
    "            bottom = y - half_height\n",
    "            width = 2 * half_width\n",
    "            height = 2 * half_height\n",
    "            ax.add_patch(plt.Rectangle((left, bottom), width, height, facecolor='gray'))\n",
    "    \n",
    "        ax.plot(x_cursor, y_cursor, label='Cursor Path')\n",
    "        ax.plot(x_pred, y_pred, label='Predicted Path')\n",
    "        ax.plot(x_coords, y_coords, label='True Path')\n",
    "        ax.scatter(target_x, target_y, label='Target Position', color='red')\n",
    "        ax.scatter(x_coords[0], y_coords[0], label='Start Point', color='black')\n",
    "        ax.set_title(f'Trial {trials[i]}\\nLasted {duration} seconds\\nDistance Final: {distance}')\n",
    "        ax.set_xlim(-lim_value, lim_value)\n",
    "        ax.set_ylim(-lim_value, lim_value)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Trial Trajectories', y=1, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e1ed3-eb93-4635-93b9-8cbef4afcd06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d381b-8b9d-4db9-845e-cb33d37196fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximaze_corrParameters(cross_corr, lags):\n",
    "    max_corr_index = np.argmax(cross_corr)\n",
    "    min_corr_index = np.argmin(cross_corr)\n",
    "    max_corr_lag = lags[max_corr_index]\n",
    "    min_corr_lag = lags[min_corr_index]\n",
    "    max_corr = round(cross_corr[max_corr_index],2)\n",
    "    min_corr = round(cross_corr[min_corr_index],2)\n",
    "    if(abs(max_corr)<abs(min_corr)): \n",
    "        max_corr = min_corr\n",
    "        max_corr_lag = min_corr_lag\n",
    "    return max_corr, max_corr_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94aa2d-5e5e-455a-b2cb-7b1950289c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computes and plots the cross-correlation for a given trial in a dataset\n",
    "'''\n",
    "def trial_crossCorrelation (dataset, trial, metric, time_window=150, correlation='cross', bin_size=5, plot=True, compare_axis=False):\n",
    "    bin_window = time_window//bin_size\n",
    "    if(not isinstance(trial, numbers.Integral)):\n",
    "        print('Please input only one trial as an integer')\n",
    "        return\n",
    "\n",
    "    trial_dataset = trial_datasetMaker(dataset, [trial])\n",
    "    predicted = np.asarray(trial_dataset[metric[1]], dtype='float64')\n",
    "    actual = np.asarray(trial_dataset[metric[0]], dtype='float64')\n",
    "    '''\n",
    "    print('Predicted: ', predicted)\n",
    "    print('Actual: ', actual)\n",
    "    print('len of predicted: ', len(predicted))\n",
    "    print('len of actual: ', len(actual))\n",
    "    '''\n",
    "    if(correlation == 'cross'):\n",
    "        k = np.sqrt((np.sum(actual*actual))*(np.sum(predicted*predicted)))\n",
    "        cross_corr = np.correlate(actual, predicted, mode='full')/k\n",
    "    elif(correlation == 'pearson'):\n",
    "        cross_corr = cross_pearson(actual, predicted)\n",
    "    lags = np.arange(-time_window + 1, time_window, bin_size)\n",
    "    cross_corr = cross_corr[len(cross_corr)//2-bin_window:len(cross_corr)//2+bin_window]\n",
    "\n",
    "    if(plot): plot_crossCorrelation(cross_corr, lags, trial, compare_axis=compare_axis)   \n",
    "    return cross_corr, lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd553a31-9492-4999-8865-be86c1b2a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the dataset for only one trial\n",
    "'''\n",
    "def trial_datasetMaker(dataset, trials):\n",
    "    if(len(trials) == 1):\n",
    "        trials_possible = trials_present(dataset)\n",
    "        check_mask = np.isin(trials, trials_possible)[0]\n",
    "    \n",
    "        if(check_mask):\n",
    "            columns = dataset.columns\n",
    "            matrix = np.asarray(dataset)\n",
    "            \n",
    "            trial_ids = np.array(dataset['trial_id'])\n",
    "            mask = trial_ids == trials[0]\n",
    "            matrix = matrix[mask]\n",
    "            trial_dataset = pd.DataFrame(matrix, columns=columns)\n",
    "            return trial_dataset\n",
    "        else:\n",
    "            print('Trial ', trials[0], ' not in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac35212-0a13-40ad-bc1a-6aeded723db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the trials present in one dataset\n",
    "'''\n",
    "def trials_present(dataset):\n",
    "    trial_dataset = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    return np.unique(trial_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc2ad7-db7b-47e3-9cbd-e0207e628856",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the cross-correlation with pearson correlation values\n",
    "'''\n",
    "def cross_pearson(array1, array2):\n",
    "    corr_p = np.zeros(len(array2) + len(array1) - 1)\n",
    "    size = len(array2) - 1\n",
    "    #left pad\n",
    "    for index, i in enumerate(range(size, 0, -1)):\n",
    "        x = np.pad(array2,(i), mode='constant')\n",
    "        x = x[i*2:]\n",
    "        corr_p[index] = np.corrcoef(array1,x)[0,1]\n",
    "    #righ pad\n",
    "    for index, i in enumerate(range(0, size+1)):\n",
    "        index = index+size\n",
    "        x = np.pad(array2,(i), mode='constant')\n",
    "        x = x[:len(x)-i*2]\n",
    "        corr_p[index] = np.corrcoef(array1,x)[0,1]\n",
    "    return corr_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5b7d0-bf66-4aa5-bc47-83b70c9b694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates the Train/Val subset\n",
    "'''\n",
    "def generate_subset(dataset, subset_name, start_time='move_onset_time'):\n",
    "    mask = np.all(dataset.trial_info[['split']] == subset_name, axis=1)\n",
    "    return dataset.make_trial_data(start_field=start_time, end_field='reach_time', ignored_trials=~mask), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb59b91-2214-43ed-bd46-76d2e616c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the firing rate of a dataset's spikes with a specific window size\n",
    "'''\n",
    "def firing_rate(dataset, window_size, bin_time=5, panda=True):\n",
    "    size = len(dataset)\n",
    "    neurons_names = dataset['spikes'].columns\n",
    "    nr_neurons = len(neurons_names)\n",
    "    spikes_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    firing_matrix = []\n",
    "    trials = np.unique(trial_ids)\n",
    "    \n",
    "    for trial in trials:\n",
    "        mask = trial_ids == trial\n",
    "        matrix = spikes_data[mask]\n",
    "        convolved = (convolve2d(matrix, (np.ones((window_size,1))/window_size), mode='same')/5)*1000\n",
    "        firing_matrix.append(convolved)\n",
    "    firing_matrix = np.concatenate(firing_matrix, axis=0)\n",
    "    if(panda): dataset['spikes'] = pd.DataFrame(firing_matrix, columns=np.asarray(neurons_names, dtype='int'))\n",
    "    else: return firing_matrix, trial_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f43be-a9e2-4d5a-ac65-0348a9f72463",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds decoder by fiting the firing rate to cosines. This function does not support delay\n",
    "'''\n",
    "def decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=5):\n",
    "    #Change spikes to firing rates in ms (moving average) ----------- Can be Optimized\n",
    "    rate_data, trials_vector = firing_rate(dataset, window_size, bin_time=bin_time, panda=False)\n",
    "    train_size = len(rate_data)\n",
    "    nr_neurons = len(neurons)\n",
    "    \n",
    "    #Initiates the vectors for calculating the vector M\n",
    "    hand_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    cosines_matrix = np.zeros((train_size, 2))\n",
    "    angle = np.zeros(train_size)\n",
    "    \n",
    "    #Adds the column vectorM with the cosines components to the train_dataset\n",
    "    for i in range(train_size):\n",
    "        if (i == 0 or ((trials_vector[i] != trials_vector[i-1]))):\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        else:\n",
    "            preX = hand_pos[i-1,0]\n",
    "            preY = hand_pos[i-1,1]\n",
    "        x = hand_pos[i,0]\n",
    "        y = hand_pos[i,1]\n",
    "        angle_temp = np.arctan2((y - preY), (x - preX))\n",
    "        cosX = np.cos(angle_temp)\n",
    "        cosY = np.sin(angle_temp)\n",
    "        cosines_matrix[i,0] = cosX\n",
    "        cosines_matrix[i,1] = cosY\n",
    "        angle[i] = angle_temp\n",
    "    \n",
    "    #Trains the decoder fiting the data to a cos\n",
    "    decoder = []\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = rate_data[:,i]\n",
    "        weights = linear_regression_model(cosines_matrix, neuron_data, loss='ridge')\n",
    "        k = np.sqrt(np.power(weights[1],2) + np.power(weights[2],2))\n",
    "        if(k == 0): \n",
    "            print(f'neuron {neurons[i]} did not fire')\n",
    "            cx = 1\n",
    "            cy = 0\n",
    "        else:\n",
    "            cx = weights[1]/k\n",
    "            cy = weights[2]/k\n",
    "        angleC = np.arctan2(cy, cx)\n",
    "        decoder.append([neurons[i], cx, cy, weights[0], angleC, k])\n",
    "    train_dataset['spikes'] = rate_data\n",
    "    train_dataset['angle'] = angle\n",
    "    return np.array(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566088a7-82af-47e4-ab9e-c68e15da2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds decoder by fiting the firing rate to cosines. This function applies delay\n",
    "'''\n",
    "def decoder_cosinesDelay(dataset, window_size, delay, bin_time=5):\n",
    "    #Change spikes to firing rates in ms (moving average)\n",
    "    train_size = len(dataset)\n",
    "    neurons_names = dataset['spikes'].columns\n",
    "    nr_neurons = len(neurons_names)\n",
    "    \n",
    "    spikes_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    hand_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    trials = np.unique(trial_ids)\n",
    "    rate_data = []\n",
    "    cosines_matrix = []\n",
    "    angle = []\n",
    "    \n",
    "    for trial in trials:\n",
    "        mask = trial_ids == trial\n",
    "        trial_spike = spikes_data[mask]\n",
    "        trial_rate = (convolve2d(trial_spike, (np.ones((window_size,1))/window_size), mode='same')/5)*1000\n",
    "        rate_data.append(trial_rate)\n",
    "    \n",
    "        #Gets the delay if needed\n",
    "        trial_pos = hand_pos[mask]\n",
    "        trial_pos = np.pad(trial_pos,(delay), mode='constant')\n",
    "        size = len(trial_pos)\n",
    "        trial_pos = trial_pos[:size-delay*2,1*delay:-1*delay]\n",
    "        \n",
    "        #Gets the data for the cosines\n",
    "        for i in range(len(trial_spike)):\n",
    "            if (i == 0):\n",
    "                preX = 0\n",
    "                preY = 0\n",
    "            else:\n",
    "                preX = trial_pos[i-1,0]\n",
    "                preY = trial_pos[i-1,1]\n",
    "            x = trial_pos[i,0]\n",
    "            y = trial_pos[i,1]\n",
    "            angle_temp = np.arctan2((y - preY), (x - preX))\n",
    "            cosX = np.cos(angle_temp)\n",
    "            cosY = np.sin(angle_temp) \n",
    "            cosines_matrix.append([cosX, cosY])\n",
    "            angle.append(angle_temp)\n",
    "            \n",
    "    rate_data = np.concatenate(rate_data, axis=0)\n",
    "    \n",
    "    #Trains the decoder fiting the data to a cos\n",
    "    decoder = []\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = rate_data[:,i]\n",
    "        weights = linear_regression_model(cosines_matrix, neuron_data, loss='ridge')\n",
    "        k = np.sqrt(np.power(weights[1],2) + np.power(weights[2],2))\n",
    "        if(k == 0): \n",
    "            print(f'neuron {neurons[i]} did not fire')\n",
    "            cx = 1\n",
    "            cy = 0\n",
    "        else:\n",
    "            cx = weights[1]/k\n",
    "            cy = weights[2]/k\n",
    "        angleC = np.arctan2(cy, cx)\n",
    "        decoder.append([neurons_names[i], cx, cy, weights[0], angleC, k])\n",
    "\n",
    "    train_dataset['spikes'] = rate_data\n",
    "    train_dataset['angle'] = angle\n",
    "    return np.array(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b705de8-8a64-4b79-9086-cff3a700eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Produces the tune curve data for a set number of angles for the observed firing rate and for the predicted firing rate\n",
    "'''\n",
    "\n",
    "def tune_neurons(dataset, decoder, nr_directions, neurons):\n",
    "    rate_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    angle_data = np.asarray(dataset['angle'], dtype='float64')\n",
    "    target_angles = np.linspace(-np.pi, np.pi, nr_directions, endpoint=False)\n",
    "    \n",
    "    closest_angle_idxs = np.argmin(np.abs(target_angles[:, np.newaxis] - angle_data), axis=0)\n",
    "    angle_data = target_angles[closest_angle_idxs]\n",
    "    \n",
    "    #Gets the observed curve\n",
    "    mean_matrix = []\n",
    "    for target_angle in (target_angles):\n",
    "        mask = (angle_data == target_angle)\n",
    "        relevant_rates = rate_data[mask]\n",
    "        mean_rate = np.mean(relevant_rates, axis=0)\n",
    "        mean_matrix.append(mean_rate)\n",
    "    mean_matrix = np.asarray(mean_matrix, dtype='float64')\n",
    "    \n",
    "    #Gets the predicted curve\n",
    "    bias = decoder[:, 3]\n",
    "    k = decoder[:, 5]\n",
    "    angle = decoder[:, 4]\n",
    "    prediced_matrix = bias[:, np.newaxis] + k[:, np.newaxis] * np.cos(target_angles - angle[:, np.newaxis])\n",
    "    prediced_matrix = np.asarray(prediced_matrix, dtype='float64')\n",
    "    \n",
    "    return mean_matrix, prediced_matrix, target_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131b801-2243-4776-bf07-cd085ec2ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes the neurons from the decoder and prepares the val_dataset for predictions\n",
    "'''\n",
    "def remove_neurons(data, dataset, neurons, threshold, remove_fromDataset=True):\n",
    "    #Gets the values from the metric used (depth)\n",
    "    max_vector = np.max(data, axis=1)\n",
    "    min_vector = np.min(data, axis=1)\n",
    "    depth_vector = max_vector-min_vector\n",
    "    \n",
    "    #Builds arrays with the rejected and acepted neurons\n",
    "    acepted_neurons = np.where(depth_vector>=threshold, neurons, None)\n",
    "    rejected_neurons = np.where(depth_vector<threshold, neurons, None)\n",
    "    acepted_neurons = np.asarray(np.ma.masked_equal(acepted_neurons, None).compressed(), dtype='int64')\n",
    "    \n",
    "    #Removes from the decoder\n",
    "    new_decoder = decoder[rejected_neurons == None]\n",
    "    \n",
    "    #Removes those columns from the dataset\n",
    "    rejected_neurons = np.asarray(np.ma.masked_equal(rejected_neurons, None).compressed(), dtype='int64')\n",
    "    if(remove_fromDataset):\n",
    "        col_toRemove = np.empty((len(rejected_neurons)), dtype=tuple)\n",
    "        for i, neuron in enumerate(rejected_neurons):\n",
    "            col_toRemove[i] = ('spikes', neuron)\n",
    "        dataset.drop(columns=col_toRemove, inplace=True)\n",
    "    return new_decoder, acepted_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a71126-2605-4ce2-97f3-181239d87d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear regression with given loss function and given regularization that returns the weights\n",
    "'''\n",
    "def linear_regression_model(trainning_var, trainning_out, loss='sse', lambaRidge=2):\n",
    "    #Data preparation\n",
    "    rates = trainning_var\n",
    "    vel = trainning_out\n",
    "    \n",
    "    #Loss Function Selection\n",
    "    if(loss == 'sse'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        penrose = np.linalg.pinv(rates.astype(np.float32))\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "    elif(loss == 'ridge'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        ratesT = np.transpose(rates)\n",
    "        prod = np.matmul(ratesT,rates)\n",
    "        identity = np.identity(len(prod))\n",
    "        helper = prod + lambaRidge * identity\n",
    "        inverse = np.linalg.inv(helper)\n",
    "        penrose = np.matmul(inverse,ratesT)\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f0e70-e402-410d-8372-e0efc8629f65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42761a-8b0f-47ee-801c-23fdb428f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a decoder by fiting the training data to cosines (based in georgopolos et. al)\n",
    "'''\n",
    "def angle_decoder(dataset, window_size, neurons, delay=0, bin_size=5):\n",
    "    if(delay != 0): decoder = decoder_cosinesDelay(dataset, window_size, delay, bin_time=bin_size)\n",
    "    else: decoder = decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=bin_size)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64168736-740f-46e1-9c88-2cde203f7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decodes the hand position (angle, cosines and trajectory) by applying a decoder fitted to the cosines\n",
    "'''\n",
    "\n",
    "def angle_predictor(dataset, decoder, window_size, neurons, bin_time=5, distance_coef=0.05, pred_pos=True):\n",
    "    #Prepares the data\n",
    "    rate_data, trials_ids = firing_rate(dataset, window_size, bin_time=bin_time, panda=False)\n",
    "    val_size = len(dataset)\n",
    "    \n",
    "    #Predicts the direction\n",
    "    individual_predX = (rate_data - decoder[:,3])*decoder[:,1]\n",
    "    individual_predY = (rate_data - decoder[:,3])*decoder[:,2]\n",
    "    pred_cosX = np.sum(individual_predX, axis=1)\n",
    "    pred_cosY = np.sum(individual_predY, axis=1)\n",
    "    pred_angle = np.arctan2(pred_cosY, pred_cosX)\n",
    "    pred_magnitude = np.sqrt(np.power(pred_cosX,2) + np.power(pred_cosY,2))\n",
    "    #Normalizes the cos and sin\n",
    "    pred_cosX = np.cos(pred_angle)\n",
    "    pred_cosY = np.sin(pred_angle)\n",
    "    \n",
    "    #Adds it to the dataset\n",
    "    dataset['pred_mag'] = pred_magnitude\n",
    "    dataset['pred_cosX'] = pred_cosX\n",
    "    dataset['pred_cosY'] = pred_cosY\n",
    "    dataset['pred_angle'] = pred_angle\n",
    "    dataset['spikes'] = rate_data\n",
    "    \n",
    "    #Builds the vectorM for comparition\n",
    "    trial_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    mask = np.concatenate(([True], trials_ids[1:] != trials_ids[:-1]))\n",
    "    diffs = trial_pos - np.roll(trial_pos, 1, axis=0)\n",
    "    diffs = np.where(mask[:, np.newaxis], np.zeros_like(diffs), diffs)\n",
    "    angles = np.arctan2(diffs[:, 1], diffs[:, 0])\n",
    "    cosines_matrix = np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "    \n",
    "    dataset['mx'] = cosines_matrix[:,0]\n",
    "    dataset['my'] = cosines_matrix[:,1]\n",
    "    dataset['angle'] = angles\n",
    "    \n",
    "    #Predicts the position if needed\n",
    "    if(pred_pos):\n",
    "        pred_cosNX = np.roll(pred_cosX, 1)\n",
    "        pred_cosNY = np.roll(pred_cosY, 1)\n",
    "        pred_magnitude = np.roll(pred_magnitude, 1)\n",
    "        pred_x = np.where(mask, trial_pos[:,0], pred_cosNX * pred_magnitude * distance_coef)\n",
    "        pred_y = np.where(mask, trial_pos[:,1], pred_cosNY * pred_magnitude * distance_coef)\n",
    "    \n",
    "        for i in range(len(pred_x)):\n",
    "            if(not mask[i]): \n",
    "                pred_x[i] = pred_x[i-1] + pred_x[i]\n",
    "                pred_y[i] = pred_y[i-1] + pred_y[i]\n",
    "        \n",
    "        dataset['pred_X'] = pred_x\n",
    "        dataset['pred_Y'] = pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bac12c-4bf2-4913-bfed-3e05c495c8fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Crafting Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e79ec-83b6-4302-b308-3d26c3af2c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosines angles (paper)\n",
    "Moving Average Size: 35\n",
    "Time Required: 1 sec\n",
    "'''\n",
    "dataset = smallDS\n",
    "neurons = small_neurons\n",
    "plot_correlation = True\n",
    "\n",
    "#Creates the training data\n",
    "train_dataset, mask = generate_subset(dataset, 'train')\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "start_time = time.time()\n",
    "decoder = angle_decoder(train_dataset, 35, neurons)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'Decoding took {elapsed_time} sec')\n",
    "\n",
    "#Creates the validation data\n",
    "val_dataset, mask = generate_subset(dataset, 'val')\n",
    "\n",
    "#Runs the tuning curve for every neuron\n",
    "start_time = time.time()\n",
    "_, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 360, neurons)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'Tuning curve took {elapsed_time} sec')\n",
    "\n",
    "#Removes unecessary neurons\n",
    "start_time = time.time()\n",
    "decoder, neurons = remove_neurons(predicted_matrix, val_dataset, neurons, 0)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'Neuron removal took {elapsed_time} sec')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "start_time = time.time()\n",
    "angle_predictor(val_dataset, decoder, 35, neurons, distance_coef=0.02)\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'Angle predictor took {elapsed_time} sec')\n",
    "\n",
    "#Plots Correlation\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "file_names = ['mean_pearson_all_cosx_standardDS_removed1.png','mean_pearson_all_cosy_standardDS_removed1.png',\n",
    "              'mean_pearson_all_angle_standardDS_removed1.png']\n",
    "trials = trials_present(val_dataset)\n",
    "if(plot_correlation):\n",
    "    for i in range(len(metrics)):\n",
    "        plot_ccTogether(val_dataset, trials , metrics[i], focus_trial='mean', correlation='pearson', window_size=150, \n",
    "                        compare_axis=True, color_template='twilight', save=True, save_name=file_names[i])\n",
    "\n",
    "#plot_allPredictions(val_dataset, trials, nr_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe95678-882a-4252-a522-749b68f5e4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [['angle', 'pred_angle'],['my', 'pred_cosY'],['mx', 'pred_cosX']]\n",
    "file_names = ['pearson_angle_smallDS.png','pearson_cosy_smallDS.png','pearson_cosx_smallDS.png']\n",
    "#metric = ['angle', 'pred_angle']\n",
    "#metric = ['my', 'pred_cosY']\n",
    "#metric = ['mx', 'pred_cosX']\n",
    "\n",
    "for i in range(len(metrics)):\n",
    "    plot_ccSeperate(val_dataset, trials, metrics[i], correlation='pearson', window_size=200, \n",
    "                compare_axis=True, save=False, save_name=file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d1d5f-372d-441a-aecf-c9c22285b6d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#metric = ['angle', 'pred_angle']\n",
    "#metric = ['my', 'pred_cosY']\n",
    "metric = ['mx', 'pred_cosX']\n",
    "#compare_axis=(-np.pi-1, np.pi +1)\n",
    "compare_axis=(-1.1, 1.1)\n",
    "\n",
    "plot_metricComparition_allS(val_dataset, trials, metric, compare_axis=compare_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e0082-5b04-4a99-8e7f-ffe57f5f5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(mean_matrix[0])//nr_cols\n",
    "else: nr_rows = len(mean_matrix[0])//nr_cols + 1\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*3, nr_rows*3))\n",
    "\n",
    "# Flatten the axs array for easier iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each array\n",
    "for i in range(len(neurons)):\n",
    "    ax = axs[i]\n",
    "    observed = mean_matrix[:,i]\n",
    "    predicted = predicted_matrix[i]\n",
    "    height = round(np.max(predicted_matrix[i])-np.min(predicted_matrix[i]),3)\n",
    "    if(height > threshold):\n",
    "        ax.plot(target_angles, observed, label='Observed Tuning Curve')\n",
    "        ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nHeight: {height}')\n",
    "        ax.set_title(f'Neuron {neurons[i]}', fontsize=10)\n",
    "    else: \n",
    "        ax.plot(target_angles, observed, label='Observed Tuning Curve', alpha=0.3)\n",
    "        ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nHeight: {height}', alpha=0.3)\n",
    "        ax.set_title(f'Neuron {neurons[i]}', fontsize=10)\n",
    "    ax.set_ylim(0, max_rate)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "    \n",
    "plt.suptitle(f'Tuning Curves\\n with {nr_directions} angles and threshold at {threshold}', y=1, fontsize=15)\n",
    "plt.tight_layout()\n",
    "if(save): plt.savefig(save_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49036c03-b76c-4796-83b9-d15761bdce38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "plot_allTrajectoriesS(val_dataset, standardDS, nr_cols=24, save=True, save_name='standardDS_trajectories.png')\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c06482-71f3-4f63-b296-9124b384c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Computes and plots the cross-correlation for a given trial in a dataset\n",
    "'''\n",
    "def trial_crossCorrelation (dataset, trial, metric, time_window=150, correlation='cross', bin_size=5, plot=True, compare_axis=False):\n",
    "    bin_window = time_window//bin_size\n",
    "    if(not isinstance(trial, numbers.Integral)):\n",
    "        print('Please input only one trial as an integer')\n",
    "        return\n",
    "\n",
    "    trial_dataset = trial_datasetMaker(dataset, [trial])\n",
    "    predicted = np.asarray(trial_dataset[metric[1]], dtype='float64')\n",
    "    actual = np.asarray(trial_dataset[metric[0]], dtype='float64')\n",
    "    '''\n",
    "    print('Predicted: ', predicted)\n",
    "    print('Actual: ', actual)\n",
    "    print('len of predicted: ', len(predicted))\n",
    "    print('len of actual: ', len(actual))\n",
    "    '''\n",
    "    if(correlation == 'cross'):\n",
    "        k = np.sqrt((np.sum(actual*actual))*(np.sum(predicted*predicted)))\n",
    "        cross_corr = np.correlate(actual, predicted, mode='full')/k\n",
    "    elif(correlation == 'pearson'):\n",
    "        cross_corr = cross_pearson(actual, predicted)\n",
    "    lags = np.arange(-time_window + 1, time_window, bin_size)\n",
    "    cross_corr = cross_corr[len(cross_corr)//2-bin_window:len(cross_corr)//2+bin_window]\n",
    "\n",
    "    if(plot): plot_crossCorrelation(cross_corr, lags, trial, compare_axis=compare_axis)\n",
    "    print(len(cross_corr))\n",
    "    return cross_corr, lags\n",
    "\n",
    "'''\n",
    "corr_matrix = []\n",
    "max_matrix = []\n",
    "max_correlations = []\n",
    "max_correlationsAbs = []\n",
    "max_lags = []\n",
    "max_lagsAbs = []\n",
    "for i in range(nr_trials):\n",
    "    cross_corr, lags = trial_crossCorrelation(dataset, trials[i], metric, correlation=correlation, time_window=window_size, \n",
    "                                              bin_size=bin_size, plot=False)\n",
    "    max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "    max_correlations += [max_corr]\n",
    "    max_correlationsAbs += [np.abs(max_corr)]\n",
    "    max_lags += [max_corr]\n",
    "    max_lagsAbs += [np.abs(max_lag)]\n",
    "    corr_matrix += [[cross_corr, lags]]\n",
    "    max_matrix += [[max_corr, max_lag]]\n",
    "corr_matrix = np.asarray(corr_matrix, dtype='float64')\n",
    "if(focus_trial == 'mean'):\n",
    "    mean_vector = np.mean(corr_matrix, axis=0)\n",
    "    ste_vector = np.std(corr_matrix, axis=0)\n",
    "    ste_values = ste_vector[0] / np.sqrt(len(ste_vector[0]))\n",
    "    cross_corr = mean_vector[0]\n",
    "    lags = mean_vector[1]\n",
    "    max_corr, max_lag = maximaze_corrParameters(cross_corr, lags)\n",
    "    plt.plot(lags, cross_corr, color='black', label='Mean Corr', alpha=1)\n",
    "    plt.fill_between(lags, cross_corr-ste_values, cross_corr+ste_values, color='black', label='Stand Err', alpha=0.25)\n",
    "    plt.scatter(max_lag, max_corr, color='red', label=f'Corr: {round(max_corr,2)}\\nLag: {max_lag} ms')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfeaa8c-d0e3-40dd-a8b2-d416dafb9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retruns the mean correlation (cross or pearsons) and the standard error\n",
    "'''\n",
    "\n",
    "def mean_corr(dataset, metric, correlation, time_window=150, bin_size=5):\n",
    "    bin_window = time_window//bin_size\n",
    "    \n",
    "    actual_vector = np.asarray(dataset[metric[0]], dtype='float64')\n",
    "    pred_vector = np.asarray(dataset[metric[1]], dtype='float64')\n",
    "    trial_id = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    trials = np.unique(trial_id)\n",
    "    nr_trials = len(trials)\n",
    "    mask = np.concatenate(([True], trial_id[1:] != trial_id[:-1]))\n",
    "    actual_vector = np.split(actual_vector, np.where(mask)[0])[1:]\n",
    "    pred_vector = np.split(pred_vector, np.where(mask)[0])[1:]\n",
    "    \n",
    "    cross_corr_vector = []\n",
    "    lags = np.arange(-time_window + 1, time_window, bin_size)\n",
    "    for i in range(nr_trials):\n",
    "        if(correlation == 'pearson'):\n",
    "            cross_corr = cross_pearson(actual_vector[i], pred_vector[i])\n",
    "        elif(correlation == 'cross'):\n",
    "            k = np.sqrt((np.sum(np.power(actual_vector[i],2)))*(np.sum(np.power(pred_vector[i],2))))\n",
    "            cross_corr = np.correlate(actual_vector[i], pred_vector[i])/k\n",
    "        cross_corr = cross_corr[len(cross_corr)//2-bin_window:len(cross_corr)//2+bin_window]\n",
    "        cross_corr_vector += [np.array(cross_corr)]\n",
    "    \n",
    "    cross_corr_vector = np.array(cross_corr_vector)\n",
    "    mean_correlation = np.mean(cross_corr_vector, axis=0)\n",
    "    return maximaze_corrParameters(mean_correlation, lags)\n",
    "\n",
    "start_time = time.time()\n",
    "max_corr, max_lag = mean_corr(val_dataset, metric, 'pearson')\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76e86a-66d9-4bee-a1a5-5f5bc873c221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This will run the decoder multiple times and return the different mean correlations and lags\n",
    "'''\n",
    "\n",
    "dataset = smallDS\n",
    "neurons = small_neurons\n",
    "bin_size = 5\n",
    "window_size = [2,5,10,35,50]\n",
    "thresholds = [0.5,1,1.5,2,2.5]\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "correlations = ['pearson']\n",
    "results = []\n",
    "parameters = []\n",
    "\n",
    "for i in window_size:\n",
    "    for j in thresholds:\n",
    "        neurons = small_neurons\n",
    "        #Creates the training data\n",
    "        train_dataset, mask = generate_subset(dataset, 'train')\n",
    "        decoder = angle_decoder(train_dataset, i, neurons)\n",
    "        val_dataset, mask = generate_subset(dataset, 'val')\n",
    "        _, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 360, neurons)\n",
    "        decoder, neurons = remove_neurons(predicted_matrix, val_dataset, neurons, j)\n",
    "        angle_predictor(val_dataset, decoder, i, neurons, distance_coef=0.02, pred_pos=False)\n",
    "        for l in correlations:\n",
    "            for k in metrics:\n",
    "                max_corr, max_lag = mean_corr(val_dataset, k, l)\n",
    "                results += [[max_corr, max_lag]]\n",
    "                parameters += [[i*bin_size, j, k[1], l]]\n",
    "                print(f'Parameteres:\\nwindow_size = {i*bin_size} ms\\nthreshold = {j} depth\\nmetric = {k[1]}\\ncorrelation = {l}')\n",
    "                print('')\n",
    "                print(f'Results:\\nCorrelation = {max_corr}\\nLag = {max_lag}')\n",
    "                print('---------')\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca61acb-5794-4a91-a84d-825fc896e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(results)\n",
    "max_value = np.max(results[:,0])\n",
    "max_pos = np.argmax(results[:,0])\n",
    "\n",
    "print(f'Parameteres:\\nwindow_size = {parameters[max_pos][0]} ms\\nthreshold = {parameters[max_pos][1]} depth\\nmetric = {parameters[max_pos][2]}\\ncorrelation = {parameters[max_pos][3]}')\n",
    "print(f'Result\\nCorrelation: {max_value}\\nLag: {results[max_pos, 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e18c4-851d-4cf9-be04-639dd6ab72c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9c89d-5fc5-44a2-84c4-5108a89fe059",
   "metadata": {},
   "outputs": [],
   "source": [
    "35*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ca594-aa2d-4399-bb0f-ae77681f8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22047b-f4b7-4d6c-8702-4d533d3433cb",
   "metadata": {},
   "source": [
    "## 4º Meeting - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b2a8e-9bb6-4ed6-a2d6-4b7466b7080b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915fe28-5339-44ca-82ab-37b526fc3978",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Tuning Curves Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e675359-907e-4faf-b2c9-2a9c87b15e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the 3d tunning curves data with a given resolution and the predicted fitted curve --- Optimization Needed\n",
    "'''\n",
    "def tune_neurons3D(dataset, model='Gaussian', resolution=50):\n",
    "    #Extracts data\n",
    "    vel_x = np.array(dataset['hand_vel']['x'], dtype='float64')\n",
    "    vel_y = np.array(dataset['hand_vel']['y'], dtype='float64')\n",
    "    rate_data = np.array(dataset['spikes'], dtype='float64')\n",
    "    \n",
    "    #Gets limit points for binarization\n",
    "    max_velX = np.max(vel_x)\n",
    "    max_velY = np.max(vel_y)\n",
    "    min_velX = np.min(vel_x)\n",
    "    min_velY = np.min(vel_y)\n",
    "    maximum = np.max([max_velX, max_velY])\n",
    "    minimum = np.max([min_velX, min_velY])\n",
    "    vels_resolution = np.linspace(minimum, maximum, resolution, endpoint=False)\n",
    "    grid_x, grid_y = np.mgrid[minimum:maximum:resolution*1j, minimum:maximum:resolution*1j]\n",
    "    \n",
    "    #Binarizes the velocities\n",
    "    closest_velX_idxs = np.argmin(np.abs(vels_resolution[:, np.newaxis] - vel_x), axis=0)\n",
    "    closest_velY_idxs = np.argmin(np.abs(vels_resolution[:, np.newaxis] - vel_y), axis=0)\n",
    "    vel_x = vels_resolution[closest_velX_idxs]\n",
    "    vel_y = vels_resolution[closest_velY_idxs]\n",
    "    \n",
    "    grid1, grid2 = np.meshgrid(vels_resolution, vels_resolution)\n",
    "    vels_pairs = np.vstack([grid1.ravel(), grid2.ravel()]).T\n",
    "    velocity_vector = np.column_stack((vel_x, vel_y))\n",
    "    \n",
    "    #Average rates in same velocity quadrant ----- Optimize\n",
    "    mean_matrix = []\n",
    "    found_pairs = []\n",
    "    for pair in vels_pairs:\n",
    "        mask = (pair == velocity_vector)\n",
    "        mask = np.logical_and(mask[:,0], mask[:,1])\n",
    "        if(any(mask)):\n",
    "            relevant_rates = rate_data[mask]\n",
    "            mean_rate = np.mean(relevant_rates, axis=0)\n",
    "            mean_matrix.append(mean_rate)\n",
    "            found_pairs.append(pair)\n",
    "\n",
    "    mean_matrix = np.asarray(mean_matrix, dtype='float64')\n",
    "    found_pairs = np.asarray(found_pairs, dtype='float64')\n",
    "\n",
    "    #Fits a Gaussian Curve ---- Optimize\n",
    "    storage_gridZ = []\n",
    "    predicted = []\n",
    "    if(model == 'Gaussian'):\n",
    "        bound = ([0,np.nanmin(grid_x), np.nanmin(grid_y), 0,0, -np.inf, -1], \n",
    "                         [100,np.nanmax(grid_x), np.nanmax(grid_y), 1000, 1000, np.inf, 100])\n",
    "    elif(model == 'Cauchy'):\n",
    "        bound = ([0,np.nanmin(grid_x), np.nanmin(grid_y), 0, -1], \n",
    "                         [100,np.nanmax(grid_x), np.nanmax(grid_y), 1000, 100])\n",
    "    nr_neurons = len(mean_matrix[0])\n",
    "\n",
    "    #Percentage indicator\n",
    "    tf_cal = False\n",
    "    ft_cal = False\n",
    "    st_cal = False\n",
    "    hd_cal = False\n",
    "\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = mean_matrix[:, i]\n",
    "        grid_z = griddata((found_pairs[:,0], found_pairs[:,1]), neuron_data, (grid_x, grid_y), method='linear')\n",
    "        gz = grid_z.ravel()\n",
    "        total_spikes = np.sum(neuron_data)\n",
    "        \n",
    "        x_mean = np.sum((neuron_data * found_pairs[:,0]))/total_spikes\n",
    "        y_mean = np.sum(neuron_data * found_pairs[:,1])/total_spikes\n",
    "        \n",
    "        N = np.count_nonzero(neuron_data)\n",
    "        if(N == 0): print(f'N equal to 0 at {i}')\n",
    "        if(N == 1): \n",
    "            print(f'N equal to 1 at {i}')\n",
    "            x_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 0] - x_mean)**2) / (((N)*total_spikes)/N))\n",
    "            y_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 1] - y_mean)**2) / (((N)*total_spikes)/N))\n",
    "        else:\n",
    "            x_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 0] - x_mean)**2) / (((N-1)*total_spikes)/N))\n",
    "            y_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 1] - y_mean)**2) / (((N-1)*total_spikes)/N))\n",
    "\n",
    "        mean_std = np.mean([x_std, y_std])\n",
    "\n",
    "        if(model == 'Gaussian'):\n",
    "            initial_guess = (np.nanmax(grid_z),x_mean,y_mean,x_std,y_std,0,np.nanmin(grid_z))\n",
    "    \n",
    "            try:\n",
    "                popt, pcov = curve_fit(ex_twoD_Gaussian, (grid_x.ravel(), grid_y.ravel()), gz.ravel(), \n",
    "                               p0=initial_guess, maxfev=10000, bounds=bound, nan_policy='omit')\n",
    "            except Exception as e:\n",
    "                popt = initial_guess\n",
    "                print(f\"-> Fitting failed: {e}. Returning initial guess.\")\n",
    "        elif(model == 'Cauchy'):\n",
    "            initial_guess = (np.nanmax(grid_z),x_mean,y_mean,mean_std,np.nanmin(grid_z))\n",
    "    \n",
    "            try:\n",
    "                popt, pcov = curve_fit(cauchy_surface, (grid_x.ravel(), grid_y.ravel()), gz.ravel(), \n",
    "                               p0=initial_guess, maxfev=100000, bounds=bound, nan_policy='omit')\n",
    "            except Exception as e:\n",
    "                popt = initial_guess\n",
    "                print(f\"-> Fitting failed: {e}. Returning initial guess.\")\n",
    "            \n",
    "        if(np.round((i+1)/nr_neurons, 2) == 0.25 and not tf_cal): \n",
    "            print('25% Fitted')\n",
    "            tf_cal = True\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 0.5 and not ft_cal): \n",
    "            print('50% Fitted')\n",
    "            ft_cal = True\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 0.75 and not st_cal): \n",
    "            print('75% Fitted')\n",
    "            st_cal = True\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 1 and not hd_cal): \n",
    "            print('100% Fitted')\n",
    "            hd_cal = True\n",
    "        '''\n",
    "        try:\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000)\n",
    "        except RuntimeError or popt[0] > 200:\n",
    "            print(f\"Unbounded fit failed for neuron {i}, trying bounded fit.\")\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000, bounds=bound)\n",
    "        if(popt[0] > 100):\n",
    "            print(f'Wamp Wamp, wrong dimension found for amplitude at Neuron {i}. You got {np.round(popt[0],2)} for an aplitude')\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000, bounds=bound)\n",
    "        '''\n",
    "        if(popt[-1] < 0): \n",
    "            popt = list(popt)\n",
    "            popt[-1] = 0 #Avoids negative values\n",
    "            popt = tuple(popt)\n",
    "        \n",
    "        storage_gridZ.append(grid_z)\n",
    "        predicted.append(popt)\n",
    "        \n",
    "    storage_gridZ = np.asarray(storage_gridZ, dtype='float64')\n",
    "    predicted = np.asarray(predicted, dtype='float64')\n",
    "    grids = [grid_x, grid_y] \n",
    "    observed = storage_gridZ\n",
    "    \n",
    "    return mean_matrix, found_pairs, velocity_vector, grids, observed, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23a606-0b88-4d3b-abf4-941d726b214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian surface generator (used to fit the data)\n",
    "'''\n",
    "def twoD_Gaussian(xy, param):\n",
    "    amplitude, xo, yo, sigma_x, sigma_y, theta, offset = param\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752a94b-895e-4ba6-b514-b2f793138242",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian surface generator (used to fit the data)\n",
    "'''\n",
    "def ex_twoD_Gaussian(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d179e-498e-4f09-923a-4242e385092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian surface generator (used to fit the data)\n",
    "'''\n",
    "def ex_twoD_Gaussian2(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "\n",
    "    a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "    b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "    c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "\n",
    "    g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5033f7-ace2-4194-a4cf-05e849f47162",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorized two_gaussian. This works as well for single input. Migh depecrate the other version\n",
    "'''\n",
    "def vect_twoD_Gaussian(xy, param):\n",
    "    amplitude, xo, yo, sigma_x, sigma_y, theta, offset = param\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude * np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) + c*((y-yo)**2)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bc321-8ba9-4f2f-b709-65adc95ded17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorized two_gaussian. This works as well for single input. Migh depecrate the other version -> 2\n",
    "'''\n",
    "def vect2_twoD_Gaussian(xy, params):\n",
    "    x, y = xy\n",
    "    amplitude, xo, yo, sigma_x, sigma_y, theta, offset = params\n",
    "    #xo = float(xo)\n",
    "    #yo = float(yo)\n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e323d66f-ffdc-4140-9f51-5bdab00ec858",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Bayes Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67161f3b-9159-4a2e-b138-8b49348979cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns all components needed for the Simple Naive Bayses Decoder\n",
    "'''\n",
    "def SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, model='Gaussian', spike_range=None, bin_size=5, window_size=35):\n",
    "    #Unpacking data\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    probability_s = all_prob_output(found_pairs, velocity_vector)\n",
    "    rate_data = np.array(train_dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "    if(spike_range == None): spike_range = np.max(spike_data) + 1\n",
    "    nr_neurons = len(spike_data[0])\n",
    "    \n",
    "    #Gets all neuron probability to all velocities for all spikes count in the train\n",
    "    all_conditional = []\n",
    "    for i, pair in enumerate(found_pairs):\n",
    "        if(model == 'Gaussian'): fs = vect2_twoD_Gaussian(([found_pairs[i,0]], [found_pairs[i,1]]), predicted.T)\n",
    "        elif(model == 'Cauchy'): fs = cauchy_surface(([found_pairs[i,0]], [found_pairs[i,1]]), *predicted.T)\n",
    "        all_store = []\n",
    "        for j in range(nr_neurons):\n",
    "            f = fs[j] * tau\n",
    "            all_prob = []\n",
    "            for k in range(spike_range):\n",
    "                probability = (np.exp(-f)*np.power(f, k))/math.factorial(k)\n",
    "                all_prob.append(probability)\n",
    "            all_store.append(all_prob)\n",
    "        all_conditional.append(all_store)\n",
    "    all_conditional = np.array(all_conditional) #found_pair, neuron, spike_count\n",
    "    return all_conditional, probability_s, found_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca1ae3-bbaf-488c-a4fe-1ea528002c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the Simple Naive Bayes Decoder\n",
    "'''\n",
    "def SNB_apply(dataset, decoder, bin_size=5, window_size=35):\n",
    "    #Data Extraction\n",
    "    all_conditional, probability_s, found_pairs = decoder\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    rate_data = np.array(dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "\n",
    "    #Loop Apply - Tries all s for each entry and returns the most likely -> Can be optimized (parallelization)\n",
    "    max_prob = []\n",
    "    directions = []\n",
    "    i_found_pairs = np.arange(len(found_pairs))\n",
    "    i_neurons = np.arange(len(spike_data[0]))\n",
    "    for k in range(len(spike_data)): #Maybe can be optimized\n",
    "        spikes = spike_data[k]\n",
    "        values = all_conditional[i_found_pairs[:, np.newaxis], i_neurons, spikes]\n",
    "        conditional_vector = np.prod(values, axis=1)\n",
    "        probability_vector = conditional_vector * probability_s\n",
    "        direction_loc = np.argmax(probability_vector)\n",
    "        max_prob.append(probability_vector[direction_loc])\n",
    "        directions.append(found_pairs[direction_loc])\n",
    "    max_prob = np.array(max_prob)\n",
    "    directions = np.array(directions)\n",
    "\n",
    "    return directions, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98633bf-4c1d-4e5b-a8d0-6b248e7bac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removed Applies the Simple Naive Bayes Decoder\n",
    "'''\n",
    "def RNB_apply(dataset, decoder, bin_size=5, window_size=35):\n",
    "    #Data Extraction\n",
    "    all_conditional, probability_s, found_pairs = decoder\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    rate_data = np.array(dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "\n",
    "    #Loop Apply - Tries all s for each entry and returns the most likely -> Can be optimized (parallelization)\n",
    "    max_prob = []\n",
    "    directions = []\n",
    "    i_found_pairs = np.arange(len(found_pairs))\n",
    "    i_neurons = np.arange(len(spike_data[0]))\n",
    "    for k in range(len(spike_data)): #Maybe can be optimized\n",
    "        spikes = spike_data[k]\n",
    "        values = all_conditional[i_found_pairs[:, np.newaxis], i_neurons, spikes]\n",
    "        conditional_vector = np.prod(values, axis=1)\n",
    "        probability_vector = conditional_vector\n",
    "        direction_loc = np.argmax(probability_vector)\n",
    "        max_prob.append(probability_vector[direction_loc])\n",
    "        directions.append(found_pairs[direction_loc])\n",
    "    max_prob = np.array(max_prob)\n",
    "    directions = np.array(directions)\n",
    "\n",
    "    return directions, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09afd3a-d49a-4474-9c92-dbce81b4fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns all components needed for the Naive Bayses Decoder\n",
    "'''\n",
    "def NB_decoder(train_dataset, found_pairs, velocity_vector, predicted, model='Gaussian', spike_range=None, bin_size=5, window_size=35):\n",
    "    #Unpacking data\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    probability_s = all_prob_output(found_pairs, velocity_vector)\n",
    "    rate_data = np.array(train_dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "    if(spike_range == None): spike_range = np.max(spike_data)\n",
    "    nr_neurons = len(spike_data[0])\n",
    "    \n",
    "    #Gets all neuron probability to all velocities for all spikes count in the train\n",
    "    all_conditional = []\n",
    "    for i, pair in enumerate(found_pairs):\n",
    "        if(model == 'Gaussian'): fs = vect2_twoD_Gaussian(([found_pairs[i,0]], [found_pairs[i,1]]), predicted.T)\n",
    "        elif(model == 'Cauchy'): fs = cauchy_surface(([found_pairs[i,0]], [found_pairs[i,1]]), *predicted.T)\n",
    "        all_store = []\n",
    "        for j in range(nr_neurons):\n",
    "            f = fs[j] * tau\n",
    "            all_prob = []\n",
    "            for k in range(spike_range):\n",
    "                probability = (np.exp(-f)*np.power(f, k))/math.factorial(k)\n",
    "                all_prob.append(probability)\n",
    "            all_store.append(all_prob)\n",
    "        all_conditional.append(all_store)\n",
    "    all_conditional = np.array(all_conditional) #found_pair, neuron, spike_count\n",
    "\n",
    "    #Gets the distance probability matrix\n",
    "    std = get_train_std(train_dataset)\n",
    "    probability_dist = prob_distances(found_pairs, std*50)\n",
    "    return all_conditional, probability_s, probability_dist, found_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0c22a-3319-40d0-92c0-17e1f02bb5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the Naive Bayes Decoder\n",
    "'''\n",
    "def NB_apply(dataset, decoder, bin_size=5, window_size=35):\n",
    "    #Data Extraction\n",
    "    all_conditional, probability_s, probability_dist, found_pairs = decoder\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    rate_data = np.array(dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "\n",
    "    #Gets all trial changes boolean\n",
    "    _, first_occurrences = np.unique(trial_ids, return_index=True)\n",
    "    trial_change = np.zeros_like(trial_ids, dtype=bool)\n",
    "    trial_change[first_occurrences] = True\n",
    "    \n",
    "    #Loop Apply - Tries all s for each entry and returns the most likely -> Can be optimized (parallelization)\n",
    "    max_prob = []\n",
    "    directions = []\n",
    "    i_found_pairs = np.arange(len(found_pairs))\n",
    "    i_neurons = np.arange(len(spike_data[0]))\n",
    "    \n",
    "    for k in range(len(spike_data)):\n",
    "        spikes = spike_data[k]\n",
    "        values = all_conditional[i_found_pairs[:, np.newaxis], i_neurons, spikes]\n",
    "        conditional_vector = np.prod(values, axis=1)\n",
    "        if(trial_change[k]): probability_vector = conditional_vector #Firs entry of each trial should not check past\n",
    "        else: \n",
    "            previos_loc = directions[-1]\n",
    "            loc_index = np.where(np.all(found_pairs == directions[-1], axis=1))[0][0]\n",
    "            probability_vector = conditional_vector * probability_dist[loc_index]\n",
    "        direction_loc = np.argmax(probability_vector)\n",
    "        max_prob.append(probability_vector[direction_loc])\n",
    "        directions.append(found_pairs[direction_loc])\n",
    "    \n",
    "    max_prob = np.array(max_prob)\n",
    "    directions = np.array(directions)\n",
    "\n",
    "    return directions, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5955d0-77a7-49a8-9a36-6541ab2ffe27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Naive Bayses Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad92c9-18f1-4348-b8ce-89f3c7114ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates the P(s) -> Probability Distribution of Output variables\n",
    "'''\n",
    "def prob_output(s, found_pairs, velocity_vector):\n",
    "    #Binarize the input into the found_pairs binarization\n",
    "    distances = np.linalg.norm(found_pairs - s, axis=1)\n",
    "    position = np.argmin(distances)\n",
    "    s = found_pairs[position]\n",
    "    \n",
    "    #Count the number of times the found bin appears in velocity_vector\n",
    "    s_count = np.sum((velocity_vector[:,0] == s[0]) & (velocity_vector[:,1] == s[1]))\n",
    "    \n",
    "    #Divide by len of velocity_vector\n",
    "    size = len(velocity_vector)\n",
    "    probability = s_count / size\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b713f2-cc70-4945-b805-72c8fe07db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates the fi(s) of the ith neuron -> Fitted tuning curve value extraction\n",
    "'''\n",
    "def tune_value(predicted, neuron, s):\n",
    "    parameters = predicted[neuron]\n",
    "    f = twoD_Gaussian((s[0], s[1]), parameters)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7ca35-2ac9-4977-a25d-96fac286e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates the P(ri|s) for i neuron -> Probability of a given rate given a certain velocity state based on a 2D Poisson\n",
    "'''\n",
    "def poisson(predicted, neuron, s, spike_count, tau, bin_size=5, window_size=35, print_info=False):\n",
    "    if not isinstance(spike_count, np.integer):\n",
    "        print('Not integer, going to round')\n",
    "        spike_count = np.round(spike_count)\n",
    "    f = tune_value(predicted, neuron, s) * tau\n",
    "    probability = (np.exp(-f)*np.power(f, spike_count))/math.factorial(spike_count)\n",
    "    if(print_info):\n",
    "        print(f'n = {spike_count}')\n",
    "        print(f'f(s) = {f}')\n",
    "        print(f'P(n|s) = {probability}')\n",
    "        print()\n",
    "    return probability[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c683e42-dbe0-43ce-98ed-3b2dc4d45100",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets the full P(r|s) by product -> Independent neurons\n",
    "'''\n",
    "def independent_conditional(predicted, s, spike_vector, print_info=False, bin_size=5, window_size=35):\n",
    "    nr_neurons = len(predicted)\n",
    "    product = 1\n",
    "    tau = bin_size*window_size*0.001\n",
    "    for i in range(nr_neurons):\n",
    "        if(print_info): print(f'Neuron nº {i}')\n",
    "        value = poisson(predicted, i, s, spike_vector[i], tau, print_info=print_info)\n",
    "        product *= value\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9c724-d2df-4f6f-955b-0faf918c3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates all the P(s) for all possible states --- Optimize\n",
    "'''\n",
    "def all_prob_output(found_pairs, velocity_vector):\n",
    "    nr_pairs = len(found_pairs)\n",
    "    probability_vector = []\n",
    "    for i in range(nr_pairs):\n",
    "        s = found_pairs[i]\n",
    "        probability_vector.append(prob_output(s, found_pairs, velocity_vector))\n",
    "    probability_vector = np.array(probability_vector)\n",
    "    \n",
    "    ''' Find why doesnt work\n",
    "    nr_pairs = len(found_pairs)\n",
    "    probability_vector = np.where(found_pairs == found_pairs, prob_output(found_pairs, found_pairs, velocity_vector), 'NA')[:,0]\n",
    "    probability_vector = np.asarray(probability_vector, dtype='float64')\n",
    "    '''\n",
    "    return probability_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866073da-3382-403d-9d5c-141172c0d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fits a gaussian to the distances and visualizes them\n",
    "'''\n",
    "def prob_distances(found_pairs, std, plot=None, cmap='hot', dist_alpha=0, prob_alpha=1, save=False, save_name='plot.png'):\n",
    "    #Get the distances between each pair in a square matrix (found_pairs x found_pairs)\n",
    "    general_dists = squareform(pdist(found_pairs))\n",
    "    extra_prob = norm.pdf(general_dists, 0, std)\n",
    "    \n",
    "    #Ploting if not None\n",
    "    if(plot == '3D'):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        size = len(found_pairs)\n",
    "        grid_x, grid_y = np.mgrid[0:size:size*1j, 0:size:size*1j]\n",
    "        if(dist_alpha != 0):\n",
    "            ax.plot_surface(grid_x, grid_y, general_dists, cmap=cmap, edgecolor='none', alpha=dist_alpha)\n",
    "        ax.plot_surface(grid_x, grid_y, extra_prob, cmap=cmap, edgecolor='none', alpha=prob_alpha)\n",
    "        ax.set_xlabel('Found_Pairs')\n",
    "        ax.set_ylabel('Found_Pairs - 1')\n",
    "        ax.set_zlabel('Distances')\n",
    "        \n",
    "        if(view != None):\n",
    "            ax.view_init(view[0], view[1], view[2])\n",
    "    \n",
    "        plt.title('Distances', fontsize=20)\n",
    "        if(save): plt.savefig(save_name)\n",
    "        plt.show()\n",
    "        \n",
    "    elif(plot == 'heat'):\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot the first heatmap on the left subplot\n",
    "        im1 = axs[0].imshow(extra_prob.T, cmap=cmap, origin='lower')\n",
    "        cb_max1 = np.nanmax(extra_prob)\n",
    "        cb_min1 = np.nanmin(extra_prob)\n",
    "        cb_range1 = np.linspace(cb_min1, cb_max1, 10)\n",
    "        fig.colorbar(im1, ax=axs[0], ticks=cb_range1, label='P(s|s-1)')\n",
    "        axs[0].set_title('Heat Map of Distance Probability', fontsize=20)\n",
    "        axs[0].set_xlabel('Found Pairs')\n",
    "        axs[0].set_ylabel('Found Pairs - 1')\n",
    "        \n",
    "        # Plot the second heatmap on the right subplot\n",
    "        im2 = axs[1].imshow(general_dists.T, cmap=cmap, origin='lower', extent=(0,1790,0,1790))  \n",
    "        cb_max2 = np.nanmax(general_dists)  \n",
    "        cb_min2 = np.nanmin(general_dists)  \n",
    "        cb_range2 = np.linspace(cb_min2, cb_max2, 10)  \n",
    "        fig.colorbar(im2, ax=axs[1], ticks=cb_range2, label='Distance')  \n",
    "        axs[1].set_title('Heat Map of Distances', fontsize=20)\n",
    "        axs[1].set_xlabel('Found Pairs')\n",
    "        axs[1].set_ylabel('Found Pairs - 1')\n",
    "    \n",
    "        #Save plot\n",
    "        if save:\n",
    "            plt.savefig(save_name)\n",
    "        plt.show()\n",
    "    return extra_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc21bd-fb76-40d8-8a04-eba0edc8fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets the distances (euclidean) between all training dataset velocities and their std\n",
    "'''\n",
    "def get_train_std(train_dataset, get_distance=False):\n",
    "    #Data extraction\n",
    "    velocities = np.array(train_dataset['hand_vel'])\n",
    "    delta = velocities[1:] - velocities[:-1]\n",
    "    distances = np.sqrt(np.sum(delta**2, axis=1))\n",
    "    std = np.std(distances)\n",
    "\n",
    "    if(get_distance): return std, distances\n",
    "    else: return std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fb9aa-9dbb-4a63-a6a8-25a715c86a74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579f4c07-d589-4874-bfe8-c478747af620",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts the predicted velocities into coordinates\n",
    "'''\n",
    "def convert_vel2pos(dataset, bin_size=5, save_pandas=True):\n",
    "    #Data Extraction\n",
    "    trials = trials_present(dataset)\n",
    "    all_trials = np.array(dataset['trial_id'])\n",
    "    pred_vel_x = np.array(dataset['pred_vel_X'])\n",
    "    pred_vel_y = np.array(dataset['pred_vel_Y'])\n",
    "    pos_x = np.array(dataset['cursor_pos']['x'])\n",
    "    pos_y = np.array(dataset['cursor_pos']['y'])\n",
    "    \n",
    "    #Data Preparation\n",
    "    single_duration = bin_size * 0.001\n",
    "    trial_mask = np.insert(all_trials[1:] != all_trials[:-1], 0, True)\n",
    "    pred_pos_x = np.zeros_like(pred_vel_x)\n",
    "    pred_pos_y = np.zeros_like(pred_vel_y)\n",
    "    \n",
    "    #Position Calculation\n",
    "    pred_pos_x = np.where(trial_mask, pos_x, 0)\n",
    "    pred_pos_y = np.where(trial_mask, pos_y, 0)\n",
    "    for i in range(len(pred_pos_x)): # Optimize this loop\n",
    "        if(not trial_mask[i]):\n",
    "            position_x = pred_pos_x[i-1]\n",
    "            position_y = pred_pos_y[i-1]\n",
    "            velocity_x = pred_vel_x[i-1]\n",
    "            velocity_y = pred_vel_y[i-1]\n",
    "            new_x = position_x + velocity_x * single_duration\n",
    "            new_y = position_y + velocity_y * single_duration\n",
    "            pred_pos_x[i] = new_x\n",
    "            pred_pos_y[i] = new_y\n",
    "    pred_pos = np.stack((pred_pos_x, pred_pos_y), axis=-1)\n",
    "    \n",
    "    #Pandas Saving (Optional)\n",
    "    if(save_pandas):\n",
    "        apply_2D_data(dataset, pred_pos, col_name='pred')\n",
    "    else: return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282d064-d1e1-4a94-96d4-76baaa9af14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the mean correlation\n",
    "'''\n",
    "def mean_corr(dataset, metric, correlation, time_window=150, bin_size=5):\n",
    "    bin_window = time_window//bin_size\n",
    "    \n",
    "    actual_vector = np.asarray(dataset[metric[0]], dtype='float64')\n",
    "    pred_vector = np.asarray(dataset[metric[1]], dtype='float64')\n",
    "    trial_id = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    trials = np.unique(trial_id)\n",
    "    nr_trials = len(trials)\n",
    "    mask = np.concatenate(([True], trial_id[1:] != trial_id[:-1]))\n",
    "    actual_vector = np.split(actual_vector, np.where(mask)[0])[1:]\n",
    "    pred_vector = np.split(pred_vector, np.where(mask)[0])[1:]\n",
    "    \n",
    "    cross_corr_vector = []\n",
    "    lags = np.arange(-time_window + 1, time_window, bin_size)\n",
    "    for i in range(nr_trials):\n",
    "        if(correlation == 'pearson'):\n",
    "            cross_corr = cross_pearson(actual_vector[i], pred_vector[i])\n",
    "        elif(correlation == 'cross'):\n",
    "            k = np.sqrt((np.sum(np.power(actual_vector[i],2)))*(np.sum(np.power(pred_vector[i],2))))\n",
    "            cross_corr = np.correlate(actual_vector[i], pred_vector[i])/k\n",
    "        cross_corr = cross_corr[len(cross_corr)//2-bin_window:len(cross_corr)//2+bin_window]\n",
    "        cross_corr_vector += [np.array(cross_corr)]\n",
    "    \n",
    "    cross_corr_vector = np.array(cross_corr_vector)\n",
    "    mean_correlation = np.mean(cross_corr_vector, axis=0)\n",
    "    std_corr = np.std(cross_corr_vector, axis=0)\n",
    "    return mean_correlation, std_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbb236-dea0-4a24-a0c4-ceea3a4313a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4896695-cf00-44a3-977b-5014f6d23e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies 2D to the pandas dataset\n",
    "'''\n",
    "def apply_2D_data(dataset, data, col_name='pred_vel'):\n",
    "    dataset[col_name + '_X'] = data[:,0]\n",
    "    dataset[col_name + '_Y'] = data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df550b27-146b-491c-b455-564bed675f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Removes the neurons according to a certain threshold\n",
    "'''\n",
    "def remove_neurons(datasets, neurons, predicted, method, min_depth=None, max_std=None, remove_fromDataset=True):\n",
    "    #Finding the neurons\n",
    "    if(method == 'depth'):\n",
    "        amplitude = predicted[:,0]\n",
    "        mask = np.where(amplitude >= min_depth, True, False)\n",
    "    elif(method == 'std'):\n",
    "        x_std = np.array(predicted[:,3])\n",
    "        y_std = np.array(predicted[:,4])\n",
    "        mean_std = (x_std + y_std)/2\n",
    "        mask = np.where(mean_std <= max_std, True, False)\n",
    "    elif(method == 'depth&std'):\n",
    "        amplitude = predicted[:,0]\n",
    "        x_std = np.array(predicted[:,3])\n",
    "        y_std = np.array(predicted[:,4])\n",
    "        mean_std = (x_std + y_std)/2\n",
    "        \n",
    "        mask1 = np.where(amplitude >= min_depth, True, False)\n",
    "        mask2 = np.where(mean_std <= max_std, True, False)\n",
    "        mask = np.logical_and(mask1, mask2)\n",
    "\n",
    "    #Removing the neurons\n",
    "    neurons = np.array(neurons)\n",
    "    removed_neurons = neurons[~mask]\n",
    "    acepted_neurons = neurons[mask]\n",
    "    acepted_predicted = predicted[mask]\n",
    "    \n",
    "    if(remove_fromDataset):\n",
    "        for j, dataset in enumerate(datasets):\n",
    "            col_toRemove = np.empty((len(removed_neurons)), dtype=tuple)\n",
    "            for i, neuron in enumerate(removed_neurons):\n",
    "                col_toRemove[i] = ('spikes', neuron)\n",
    "            dataset.drop(columns=col_toRemove, inplace=True)\n",
    "    return acepted_neurons, acepted_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c380c69-c6d0-4eca-9053-8207fdc94627",
   "metadata": {},
   "source": [
    "#### Visualize Tuning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b14ae0-6ffa-4922-ac47-ba72a92c8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the 3d tunning curve (heat map) of a given neuron\n",
    "'''\n",
    "def plot_Heatmap_single(mean_matrix, found_pairs, neuron, grids, observed, predicted, cmap='hot', fill_nan=False, plot_predicted=True):\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    grid_x, grid_y = grids\n",
    "    grid_z = observed[neuron]\n",
    "    popt = predicted[neuron]\n",
    "    data_fitted = twoD_Gaussian((grid_x, grid_y), popt)\n",
    "    \n",
    "    if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "    max_locationF = np.nanargmax(grid_z)\n",
    "    max_location = np.unravel_index(max_locationF, grid_z.shape)\n",
    "    \n",
    "    plt.figure(figsize=(20,15))\n",
    "    im = plt.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "    plt.scatter(grid_x[max_location], grid_y[max_location], c='white', linewidths=50, marker='o', alpha=0.7,\n",
    "               label=f'X: {np.round(grid_x[max_location],0)}\\nY: {np.round(grid_y[max_location],0)}')\n",
    "    if(plot_predicted): plt.contour(grid_x, grid_y, data_fitted.reshape(resolution, resolution), 8, colors='w')\n",
    "\n",
    "    cb_max = np.nanmax(grid_z)\n",
    "    cb_range = np.linspace(0, cb_max, 10)\n",
    "    plt.colorbar(im, ticks=cb_range, label='Firing Rate')\n",
    "    plt.title('Heat Map of Neuron Firing Rates')\n",
    "    plt.xlabel('X Velocity')\n",
    "    plt.ylabel('Y Velocity')\n",
    "    #plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa4aa4-71f6-4efa-84b3-b616c0495d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes all the tuning curves 3D (heat map)\n",
    "'''\n",
    "def plot_heatmap_all(mean_matrix, neurons, found_pairs, grids, observed, predicted, nr_cols=5, model='Gaussian', uniform=False, \n",
    "                     cmap='hot', fill_nan=False, save=False, save_name='plot.png'):\n",
    "    max_rate = np.max(mean_matrix)\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(mean_matrix[0])//nr_cols\n",
    "    else: nr_rows = len(mean_matrix[0])//nr_cols + 1\n",
    "        \n",
    "    nr_neurons = len(neurons)\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    grid_x, grid_y = grids\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*5, nr_rows*5))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i in range(nr_neurons):\n",
    "        ax = axs[i]\n",
    "        grid_z = observed[i]\n",
    "        if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "        if(uniform): im = ax.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum), vmin=0, vmax=max_rate)\n",
    "        else: im = ax.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "\n",
    "        popt = predicted[i]\n",
    "        if(model == 'Gaussian'): data_fitted = twoD_Gaussian((grid_x, grid_y), popt)\n",
    "        elif(model == 'Cauchy'): data_fitted = cauchy_surface((grid_x, grid_y), *popt)\n",
    "        ax.contour(grid_x, grid_y, data_fitted.reshape(resolution, resolution), 8, colors='#18E789')\n",
    "        \n",
    "        cb_max = np.nanmax(grid_z)\n",
    "        max_locationF = np.nanargmax(grid_z)\n",
    "        max_location = np.unravel_index(max_locationF, grid_z.shape)\n",
    "        ax.scatter(grid_x[max_location], grid_y[max_location], c='#18E789', linewidths=5, marker='o', alpha=0.7,\n",
    "                   label=f'X: {np.round(grid_x[max_location],0)}\\nY: {np.round(grid_y[max_location],0)}\\nPredicted Peak: {np.round(popt[0], 2)}')\n",
    "        \n",
    "        ax.set_title(f'Neuron {neurons[i]}')\n",
    "        ax.set_xlabel('X Velocity')\n",
    "        ax.set_ylabel('Y Velocity')\n",
    "    \n",
    "        cb_range = np.linspace(0, cb_max, 5)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical', ticks=cb_range)\n",
    "        ax.legend()\n",
    "        \n",
    "    #for ax in fig.get_axes():\n",
    "    #    ax.label_outer()\n",
    "    \n",
    "    \n",
    "    plt.suptitle(f'Tuning Curves\\n in a {resolution}x{resolution} grid ', y=0.999, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5e297-181c-42c9-8c8d-cfea030ed757",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the 3d representation of the tunning curve of  agiven neuron\n",
    "'''\n",
    "def plot_tune3D_single(mean_matrix, found_pairs, neuron, grids, observed, predicted, model='Gaussian',\n",
    "                       view=None, fill_nan=False, view_points=False, observed_alpha=0.05, predicted_alpha=1):\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    length = len(np.unique(found_pairs[:,0]))\n",
    "    neuron_data = mean_matrix[:,neuron]\n",
    "    parameters = predicted[neuron]\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    grid_x, grid_y = grids\n",
    "    grid_z = observed[neuron]\n",
    "    if(model == 'Gaussian'): predicted_data = twoD_Gaussian((grid_x, grid_y), parameters)\n",
    "    elif(model == 'Cauchy'): predicted_data = cauchy_surface((grid_x, grid_y), *parameters)\n",
    "    if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "        \n",
    "    if(view_points): ax.scatter(found_pairs[:,0], found_pairs[:,1], mean_matrix[:,0], marker='o', linewidths=1.5, alpha=0.5)\n",
    "    ax.plot_surface(grid_x, grid_y, predicted_data.reshape(resolution, resolution), cmap='viridis', edgecolor='none', alpha=predicted_alpha)\n",
    "    ax.plot_surface(grid_x, grid_y, grid_z, cmap='hot', edgecolor='none', alpha=observed_alpha)\n",
    "    ax.set_xlabel('Velocity X')\n",
    "    ax.set_ylabel('Velocity Y')\n",
    "    ax.set_zlabel('Firing Rate')\n",
    "    \n",
    "    if(view != None):\n",
    "        ax.view_init(view[0], view[1], view[2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17274f61-2b70-4b0e-9a1d-2ca20b4b2db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "View all predicted and observed tuning curves in 3D\n",
    "'''\n",
    "def plot_tune3D_all(mean_matrix, neurons, found_pairs, grids, observed, predicted, nr_cols=5, uniform=False, cmap='viridis', fill_nan=False, \n",
    "                     observed_alpha=0.05, predicted_alpha=1, model='Gaussian', save=False, save_name='plot.png'):\n",
    "    #Size calculations for subplot\n",
    "    max_rate = np.max(mean_matrix)\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(mean_matrix[0])//nr_cols\n",
    "    else: nr_rows = len(mean_matrix[0])//nr_cols + 1\n",
    "    \n",
    "    #Parameters extraction\n",
    "    nr_neurons = len(neurons)\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    grid_x, grid_y = grids\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig = plt.figure(figsize=(nr_cols*4, nr_rows*4))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    #axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i in range(nr_neurons):\n",
    "        ax = fig.add_subplot(nr_rows, nr_cols, i+1, projection='3d')\n",
    "        grid_z = observed[i]\n",
    "        if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "            \n",
    "        if(uniform): ax.plot_surface(grid_x, grid_y, grid_z, cmap='hot', edgecolor='none', alpha=observed_alpha, vmax=max_rated)\n",
    "        else: ax.plot_surface(grid_x, grid_y, grid_z, cmap='hot', edgecolor='none', alpha=observed_alpha)\n",
    "    \n",
    "        popt = predicted[i]\n",
    "        if(model == 'Gaussian'): predicted_data = twoD_Gaussian((grid_x, grid_y), popt)\n",
    "        elif(model == 'Cauchy'): predicted_data = cauchy_surface((grid_x, grid_y), *popt)\n",
    "        if(uniform): ax.plot_surface(grid_x, grid_y, predicted_data.reshape(resolution, resolution), cmap=cmap, edgecolor='none', \n",
    "                                     alpha=predicted_alpha, vmax=max_rate)\n",
    "        else: ax.plot_surface(grid_x, grid_y, predicted_data.reshape(resolution, resolution), cmap=cmap, edgecolor='none', \n",
    "                                     alpha=predicted_alpha)\n",
    "        \n",
    "        ax.set_title(f'Neuron {neurons[i]}')\n",
    "        ax.set_xlabel('Velocity X')\n",
    "        ax.set_ylabel('Velocity Y')\n",
    "        ax.set_zlabel('Firing Rate')\n",
    "    \n",
    "    plt.suptitle(f'Tuning Curves\\n in a {resolution}x{resolution} 3D grid ', y=0.999, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d54cd1-3355-4086-9a96-a2a3ce80a48c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Visualize Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee72457-7e85-46ed-b6d3-ea18e0da33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the P(s) for all possible states\n",
    "'''\n",
    "def plot_prob_output(probability_vector, found_pairs, grids, fill_nan=False, cmap='hot', logged=True, save=False, save_name='plot.png'):\n",
    "    #Log or not log values\n",
    "    if(logged): logged_prob = np.log(probability_vector)\n",
    "    else: logged_prob = probability_vector\n",
    "        \n",
    "    #Unpack Input\n",
    "    grid_x, grid_y = grids\n",
    "    grid_probability = griddata((found_pairs[:,0], found_pairs[:,1]), logged_prob, (grid_x, grid_y), method='linear')\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    resolution = len(grid_x)\n",
    "    if(fill_nan): grid_probability = np.nan_to_num(grid_probability, nan=np.nanmin(logged_prob))\n",
    "    max_locationF = np.nanargmax(grid_probability)\n",
    "    max_location = np.unravel_index(max_locationF, grid_probability.shape)\n",
    "\n",
    "    #Plotting General\n",
    "    plt.figure(figsize=(20,15))\n",
    "    plt.scatter(grid_x[max_location], grid_y[max_location], c='lightgreen', linewidths=1.5, marker='o', alpha=1,\n",
    "               label=f'X: {np.round(grid_x[max_location],0)}\\nY: {np.round(grid_y[max_location],0)}')\n",
    "    im = plt.imshow(grid_probability.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "\n",
    "    #Plotting Colorbar\n",
    "    cb_max = np.nanmax(grid_probability)\n",
    "    cb_min = np.nanmin(grid_probability)\n",
    "    cb_range = np.linspace(cb_min, cb_max, 10)\n",
    "    if(logged): plt.colorbar(im, ticks=cb_range, label='Log P(s)')\n",
    "    else: plt.colorbar(im, ticks=cb_range, label='P(s)')\n",
    "\n",
    "    #Plotting Titles, etc\n",
    "    if(logged): plt.title('Heat Map of Log P(s)', fontsize=20)\n",
    "    else: plt.title('Heat Map of P(s)', fontsize=20)\n",
    "    plt.xlabel('X Velocity')\n",
    "    plt.ylabel('Y Velocity')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.legend()\n",
    "    \n",
    "    #Plotting Saving\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8c625-026c-47d5-90bb-07654082fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the P(s) for all possible states in 3D\n",
    "'''\n",
    "def plot_prob_output3D(probability_vector, found_pairs, grids, fill_nan=True, cmap='hot', logged=True, view_points=False,\n",
    "                       main_alpha=1, dots_alpha=0.5, save=False, save_name='plot.png'):\n",
    "    #Log or not log values\n",
    "    if(logged): logged_prob = np.log(probability_vector)\n",
    "    else: logged_prob = probability_vector\n",
    "    \n",
    "    grid_x, grid_y = grids\n",
    "    grid_probability = griddata((found_pairs[:,0], found_pairs[:,1]), logged_prob, (grid_x, grid_y), method='linear')\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    resolution = len(grid_x)\n",
    "    if(fill_nan): grid_probability = np.nan_to_num(grid_probability, nan=np.nanmin(logged_prob))\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "\n",
    "        \n",
    "    if(view_points): ax.scatter(found_pairs[:,0], found_pairs[:,1], logged_prob, marker='o', linewidths=1.5, alpha=dots_alpha)\n",
    "    ax.plot_surface(grid_x, grid_y, grid_probability, cmap=cmap, edgecolor='none', alpha=main_alpha)\n",
    "    ax.set_xlabel('Velocity X')\n",
    "    ax.set_ylabel('Velocity Y')\n",
    "    if(logged): ax.set_zlabel('Log P(s)')\n",
    "    else: ax.set_zlabel('P(s)')\n",
    "    \n",
    "    if(view != None):\n",
    "        ax.view_init(view[0], view[1], view[2])\n",
    "\n",
    "    if(logged): plt.title('3D Plot of Log P(s)', fontsize=20)\n",
    "    else: plt.title('3D Plot of P(s)', fontsize=20)\n",
    "\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f96da-4eb8-4c4d-a0f7-5f8a779c1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the spike count distribution\n",
    "'''\n",
    "def plot_spike_count(dataset, bin_size=5, window_size=35, save=False, save_name='plot.png'):\n",
    "    #Data Extraction ≈ 2sec\n",
    "    rate_data = np.array(dataset['spikes'])\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "    unique_spikes, counts = np.unique(spike_data, return_counts=True)\n",
    "    \n",
    "    #Data Visualization\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(unique_spikes, counts, width=0.6, color='white', edgecolor='black')\n",
    "    \n",
    "    plt.xlabel('Spike Counts')\n",
    "    plt.ylabel('Histogram')\n",
    "    plt.title('Histogram of Spike Counts in the Dataset')\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e24fd-4fcc-4387-abae-d3409e1c5b9f",
   "metadata": {},
   "source": [
    "### Crafting Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508365c-ff3a-4aa8-8cdd-38a6fc46cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_sets, val_sets, test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "\n",
    "train_dataset = train_sets[0]\n",
    "val_dataset = val_sets[0]\n",
    "\n",
    "neurons = standard_neurons\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "%time firing_rate(val_dataset, 35, panda=True) \n",
    "%time firing_rate(train_dataset, 35, panda=True) \n",
    "%time firing_rate(test_dataset, 35, panda=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4be41b-b2a4-4aab-990e-b08390c84458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Gaussian Curves to Dataset\n",
    "%time mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bb466-c431-41e0-84ae-26d8e278b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pred_vel, max_prob = SNB_apply(val_dataset, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time apply_2D_data(val_dataset, pred_vel)\n",
    "%time apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time convert_vel2pos(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ebeca4-1064-4eb0-9533-f0d877c477a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(test_dataset)*0.01)/(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3866f-28fe-407d-93a7-8e52d370220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_dataset, val_dataset, test_dataset]\n",
    "%time acepted_neurons, acepted_predicted = remove_neurons(datasets, neurons, predicted, 'depth&std', min_depth=1, max_std=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadfd65-2da8-4678-897c-ba3f8b5b53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time get_train_std(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time prob_distances(found_pairs, get_train_std(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528ad6b-5bbf-4f0a-91c9-a6929b127c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time decoder = NB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30c391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = std_corr_Y\n",
    "deviation = std_std_Y\n",
    "lag = lags\n",
    "model_names = std_thresholds\n",
    "title_name = 'Deviation Threshold Hacking (VelY)'\n",
    "save = False\n",
    "save_name = 'std_thr_hack_velY.png'\n",
    "\n",
    "start_time = time.time()\n",
    "'''\n",
    "Visualizes the different models preformance\n",
    "'''\n",
    "#Data extraction -> then add error\n",
    "nr_models = len(correlation_matrix)\n",
    "max_max = []\n",
    "mm_std = []\n",
    "max_lag = []\n",
    "zero_max = []\n",
    "zm_std = []\n",
    "lag_len = len(lag)\n",
    "for i in range(nr_models):\n",
    "    max_max.append(np.max(correlation_matrix[i]))\n",
    "    max_lag.append(lag[np.argmax(correlation_matrix[i])])\n",
    "    mm_std.append(deviation[i][np.argmax(correlation_matrix[i])])\n",
    "    zero_max.append(correlation_matrix[i][int(lag_len/2)])\n",
    "    zm_std.append(deviation[i][int(lag_len/2)])\n",
    "model_names = np.asarray(model_names, dtype='int')\n",
    "model_names = np.asarray(model_names, dtype='str')\n",
    "model_names = np.insert(model_names, 0, 'Default')\n",
    "results = {\n",
    "    'Absolute Maximum': max_max,\n",
    "    'Maximum at Zero': zero_max\n",
    "}\n",
    "\n",
    "#Plotting\n",
    "model_x = np.arange(len(model_names))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0.5\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "deviation = [mm_std, zm_std]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,6), layout='constrained')\n",
    "\n",
    "for i, (attribute, measurement) in enumerate(results.items()):\n",
    "    offset = width * multiplier\n",
    "    rects = ax.errorbar(model_x + offset, measurement, deviation[i], linewidth=width*5, ls='none', capsize=5, fmt='o', \n",
    "                        label=attribute, color=colors[i])\n",
    "    #ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Pearson')\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_title(title_name)\n",
    "ax.set_xticks(model_x + width, model_names)\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.grid(True)\n",
    "\n",
    "if(save): plt.savefig(save_name)\n",
    "plt.show()\n",
    "\n",
    "elapsed_time = round(time.time() - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16823bb6-c864-4c78-8a83-463cb4811b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xo = predicted[:,1]\n",
    "yo = predicted[:,2]\n",
    "\n",
    "vector = np.column_stack((xo, yo)) / np.linalg.norm(np.column_stack((xo, yo)), axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(np.zeros(len(xo)), np.zeros(len(yo)), vector[:,0], vector[:,1], angles='xy', scale_units='xy', scale=1, color='blue')\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def72346-9bc5-4823-926a-b969ab1f31fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "vector = np.column_stack((xo, yo))\n",
    "plt.scatter(vector[:,0], vector[:,1], color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2b454-ce83-4772-a4d5-15b6d6a510e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))\n",
    "vector = np.column_stack((xo, yo)) / np.linalg.norm(np.column_stack((xo, yo)), axis=1)[:, np.newaxis]\n",
    "plt.scatter(vector[:,0], vector[:,1], linewidths=0.1, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe846a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "'''\n",
    "Divides the original training set into a train set and a validation set\n",
    "'''\n",
    "\n",
    "\n",
    "elapsed_time = round(time.time() - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d7fc8-f1ef-45b1-ae96-5b4a4fd9de67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332b0f2-40d1-4807-842e-4424012237c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_neurons = len(neurons)\n",
    "for i in range(nr_neurons):\n",
    "    if(neurons[i] == 2401): print(i)\n",
    "neurons[128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659ef79-8c32-4707-bdc2-679c5ee8a1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple Bayes Decoder - Parametrization Hacking #1 (Neurons)\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "train_sets, val_sets, otest_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "otrain_dataset = train_sets[0]\n",
    "oval_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(oval_dataset, 35, panda=True) \n",
    "firing_rate(otrain_dataset, 35, panda=True) \n",
    "firing_rate(otest_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(otrain_dataset, 50)\n",
    "print('All neurons have been fitted')\n",
    "lags = np.arange(-150 + 1, 150, 5)\n",
    "\n",
    "#Run Control\n",
    "depth_corr_X = []\n",
    "depth_corr_Y = []\n",
    "depth_std_X = []\n",
    "depth_std_Y = []\n",
    "depth_neurons = []\n",
    "std_corr_X = []\n",
    "std_corr_Y = []\n",
    "std_std_X = []\n",
    "std_std_Y = []\n",
    "std_neurons = []\n",
    "print('Starting Control Model were no neurons are eliminated')\n",
    "train_dataset = otrain_dataset.copy()\n",
    "val_dataset = oval_dataset.copy()\n",
    "test_dataset = otest_dataset.copy()\n",
    "datasets = [train_dataset, val_dataset, test_dataset]\n",
    "\n",
    "#Decoding Moment\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "print('Decoder Applied')\n",
    "#Column Management\n",
    "apply_2D_data(val_dataset, pred_vel)\n",
    "apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "\n",
    "#Correlation calculation\n",
    "mean_corrX, std_corrX = mean_corr(val_dataset, ['vel_X', 'pred_vel_X'], 'pearson')\n",
    "mean_corrY, std_corrY = mean_corr(val_dataset, ['vel_Y', 'pred_vel_Y'], 'pearson')\n",
    "print('Correlation Calculated')\n",
    "print(f'(VelX) Maximum Pearson of {np.round(np.max(np.abs(mean_corrX)),2)} at {lags[np.argmax(np.abs(mean_corrX))]}')\n",
    "print(f'(VelY) Maximum Pearson of {np.round(np.max(np.abs(mean_corrY)),2)} at {lags[np.argmax(np.abs(mean_corrY))]}')\n",
    "\n",
    "#Stores the correlation values\n",
    "depth_corr_X.append(mean_corrX)\n",
    "depth_corr_Y.append(mean_corrY)\n",
    "depth_std_X.append(std_corrX)\n",
    "depth_std_Y.append(std_corrY)\n",
    "depth_neurons.append(neurons)\n",
    "std_corr_X.append(mean_corrX)\n",
    "std_corr_Y.append(mean_corrY)\n",
    "std_std_X.append(std_corrX)\n",
    "std_std_Y.append(std_corrY)\n",
    "std_neurons.append(neurons)\n",
    "\n",
    "print()\n",
    "print('-----------------------------')\n",
    "start_depth = time.time()\n",
    "#Removing Neurons with depth threshold\n",
    "print('Trying Depth Thresholding')\n",
    "depth_thresholds = np.concatenate(([0.01, 0.1 , 0.2, 0.5],np.round(np.linspace(1, 50, 25))))\n",
    "for i, depth in enumerate(depth_thresholds):\n",
    "    print()\n",
    "    print(f'Starting Model were Neurons with depth smaller than: {depth} were eliminated')\n",
    "    #Neuron Removal\n",
    "    train_dataset = otrain_dataset.copy()\n",
    "    val_dataset = oval_dataset.copy()\n",
    "    test_dataset = otest_dataset.copy()\n",
    "    \n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    acepted_neurons, acepted_predicted = remove_neurons(datasets, neurons, predicted, 'depth', min_depth=depth)\n",
    "    print('Neurons Removed')\n",
    "    \n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, acepted_predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(val_dataset, pred_vel)\n",
    "    apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "\n",
    "    #Correlation calculation\n",
    "    mean_corrX, std_corrX = mean_corr(val_dataset, ['vel_X', 'pred_vel_X'], 'pearson')\n",
    "    mean_corrY, std_corrY = mean_corr(val_dataset, ['vel_Y', 'pred_vel_Y'], 'pearson')\n",
    "    print('Correlation Calculated')\n",
    "    print(f'(VelX) Maximum Pearson of {np.round(np.max(np.abs(mean_corrX)),2)} at {lags[np.argmax(np.abs(mean_corrX))]}')\n",
    "    print(f'(VelY) Maximum Pearson of {np.round(np.max(np.abs(mean_corrY)),2)} at {lags[np.argmax(np.abs(mean_corrY))]}')\n",
    "    print(f'Number of Removed Neurons: {len(neurons) - len(acepted_neurons)}')\n",
    "    \n",
    "    #Stores the correlation values\n",
    "    depth_corr_X.append(mean_corrX)\n",
    "    depth_corr_Y.append(mean_corrY)\n",
    "    depth_std_X.append(std_corrX)\n",
    "    depth_std_Y.append(std_corrY)\n",
    "    depth_neurons.append(acepted_neurons)\n",
    "print('Depth Thresholding Done')\n",
    "elapsed_depth = round(time.time() - start_depth,3)\n",
    "print(f'It took {elapsed_depth} sec')\n",
    "print('-------------------------')\n",
    "\n",
    "start_std = time.time()\n",
    "#Removing Neurons with std threshold\n",
    "print('Trying STD Thresholding')\n",
    "std_thresholds = np.round(np.linspace(300, 1000, 29))\n",
    "for i, std in enumerate(std_thresholds):\n",
    "    print()\n",
    "    print(f'Starting Model were Neurons with mean std bigger than: {std} were eliminated')\n",
    "    #Neuron Removal\n",
    "    train_dataset = otrain_dataset.copy()\n",
    "    val_dataset = oval_dataset.copy()\n",
    "    test_dataset = otest_dataset.copy()\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    acepted_neurons, acepted_predicted = remove_neurons(datasets, neurons, predicted, 'std', max_std=std)\n",
    "    print('Neurons Removed')\n",
    "    \n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, acepted_predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(val_dataset, pred_vel)\n",
    "    apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "\n",
    "    #Correlation calculation\n",
    "    mean_corrX, std_corrX = mean_corr(val_dataset, ['vel_X', 'pred_vel_X'], 'pearson')\n",
    "    mean_corrY, std_corrY = mean_corr(val_dataset, ['vel_Y', 'pred_vel_Y'], 'pearson')\n",
    "    print('Correlation Calculated')\n",
    "    print(f'(VelX) Maximum Pearson of {np.round(np.max(np.abs(mean_corrX)),2)} at {lags[np.argmax(np.abs(mean_corrX))]}')\n",
    "    print(f'(VelY) Maximum Pearson of {np.round(np.max(np.abs(mean_corrY)),2)} at {lags[np.argmax(np.abs(mean_corrY))]}')\n",
    "    print(f'Number of Removed Neurons: {len(neurons) - len(acepted_neurons)}')\n",
    "\n",
    "    #Stores the correlation values\n",
    "    std_corr_X.append(mean_corrX)\n",
    "    std_corr_Y.append(mean_corrY)\n",
    "    std_std_X.append(std_corrX)\n",
    "    std_std_Y.append(std_corrY)\n",
    "    std_neurons.append(acepted_neurons)\n",
    "elapsed_std = round(time.time() - start_std,3)\n",
    "print(f'It took {elapsed_std} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbfe77-1335-479f-8b36-d26685e6da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((np.round(np.linspace(300, 750, 10)), np.round(np.linspace(775, 1000, 10))))\n",
    "np.round(np.linspace(300, 1000, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781aa8b0-c716-4fd6-9702-1513090253ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trials = trials_present(val_dataset)\n",
    "%time plot_ccTogether(val_dataset, trials, ['vel_X', 'pred_vel_X'], focus_trial='mean', save=False, save_name='SNB_val_standardDS_velX.png')\n",
    "%time plot_ccTogether(val_dataset, trials, ['vel_Y', 'pred_vel_Y'], focus_trial='mean', save=False, save_name='SNB_val_standardDS_velY.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d78bb-7320-4333-adb7-e4077ed1b97b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_allTrajectoriesS(val_dataset, standardDS, nr_cols=6, save=True, save_name='NB_val_standardDS_traj.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32fcb8-2704-45aa-9293-aeed4083be07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_heatmap_all(mean_matrix, neurons, found_pairs, grids, observed, predicted, nr_cols=6, cmap='hot', fill_nan=True, save=True, save_name='test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0d052-e910-4086-a457-48c9140f4c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_tune3D_all(mean_matrix, standard_neurons, found_pairs, grids, observed, predicted, nr_cols=6, fill_nan=False, save=False, save_name='3d_tuningcurve_standard_scroll50_nanfixed.pdf', observed_alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af972a-97fb-4b77-bdad-ab3dee9c0bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_mode = 0\n",
    "\n",
    "if(view_mode == 0):\n",
    "    view=None\n",
    "elif(view_mode == 1):\n",
    "    view = [0, -90, 0]\n",
    "elif(view_mode == 2):\n",
    "    view = [90, -90, 0]\n",
    "elif(view_mode == 3):\n",
    "    view = [0, 0, 0]\n",
    "\n",
    "\n",
    "%time plot_tune3D_single(mean_matrix, found_pairs, 37, grids, observed, predicted, view, True, observed_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ed9dd-652e-4ae3-b25a-d4508f12952e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_Heatmap_single(mean_matrix, found_pairs, 37, grids, observed, predicted, fill_nan=True, plot_predicted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6143735-e3d5-4cc9-88a7-4714ef9c940b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_prob_output(probability_vector, found_pairs, grids, fill_nan=True, logged=True, cmap='inferno', save=False, save_name='logP(s).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a4ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_prob_output3D(probability_vector, found_pairs, grids, cmap='inferno', fill_nan=False, logged=True, save=False, save_name='logP(s)_3D.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13423d7-9e09-46b5-a0e0-efe277b96837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time plot_spike_count(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b180e-06f5-4bce-a28c-3ad3fd0750de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rate_vector = np.asarray(np.random.rand(nr_neurons,1)*50, dtype='int')[:,0]\n",
    "rate_vector = np.asarray(np.array(train_dataset['spikes'].iloc[0])*35*5*0.001, dtype='int')\n",
    "s = np.array(train_dataset['hand_vel'].iloc[0])\n",
    "#s = [500,500]\n",
    "\n",
    "%time product = independent_conditional(predicted, [500,500], rate_vector, print_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a76596-c751-42dc-8f22-42a9f365d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time poi = poisson(predicted, 1, [500,500], 5.809)\n",
    "poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6477fd-252e-4d1d-891b-42a020d17183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time probability = prob_output(np.array([100,-100]), found_pairs, velocity_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f75b2a-8957-43b4-b234-29b8ae80be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f = tune_value(predicted, 1, [500,500])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f28eab-246f-432d-8055-0d01f4d6446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time probability_vector = all_prob_output(found_pairs, velocity_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c2d83-d42a-47c8-a5a7-df7c12e06485",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Final Results and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375b8df-0745-4b03-a80d-ff8e033b0996",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50164e-958b-4b40-bb04-3625f151e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Shows the population vector\n",
    "'''\n",
    "%time train_sets, val_sets, test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "train_dataset = train_sets[0]\n",
    "%time val_dataset = trial_datasetMaker(test_dataset, [0])\n",
    "\n",
    "neurons = standard_neurons\n",
    "plot_correlation = True\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 35, neurons)\n",
    "\n",
    "#Runs the tuning curve for every neuron\n",
    "_, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 360, neurons)\n",
    "\n",
    "#Removes unecessary neurons\n",
    "#decoder, neurons = remove_neurons(predicted_matrix, val_dataset, neurons, 0)\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "ns = angle_predictor(val_dataset, decoder, 35, neurons, distance_coef=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f60683-b234-43ae-93dd-2f06a4bc7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Builds the Population Vector Plot\n",
    "'''\n",
    "def plot_population_vector(ns, entry=0, trial=0, ratio=1, save=False, save_name='plot.png'):\n",
    "    n_X, n_Y = ns\n",
    "    first_entry = np.stack((n_X[entry], n_Y[entry]), axis=-1)\n",
    "    #size = np.max([np.max(np.abs(first_entry)), np.max(np.abs(pop_vect))]) + 1\n",
    "    size = np.max(np.abs(first_entry)) + 1\n",
    "    pop_vect = np.sum(first_entry, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(5*ratio, 4.5*ratio))\n",
    "    for i in range(len(first_entry)):\n",
    "        plt.quiver(0, 0, first_entry[i, 0], first_entry[i, 1], angles='xy', scale_units='xy', scale=1, width=0.0005)\n",
    "    \n",
    "    plt.quiver(0, 0, pop_vect[0], pop_vect[1], angles='xy', scale_units='xy', color='r', scale=1, width=0.002, \n",
    "               label=f'Population Vector: {np.round(pop_vect,2)}')\n",
    "    \n",
    "    plt.xlim(-size, size)\n",
    "    plt.ylim(-size, size)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title(f'Population Vector for entry {entry} of trial {trial}')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4eafb4-6157-4829-920c-c65cbe9ce891",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_population_vector(ns, 10, ratio=(4/3), save=True, save_name='population_vector_0_10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c2220-bf38-4c08-8435-936948adb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "6/4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92febad2-0ba5-4320-a7de-1a2b735f92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_sets, val_sets, test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "train_dataset = train_sets[0]\n",
    "val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "%time firing_rate(val_dataset, 35, panda=True) \n",
    "%time firing_rate(train_dataset, 35, panda=True) \n",
    "%time firing_rate(test_dataset, 35, panda=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979de50-cfc2-443a-aa51-89328cd9160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the 3d tunning curves data with a given resolution and the predicted fitted curve --- Optimization Needed\n",
    "'''\n",
    "def tune_neurons3D(dataset, resolution=50):\n",
    "    #Extracts data\n",
    "    vel_x = np.array(dataset['hand_vel']['x'], dtype='float64')\n",
    "    vel_y = np.array(dataset['hand_vel']['y'], dtype='float64')\n",
    "    rate_data = np.array(dataset['spikes'], dtype='float64')\n",
    "    \n",
    "    #Gets limit points for binarization\n",
    "    max_velX = np.max(vel_x)\n",
    "    max_velY = np.max(vel_y)\n",
    "    min_velX = np.min(vel_x)\n",
    "    min_velY = np.min(vel_y)\n",
    "    maximum = np.max([max_velX, max_velY])\n",
    "    minimum = np.max([min_velX, min_velY])\n",
    "    vels_resolution = np.linspace(minimum, maximum, resolution, endpoint=False)\n",
    "    grid_x, grid_y = np.mgrid[minimum:maximum:resolution*1j, minimum:maximum:resolution*1j]\n",
    "    \n",
    "    #Binarizes the velocities\n",
    "    closest_velX_idxs = np.argmin(np.abs(vels_resolution[:, np.newaxis] - vel_x), axis=0)\n",
    "    closest_velY_idxs = np.argmin(np.abs(vels_resolution[:, np.newaxis] - vel_y), axis=0)\n",
    "    vel_x = vels_resolution[closest_velX_idxs]\n",
    "    vel_y = vels_resolution[closest_velY_idxs]\n",
    "    \n",
    "    grid1, grid2 = np.meshgrid(vels_resolution, vels_resolution)\n",
    "    vels_pairs = np.vstack([grid1.ravel(), grid2.ravel()]).T\n",
    "    velocity_vector = np.column_stack((vel_x, vel_y))\n",
    "    \n",
    "    #Average rates in same velocity quadrant ----- Optimize\n",
    "    mean_matrix = []\n",
    "    found_pairs = []\n",
    "    for pair in vels_pairs:\n",
    "        mask = (pair == velocity_vector)\n",
    "        mask = np.logical_and(mask[:,0], mask[:,1])\n",
    "        if(any(mask)):\n",
    "            relevant_rates = rate_data[mask]\n",
    "            mean_rate = np.mean(relevant_rates, axis=0)\n",
    "            mean_matrix.append(mean_rate)\n",
    "            found_pairs.append(pair)\n",
    "\n",
    "    mean_matrix = np.asarray(mean_matrix, dtype='float64')\n",
    "    found_pairs = np.asarray(found_pairs, dtype='float64')\n",
    "\n",
    "    #Fits a Gaussian Curve ---- Optimize\n",
    "    storage_gridZ = []\n",
    "    predicted = []\n",
    "    bound = ([0,np.nanmin(grid_x), np.nanmin(grid_y), 0,0, -np.inf, -1], \n",
    "                     [100,np.nanmax(grid_x), np.nanmax(grid_y), 1000, 1000, np.inf, 100])\n",
    "    nr_neurons = len(mean_matrix[0])\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = mean_matrix[:, i]\n",
    "        grid_z = griddata((found_pairs[:,0], found_pairs[:,1]), neuron_data, (grid_x, grid_y), method='linear')\n",
    "        gz = grid_z.ravel()\n",
    "        total_spikes = np.sum(neuron_data)\n",
    "        \n",
    "        x_mean = np.sum((neuron_data * found_pairs[:,0]))/total_spikes\n",
    "        y_mean = np.sum(neuron_data * found_pairs[:,1])/total_spikes\n",
    "        max_locationF = np.nanargmax(grid_z)\n",
    "        max_location = np.unravel_index(max_locationF, grid_z.shape)\n",
    "        x_peak = grid_x[max_location]\n",
    "        y_peak = grid_y[max_location]\n",
    "        \n",
    "        N = np.count_nonzero(neuron_data)\n",
    "        if(N == 0): print(f'N equal to 0 at {i}')\n",
    "        if(N == 1): \n",
    "            print(f'N equal to 1 at {i}')\n",
    "            x_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 0] - x_mean)**2) / (((N)*total_spikes)/N))\n",
    "            y_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 1] - y_mean)**2) / (((N)*total_spikes)/N))\n",
    "        else:\n",
    "            x_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 0] - x_mean)**2) / (((N-1)*total_spikes)/N))\n",
    "            y_std = np.sqrt(np.sum(neuron_data * (found_pairs[:, 1] - y_mean)**2) / (((N-1)*total_spikes)/N))\n",
    "        if(np.isnan(x_std)): x_std = 500\n",
    "        if(np.isnan(y_std)): y_std = 500\n",
    "            \n",
    "        #initial_guess = (np.nanmax(grid_z),x_peak,y_peak,x_std,y_std,0,np.nanmin(grid_z))\n",
    "        initial_guess = (np.nanmax(grid_z),x_mean,y_mean,x_std,y_std,0,np.nanmin(grid_z))\n",
    "        #initial_guess = (np.nanmax(grid_z),0,0,512.5644445100842,483.7434837601396,0,np.nanmin(grid_z))\n",
    "        popt, pcov = curve_fit(ex_twoD_Gaussian, (grid_x.ravel(), grid_y.ravel()), gz, p0=initial_guess, maxfev=5000, bounds=bound, \n",
    "                               nan_policy='omit')\n",
    "        if(np.round((i+1)/nr_neurons, 2) == 0.25): print('25% Fitted')\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 0.5): print('50% Fitted')\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 0.75): print('75% Fitted')\n",
    "        elif(np.round((i+1)/nr_neurons, 2) == 1): print('100% Fitted')\n",
    "        '''\n",
    "        try:\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000)\n",
    "        except RuntimeError or popt[0] > 200:\n",
    "            print(f\"Unbounded fit failed for neuron {i}, trying bounded fit.\")\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000, bounds=bound)\n",
    "        if(popt[0] > 100):\n",
    "            print(f'Wamp Wamp, wrong dimension found for amplitude at Neuron {i}. You got {np.round(popt[0],2)} for an aplitude')\n",
    "            popt, pcov = curve_fit(twoD_Gaussian, (grid_x, grid_y), gz, p0=initial_guess, maxfev=2000, bounds=bound)\n",
    "        '''\n",
    "        if(popt[-1] < 0): popt[-1] = 0 #Avoids negative values\n",
    "        \n",
    "        storage_gridZ.append(grid_z)\n",
    "        predicted.append(popt)\n",
    "        \n",
    "    storage_gridZ = np.asarray(storage_gridZ, dtype='float64')\n",
    "    predicted = np.asarray(predicted, dtype='float64')\n",
    "    grids = [grid_x, grid_y] \n",
    "    observed = storage_gridZ\n",
    "    \n",
    "    return mean_matrix, found_pairs, velocity_vector, grids, observed, predicted\n",
    "\n",
    "'''\n",
    "Gaussian surface generator (used to fit the data)\n",
    "'''\n",
    "def ex_twoD_Gaussian(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()\n",
    "\n",
    "#Fitting Gaussian Curves to Dataset\n",
    "%time mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4342d0-55f6-4902-a4a4-e70adbf98128",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the P(s) for all possible states\n",
    "'''\n",
    "def plot_prob_output(probability_vector, found_pairs, grids, ratio=1, fill_nan=False, cmap='hot', logged=True, save=False, save_name='plot.png'):\n",
    "    #Log or not log values\n",
    "    if(logged): logged_prob = np.log(probability_vector)\n",
    "    else: logged_prob = probability_vector\n",
    "        \n",
    "    #Unpack Input\n",
    "    grid_x, grid_y = grids\n",
    "    grid_probability = griddata((found_pairs[:,0], found_pairs[:,1]), logged_prob, (grid_x, grid_y), method='linear')\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    resolution = len(grid_x)\n",
    "    if(fill_nan): grid_probability = np.nan_to_num(grid_probability, nan=np.nanmin(logged_prob))\n",
    "    max_locationF = np.nanargmax(grid_probability)\n",
    "    max_location = np.unravel_index(max_locationF, grid_probability.shape)\n",
    "\n",
    "    #Plotting General\n",
    "    plt.figure(figsize=(20*ratio,15*ratio))\n",
    "    #plt.scatter(grid_x[max_location], grid_y[max_location], c='lightgreen', linewidths=1.5, marker='o', alpha=1,\n",
    "    #           label=f'X: {np.round(grid_x[max_location],0)}\\nY: {np.round(grid_y[max_location],0)}')\n",
    "    im = plt.imshow(grid_probability.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "\n",
    "    #Plotting Colorbar\n",
    "    cb_max = np.nanmax(grid_probability)\n",
    "    cb_min = np.nanmin(grid_probability)\n",
    "    cb_range = np.linspace(cb_min, cb_max, 10)\n",
    "    if(logged): plt.colorbar(im, ticks=cb_range, label='Log P(s)')\n",
    "    else: plt.colorbar(im, ticks=cb_range, label='P(s)')\n",
    "\n",
    "    #Plotting Titles, etc\n",
    "    if(logged): plt.title('Heat Map of Log P(s)', fontsize=20)\n",
    "    else: plt.title('Heat Map of P(s)', fontsize=20)\n",
    "    plt.xlabel('X Velocity')\n",
    "    plt.ylabel('Y Velocity')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    #plt.legend()\n",
    "    \n",
    "    #Plotting Saving\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((20*ratio,15*ratio))\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Calculates the P(s) -> Probability Distribution of Output variables\n",
    "'''\n",
    "def prob_output(s, found_pairs, velocity_vector):\n",
    "    #Binarize the input into the found_pairs binarization\n",
    "    distances = np.linalg.norm(found_pairs - s, axis=1)\n",
    "    position = np.argmin(distances)\n",
    "    s = found_pairs[position]\n",
    "    \n",
    "    #Count the number of times the found bin appears in velocity_vector\n",
    "    s_count = np.sum((velocity_vector[:,0] == s[0]) & (velocity_vector[:,1] == s[1]))\n",
    "    \n",
    "    #Divide by len of velocity_vector\n",
    "    size = len(velocity_vector)\n",
    "    probability = s_count / size\n",
    "    return probability\n",
    "\n",
    "'''\n",
    "Generates all the P(s) for all possible states --- Optimize\n",
    "'''\n",
    "def all_prob_output(found_pairs, velocity_vector):\n",
    "    nr_pairs = len(found_pairs)\n",
    "    probability_vector = []\n",
    "    for i in range(nr_pairs):\n",
    "        s = found_pairs[i]\n",
    "        probability_vector.append(prob_output(s, found_pairs, velocity_vector))\n",
    "    probability_vector = np.array(probability_vector)\n",
    "    \n",
    "    ''' Find why doesnt work\n",
    "    nr_pairs = len(found_pairs)\n",
    "    probability_vector = np.where(found_pairs == found_pairs, prob_output(found_pairs, found_pairs, velocity_vector), 'NA')[:,0]\n",
    "    probability_vector = np.asarray(probability_vector, dtype='float64')\n",
    "    '''\n",
    "    return probability_vector\n",
    "\n",
    "probability_vector = all_prob_output(found_pairs, velocity_vector)\n",
    "plot_prob_output(probability_vector, found_pairs, grids, ratio=0.4, fill_nan=False, logged=False, cmap='inferno', \n",
    "                 save=True, save_name='prior_s.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309706e-8f68-4fa2-b6fb-3f76123e2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the 3d tunning curve (heat map) of a given neuron\n",
    "'''\n",
    "def plot_Heatmap_single(mean_matrix, found_pairs, neuron, neurons, grids, observed, predicted, cmap='hot', ratio=1, fill_nan=False, \n",
    "                        plot_predicted=True, save=False, save_name='plot.png'):\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    grid_x, grid_y = grids\n",
    "    grid_z = observed[neuron]\n",
    "    popt = predicted[neuron]\n",
    "    data_fitted = twoD_Gaussian((grid_x, grid_y), popt)\n",
    "    \n",
    "    if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "    max_locationF = np.nanargmax(grid_z)\n",
    "    max_location = np.unravel_index(max_locationF, grid_z.shape)\n",
    "    \n",
    "    plt.figure(figsize=(20*ratio,15*ratio))\n",
    "    im = plt.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "    plt.scatter(popt[1], popt[2], c='#18E789', linewidths=1, marker='o', alpha=0.7,\n",
    "               label=f'Predicted Peak\\nX: {np.round(popt[1],0)}\\nY: {np.round(popt[2],0)}\\nAmplitude: {np.round(popt[0], 2)}')\n",
    "    if(plot_predicted): plt.contour(grid_x, grid_y, data_fitted.reshape(resolution, resolution), 8, colors='#18E789')\n",
    "\n",
    "    cb_max = np.nanmax(grid_z)\n",
    "    cb_range = np.linspace(0, cb_max, 8)\n",
    "    plt.colorbar(im, ticks=cb_range, label='Firing Rate (Hz)')\n",
    "    plt.title(f'Heat Map of Neuron {neurons[neuron]} Firing Rates')\n",
    "    plt.xlabel('X Velocity (px/s)')\n",
    "    plt.ylabel('Y Velocity (px/s)')\n",
    "    plt.legend()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((20*ratio, 15*ratio))\n",
    "    plt.show()\n",
    "\n",
    "plot_Heatmap_single(mean_matrix, found_pairs, 75, standard_neurons, grids, observed, predicted, ratio=0.4, \n",
    "                    save=False, save_name='tuning3D_example2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d9250-5808-4194-8cf3-622057175b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes all the tuning curves 3D (heat map)\n",
    "'''\n",
    "def plot_heatmap_all(mean_matrix, neurons, full_neurons, found_pairs, grids, observed, predicted, nr_cols=5, ratio=4, uniform=False, \n",
    "                     cmap='viridis', fill_nan=False, save=False, save_name='plot.png'):\n",
    "    max_rate = np.max(mean_matrix)\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(neurons)//nr_cols\n",
    "    else: nr_rows = len(neurons)//nr_cols + 1\n",
    "        \n",
    "    nr_neurons = len(neurons)\n",
    "    minimum = np.min(found_pairs)\n",
    "    maximum = np.max(found_pairs)\n",
    "    grid_x, grid_y = grids\n",
    "    resolution = len(observed[0])\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*ratio, nr_rows*ratio))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        ax = axs[i]\n",
    "        grid_z = observed[neuron]\n",
    "        if(fill_nan): grid_z = np.nan_to_num(grid_z)\n",
    "        if(uniform): im = ax.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum), vmin=0, vmax=max_rate)\n",
    "        else: im = ax.imshow(grid_z.T, cmap=cmap, origin='lower', extent=(minimum, maximum, minimum, maximum))\n",
    "\n",
    "        popt = predicted[neuron]\n",
    "        data_fitted = twoD_Gaussian((grid_x, grid_y), popt)\n",
    "        ax.contour(grid_x, grid_y, data_fitted.reshape(resolution, resolution), 8, colors='#18E789')\n",
    "        ax.scatter(popt[1], popt[2], c='#18E789', linewidths=0.1, marker='o', alpha=0.7,\n",
    "                   label=f'Predicted Peak\\n     X: {np.round(popt[1],0)}\\n     Y: {np.round(popt[2],0)}\\n     Amplitude: {np.round(popt[0], 2)}')\n",
    "        \n",
    "        cb_max = np.nanmax(grid_z)\n",
    "        max_locationF = np.nanargmax(grid_z)\n",
    "        max_location = np.unravel_index(max_locationF, grid_z.shape)\n",
    "        \n",
    "        if(i==0): ax.set_title(f'Tuning Surface of Neuron {full_neurons[neuron]}')\n",
    "        if(i==1): ax.set_title(f'Fitted Tuning Surface of Neuron {full_neurons[neuron]}')\n",
    "        ax.set_xlabel('X Velocity (px/s)')\n",
    "        ax.set_ylabel('Y Velocity (px/s)')\n",
    "    \n",
    "        cb_range = np.linspace(0, cb_max, 7)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        cbar = fig.colorbar(im, cax=cax, orientation='vertical', ticks=cb_range, label='Firing Rate (Hz)')\n",
    "        if(i==1): ax.legend()\n",
    "        \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "    \n",
    "    \n",
    "    #plt.suptitle(f'Tuning Curves\\n in a {resolution}x{resolution} grid ', y=0.999, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((nr_cols*ratio, nr_rows*ratio))\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "Gaussian surface generator (used to fit the data)\n",
    "'''\n",
    "def twoD_Gaussian(xy, param):\n",
    "    amplitude, xo, yo, sigma_x, sigma_y, theta, offset = param\n",
    "    x, y = xy\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)    \n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo) \n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()\n",
    "\n",
    "plot_heatmap_all(mean_matrix, [70, 75], standard_neurons, found_pairs, grids, observed, predicted, nr_cols=2, ratio=6, cmap='hot', \n",
    "                 fill_nan=False, save=True, save_name='tuning3D_example3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3567bb-1172-4930-91a1-c60c718be337",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_neurons = len(standard_neurons)\n",
    "for i in range(nr_neurons):\n",
    "    if(standard_neurons[i] == 1663): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721f2c5-a117-4de6-a69d-7df09c478fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots the trajectory of one trial (with trial duration in ms)\n",
    "'''\n",
    "def plot_single_traj(original_dataset, trial_id, nr_cols=4, ratio=4, save=False, save_name='plot.png'):\n",
    "    #Gets the trial_set\n",
    "    mask = np.array([False] * len(original_dataset.trial_info))\n",
    "    mask[trial_id] = True\n",
    "    dataset = original_dataset.make_trial_data(start_field='move_onset_time', end_field='reach_time', ignored_trials=~mask)\n",
    "    \n",
    "    #Gets the hand positions (trua and pred)\n",
    "    cursor_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    hand_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    \n",
    "    hand_x = hand_pos[:,0]\n",
    "    hand_y = hand_pos[:,1]\n",
    "    cursor_x = cursor_pos[:,0]\n",
    "    cursor_y = cursor_pos[:,1]\n",
    "    \n",
    "    #Gets the trials and time\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    mask = np.concatenate(([True], trial_ids[1:] != trial_ids[:-1]))\n",
    "    split_indices = np.where(mask)[0]\n",
    "    align_time = np.asarray(dataset['align_time'])\n",
    "    \n",
    "    #Gets the target positions per trial\n",
    "    trials = np.unique(trial_ids)\n",
    "    target_pos = np.asarray(original_dataset.trial_info[['active_pos_x', 'active_pos_y']], dtype='int64')[trials]\n",
    "    barrier_pos = np.asarray(original_dataset.trial_info['barrier_pos'])[trials]\n",
    "    barrier_lengths = np.array([len(inner_array) for inner_array in barrier_pos])\n",
    "    \n",
    "    #Splits all arrays acording to the trials\n",
    "    align_time = np.split(align_time, split_indices)[1:]\n",
    "    hand_x = np.split(hand_x, split_indices)[1:]\n",
    "    hand_y = np.split(hand_y, split_indices)[1:]\n",
    "    cursor_x = np.split(cursor_x, split_indices)[1:]\n",
    "    cursor_y = np.split(cursor_y, split_indices)[1:]\n",
    "    \n",
    "    nr_trials = len(trials)\n",
    "    #The ploting starts\n",
    "    if(nr_trials%nr_cols == 0): nr_rows = nr_trials//nr_cols\n",
    "    else: nr_rows = nr_trials//nr_cols + 1\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*ratio, nr_rows*ratio))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    #Plotting\n",
    "    for i in range(nr_trials):\n",
    "        ax = axs[i]\n",
    "        x_coords = hand_x[i]\n",
    "        y_coords = hand_y[i]\n",
    "        x_cursor = cursor_x[i]\n",
    "        y_cursor = cursor_y[i]\n",
    "        target_x = target_pos[i,0]\n",
    "        target_y = target_pos[i,1]\n",
    "        duration = int(align_time[i][-1])/1000000000\n",
    "        barriers = barrier_pos[i]\n",
    "    \n",
    "        #Gets the limit\n",
    "        x_lim = np.max([np.max(x_coords), np.max(x_cursor), np.abs(np.min(x_coords)), np.abs(np.min(x_cursor))])\n",
    "        y_lim = np.max([np.max(y_coords), np.max(y_cursor), np.abs(np.min(y_coords)), np.abs(np.min(y_cursor))])\n",
    "        lim_value = np.max([x_lim,y_lim])+15\n",
    "        \n",
    "        #Plots the barriers\n",
    "        for j in range(barrier_lengths[i]):\n",
    "            x, y, half_width, half_height = barrier_pos[i][j]\n",
    "            left = x - half_width\n",
    "            bottom = y - half_height\n",
    "            width = 2 * half_width\n",
    "            height = 2 * half_height\n",
    "            ax.add_patch(plt.Rectangle((left, bottom), width, height, facecolor='gray'))\n",
    "    \n",
    "        ax.plot(x_cursor, y_cursor, label='Cursor Path')\n",
    "        ax.plot(x_coords, y_coords, label='Hand Path', alpha=0.4, color='grey')\n",
    "        ax.scatter(target_x, target_y, label='Target Position', color='red')\n",
    "        ax.scatter(x_cursor[0], y_cursor[0], label='Start Point', color='black')\n",
    "        ax.set_title(f'Trial {trials[i]} (lasted {duration} seconds)')\n",
    "        ax.set_xlim(-lim_value, lim_value)\n",
    "        ax.set_ylim(-lim_value, lim_value)\n",
    "        ax.set_xlabel('Pixels')\n",
    "        ax.set_ylabel('Pixels')\n",
    "        ax.legend(loc='lower left')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    #plt.suptitle(f'Trial Trajectories', y=1, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((nr_cols*ratio, nr_rows*ratio))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c44cf-b67b-4a94-ad34-8fd60e3752f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_single_traj(standardDS, np.arange(2295), nr_cols=6, ratio=4.5, save=False, save_name='traj_all(page).pdf')\n",
    "#plot_single_traj(standardDS, np.arange(2295), nr_cols=48, ratio=4.5, save=False, save_name='traj_all(square).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9c192-fac8-4c52-a87f-ad67dbf92b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_plot(dataset, neurons=None, trial=0, align=None, ratio=6, save=False, save_name='raster.png'):\n",
    "    mask = np.all(dataset.trial_info[['trial_id']] == trial, axis=1)\n",
    "\n",
    "    if align is None: \n",
    "        trial_data = dataset.make_trial_data(ignored_trials=(~mask))\n",
    "    else:\n",
    "        trial_data = dataset.make_trial_data(start_field='move_onset_time', end_field='reach_time', align_range=align, ignored_trials=(~mask))\n",
    "\n",
    "    if neurons is None:\n",
    "        neurons = trial_data['spikes'].columns.tolist()  # Gets the name of all neurons\n",
    "    \n",
    "    neuron_spikes = []\n",
    "    for neuron in neurons:\n",
    "        neuron_spikes.append(trial_data['spikes'][neuron])\n",
    "\n",
    "    # Initialize a list to store spike times for each neuron\n",
    "    spike_times = [[] for _ in range(len(neuron_spikes))]\n",
    "    \n",
    "    # Iterate through the spike data and extract spike times for each neuron\n",
    "    for neuron_idx, neuron in enumerate(neuron_spikes):\n",
    "        for time_idx, spike in enumerate(neuron):\n",
    "            if spike == 1:\n",
    "                spike_times[neuron_idx].append(time_idx * 5)  # Multiply by 5 to convert bins to ms\n",
    "\n",
    "    #Get the color\n",
    "    color_array = np.round(np.linspace(0, 255, 5))\n",
    "    color_template = 'turbo'\n",
    "    \n",
    "    # Gets the tot timestamp in milliseconds\n",
    "    tot_str = dataset.trial_info.loc[trial, 'target_on_time']\n",
    "    gct_str = dataset.trial_info.loc[trial, 'go_cue_time']\n",
    "    mot_str = dataset.trial_info.loc[trial, 'move_onset_time']\n",
    "    rt_str = dataset.trial_info.loc[trial, 'reach_time']\n",
    "    tot = pd.to_timedelta(tot_str).total_seconds() * 1000  # Convert to milliseconds\n",
    "    gct = pd.to_timedelta(gct_str).total_seconds() * 1000  # Convert to milliseconds\n",
    "    mot = pd.to_timedelta(mot_str).total_seconds() * 1000  # Convert to milliseconds\n",
    "    rt = pd.to_timedelta(rt_str).total_seconds() * 1000  # Convert to milliseconds\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1*ratio, 3/4*ratio))\n",
    "    for i, spikes in enumerate(spike_times, start=1):\n",
    "        ax.eventplot(spikes, colors='k', lineoffsets=i, linelengths=0.5)\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Neurons')\n",
    "    ax.set_title(f'Raster Plot of Trial {trial}')\n",
    "    \n",
    "    # Set x-ticks to show time in ms\n",
    "    num_bins = trial_data['spikes'].shape[0]  # Total number of bins\n",
    "    max_time_ms = num_bins * 5  # Convert total bins to ms\n",
    "    ax.set_xlim(0, max_time_ms)\n",
    "    xticks = np.arange(0, max_time_ms + 1, 500)  # Set x-ticks every 500 ms\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xticks)\n",
    "    \n",
    "    # Add a vertical line at the 'tot' timestamp\n",
    "    ax.axvline(tot, color=plt.colormaps.get_cmap(color_template)(int(color_array[0])), linestyle='--', linewidth=1, \n",
    "               label=f'Target On Time: {tot/1000:.3f} sec')\n",
    "    ax.axvline(gct, color=plt.colormaps.get_cmap(color_template)(int(color_array[1])), linestyle='--', linewidth=1, \n",
    "               label=f'Go Cue Time: {gct/1000:.3f} sec')\n",
    "    ax.axvline(mot, color=plt.colormaps.get_cmap(color_template)(int(color_array[2])), linestyle='--', linewidth=1, \n",
    "               label=f'Move Onset Time: {mot/1000:.3f} sec')\n",
    "    ax.axvline(rt, color=plt.colormaps.get_cmap(color_template)(int(color_array[3])), linestyle='--', linewidth=1, \n",
    "               label=f'Reach Time: {rt/1000:.3f} sec')\n",
    "    ax.legend()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0f641-9aeb-4a0b-bed4-5a5deb11b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_plot(standardDS, standard_neurons, trial=0, ratio=6, save=True, save_name='raster_full_example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384589b9-48c8-4959-a162-8701b4d5ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = standardDS\n",
    "trial = 0\n",
    "nr_neurons = 65\n",
    "\n",
    "'''\n",
    "Plot all frequencies of a trial\n",
    "'''\n",
    "#Extract Data\n",
    "mask = np.all(dataset.trial_info[['trial_id']] == trial, axis=1)\n",
    "trial_data = dataset.make_trial_data(start_field='move_onset_time', end_field='reach_time', ignored_trials=(~mask))\n",
    "firing_rate(trial_data, 35, panda=True) \n",
    "data = np.array(trial_data['spikes'])[:,0:nr_neurons].T\n",
    "X = np.linspace(-1, 1, data.shape[-1])\n",
    "G = 0.25\n",
    "\n",
    "#Colors\n",
    "color_array = np.round(np.linspace(100, 255, nr_neurons))\n",
    "#np.random.shuffle(color_array)\n",
    "color_template = 'Blues'\n",
    "\n",
    "\n",
    "#Starts Plotting\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "# Generate line plots\n",
    "lines = []\n",
    "for i in range(len(data)):\n",
    "    # Small reduction of the X extents to get a cheap perspective effect\n",
    "    xscale = 1 - i / 200.\n",
    "    # Same for linewidth (thicker strokes on the bottom)\n",
    "    lw = 1.5 - i / 500.0\n",
    "    line, = plt.plot(xscale * X, i*2 + G * data[i], color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), lw=lw)\n",
    "    lines.append(line)\n",
    "\n",
    "plt.ylim(-1, 70)\n",
    "\n",
    "# No ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Neurons')\n",
    "plt.title(f'Trial {trial} Firing Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e0199-b26d-485c-bb15-21303e1dc5d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Sample dataset and parameters for illustration\n",
    "dataset = standardDS\n",
    "neurons = standard_neurons\n",
    "trial = 0\n",
    "nr_neurons = 182\n",
    "ratio = 0.75\n",
    "\n",
    "# Extract Data\n",
    "mask = np.all(dataset.trial_info[['trial_id']] == trial, axis=1)\n",
    "trial_data = dataset.make_trial_data(start_field='move_onset_time', end_field='reach_time', ignored_trials=(~mask))\n",
    "# Assuming `firing_rate` is a function that processes `trial_data` (adjust accordingly if needed)\n",
    "firing_rate(trial_data, 35, panda=True)\n",
    "data = np.array(trial_data['spikes'])[:, 0:nr_neurons].T\n",
    "X = np.linspace(0, data.shape[-1] * 5, data.shape[-1])\n",
    "\n",
    "# Colors\n",
    "color_array = np.round(np.linspace(100, 255, nr_neurons))\n",
    "color_template = 'hsv'\n",
    "\n",
    "# Focus adjustment\n",
    "for j in range(nr_neurons):\n",
    "    focus = j\n",
    "    numbered_neurons = np.arange(len(neurons))\n",
    "    numbered_neurons = np.delete(numbered_neurons, focus)\n",
    "    \n",
    "    # Start Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10 * ratio, 6 * ratio))\n",
    "    \n",
    "    # Generate line plots\n",
    "    lines = []\n",
    "    for i in numbered_neurons:\n",
    "        if(np.mean(data[i]) < 20): line, = ax.plot(X, data[i], color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=0.005)\n",
    "        else: line, = ax.plot(X, data[i], color=plt.colormaps.get_cmap(color_template)(int(color_array[i])), alpha=0.05)\n",
    "        lines.append(line)\n",
    "    focus_line, = ax.plot(X, data[focus], color=plt.colormaps.get_cmap(color_template)(int(color_array[focus])),\n",
    "                          label=f'Neuron {standard_neurons[focus]}')\n",
    "    lines.append(focus_line)\n",
    "\n",
    "    # Extract the legend handles and labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Sort the labels and handles based on the sorted labels\n",
    "    sorted_labels_handles = sorted(zip(labels, handles), key=lambda x: int(x[0].split()[1]))\n",
    "    sorted_labels, sorted_handles = zip(*sorted_labels_handles)\n",
    "\n",
    "    # Create the legend with sorted labels\n",
    "    ax.legend(sorted_handles, sorted_labels)\n",
    "\n",
    "    # No ticks\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Firing Rate')\n",
    "    plt.title(f'Trial {trial} Firing Rate')\n",
    "    plt.savefig(f'trial0_neuron{focus}.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae000c-cc2a-49eb-95c9-655668101841",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.rad2deg(1.51),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d913b510-cb0f-48dc-bb36-2fd7fc09bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Produces the tune curve data for a set number of angles for the observed firing rate and for the predicted firing rate\n",
    "'''\n",
    "\n",
    "def tune_neurons(dataset, decoder, nr_directions, neurons):\n",
    "    rate_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    angle_data = np.asarray(dataset['angle'], dtype='float64')\n",
    "    target_angles = np.linspace(-np.pi, np.pi, nr_directions, endpoint=False)\n",
    "    \n",
    "    closest_angle_idxs = np.argmin(np.abs(target_angles[:, np.newaxis] - angle_data), axis=0)\n",
    "    angle_data = target_angles[closest_angle_idxs]\n",
    "    \n",
    "    #Gets the observed curve\n",
    "    mean_matrix = []\n",
    "    for target_angle in (target_angles):\n",
    "        mask = (angle_data == target_angle)\n",
    "        relevant_rates = rate_data[mask]\n",
    "        mean_rate = np.mean(relevant_rates, axis=0)\n",
    "        mean_matrix.append(mean_rate)\n",
    "    mean_matrix = np.asarray(mean_matrix, dtype='float64')\n",
    "    \n",
    "    #Gets the predicted curve\n",
    "    bias = decoder[:, 3]\n",
    "    k = decoder[:, 5]\n",
    "    angle = decoder[:, 4]\n",
    "    prediced_matrix = bias[:, np.newaxis] + k[:, np.newaxis] * np.cos(target_angles - angle[:, np.newaxis])\n",
    "    prediced_matrix = np.asarray(prediced_matrix, dtype='float64')\n",
    "    \n",
    "    return mean_matrix, prediced_matrix, target_angles\n",
    "\n",
    "'''\n",
    "Generates a decoder by fiting the training data to cosines (based in georgopolos et. al)\n",
    "'''\n",
    "def angle_decoder(dataset, window_size, neurons, delay=0, bin_size=5):\n",
    "    if(delay != 0): decoder = decoder_cosinesDelay(dataset, window_size, delay, bin_time=bin_size)\n",
    "    else: decoder = decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=bin_size)\n",
    "    return decoder\n",
    "\n",
    "'''\n",
    "Builds decoder by fiting the firing rate to cosines. This function does not support delay\n",
    "'''\n",
    "def decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=5):\n",
    "    #Change spikes to firing rates in ms (moving average) ----------- Can be Optimized\n",
    "    rate_data, trials_vector = firing_rate(dataset, window_size, bin_time=bin_time, panda=False)\n",
    "    train_size = len(rate_data)\n",
    "    nr_neurons = len(neurons)\n",
    "    \n",
    "    #Initiates the vectors for calculating the vector M\n",
    "    hand_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    cosines_matrix = np.zeros((train_size, 2))\n",
    "    angle = np.zeros(train_size)\n",
    "    \n",
    "    #Adds the column vectorM with the cosines components to the train_dataset\n",
    "    for i in range(train_size):\n",
    "        if (i == 0 or ((trials_vector[i] != trials_vector[i-1]))):\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        else:\n",
    "            preX = hand_pos[i-1,0]\n",
    "            preY = hand_pos[i-1,1]\n",
    "        x = hand_pos[i,0]\n",
    "        y = hand_pos[i,1]\n",
    "        angle_temp = np.arctan2((y - preY), (x - preX))\n",
    "        cosX = np.cos(angle_temp)\n",
    "        cosY = np.sin(angle_temp)\n",
    "        cosines_matrix[i,0] = cosX\n",
    "        cosines_matrix[i,1] = cosY\n",
    "        angle[i] = angle_temp\n",
    "    \n",
    "    #Trains the decoder fiting the data to a cos\n",
    "    decoder = []\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = rate_data[:,i]\n",
    "        weights = linear_regression_model(cosines_matrix, neuron_data, loss='ridge')\n",
    "        k = np.sqrt(np.power(weights[1],2) + np.power(weights[2],2))\n",
    "        if(k == 0): \n",
    "            print(f'neuron {neurons[i]} did not fire')\n",
    "            cx = 1\n",
    "            cy = 0\n",
    "        else:\n",
    "            cx = weights[1]/k\n",
    "            cy = weights[2]/k\n",
    "        angleC = np.arctan2(cy, cx)\n",
    "        decoder.append([neurons[i], cx, cy, weights[0], angleC, k])\n",
    "    dataset['spikes'] = rate_data\n",
    "    dataset['angle'] = angle\n",
    "    return np.array(decoder)\n",
    "\n",
    "'''\n",
    "Linear regression with given loss function and given regularization that returns the weights\n",
    "'''\n",
    "def linear_regression_model(trainning_var, trainning_out, loss='sse', lambaRidge=2):\n",
    "    #Data preparation\n",
    "    rates = trainning_var\n",
    "    vel = trainning_out\n",
    "    \n",
    "    #Loss Function Selection\n",
    "    if(loss == 'sse'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        penrose = np.linalg.pinv(rates.astype(np.float32))\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "    elif(loss == 'ridge'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        ratesT = np.transpose(rates)\n",
    "        prod = np.matmul(ratesT,rates)\n",
    "        identity = np.identity(len(prod))\n",
    "        helper = prod + lambaRidge * identity\n",
    "        inverse = np.linalg.inv(helper)\n",
    "        penrose = np.matmul(inverse,ratesT)\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "\n",
    "    return w\n",
    "\n",
    "'''\n",
    "Plots the tuning curves\n",
    "'''\n",
    "def plot_tuning(dataset, decoder, nr_directions, neurons, full_neurons, nr_cols=5, ratio=4, threshold=0, save=False, save_name='plot.png'):\n",
    "    mean_matrix, predicted_matrix, target_angles = tune_neurons(dataset, decoder, nr_directions, neurons)\n",
    "    max_rate = np.max([np.max(mean_matrix), np.max(predicted_matrix)])+1\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(neurons)//nr_cols\n",
    "    else: nr_rows = len(neurons)//nr_cols + 1\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*ratio, nr_rows*ratio))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        ax = axs[i]\n",
    "        observed = mean_matrix[:,neuron]\n",
    "        predicted = predicted_matrix[neuron]\n",
    "        max_angle = target_angles[np.argmax(predicted)]\n",
    "        height = round(np.max(predicted_matrix[neuron])-np.min(predicted_matrix[neuron]),3)\n",
    "        if(height > threshold):\n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve')\n",
    "            if(i==1):ax.plot(target_angles, predicted, label=f'Fitted Tuning Curve\\nPreferred Angle: {np.round(max_angle,2)} rads')\n",
    "            if(i==0):ax.set_title(f'Tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "            else:ax.set_title(f'Fitted tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "        else: \n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve', alpha=0.3)\n",
    "            ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nPreferred Angle: {np.round(max_angle,2)} rads', alpha=0.3)\n",
    "            ax.set_title(f'Tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "        ax.set_xlabel('Angle (rads)')\n",
    "        ax.set_ylabel('Firing Rate (Hz)')\n",
    "        ax.set_ylim(0, max_rate)\n",
    "        ax.legend(loc='lower left')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "        \n",
    "    #plt.suptitle(f'Tuning Curves\\n with {nr_directions} angles and threshold at {threshold}', y=1, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((nr_cols*ratio, nr_rows*ratio))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940c3cc-a861-4a68-a15b-783fcce3597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train_sets, val_sets, test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "train_dataset = train_sets[0]\n",
    "val_dataset = val_sets[0]\n",
    "decoder = angle_decoder(train_dataset, 35, standard_neurons)\n",
    "plot_tuning(train_dataset, decoder, 100, [181,181], standard_neurons, nr_cols=2, ratio=4.5, \n",
    "            save=True, save_name='tuning2D_example_comparition.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7cf62-571c-4fab-9a51-aead21242672",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_neurons = len(standard_neurons)\n",
    "for i in range(nr_neurons):\n",
    "    if(standard_neurons[i] == 2961): print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422766e-10f5-4d26-85ca-cebeb5358352",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5d0c-97c8-4b14-963b-5caed60d0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(standard_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407659c-d641-4307-bb0b-47c9295b9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(train_dataset['trial_id']))/len(np.unique(standardDS.trial_info['trial_id'])), len(np.unique(val_dataset['trial_id']))/len(np.unique(standardDS.trial_info['trial_id'])), len(np.unique(test_dataset['trial_id']))/len(np.unique(standardDS.trial_info['trial_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770cd768-0e06-4620-8b62-f0b91f6a9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round([0.6749455337690632, 0.07494553376906318, 0.25010893246187366],4)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7726077-3e6c-42e2-80b5-45123de29e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(train_dataset['trial_id'])), len(np.unique(val_dataset['trial_id'])), len(np.unique(test_dataset['trial_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240b22c-d257-4e3e-bc89-9a823b640526",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fa6c8-b76a-4c0b-8e86-30602f9c3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decodes the hand position (angle, cosines and trajectory) by applying a decoder fitted to the cosines\n",
    "'''\n",
    "\n",
    "def angle_predictor(dataset, decoder, window_size, neurons, bin_time=5, distance_coef=0.05, pred_pos=True):\n",
    "    #Prepares the data\n",
    "    rate_data, trials_ids = firing_rate(dataset, window_size, bin_time=bin_time, panda=False)\n",
    "    val_size = len(dataset)\n",
    "    #Predicts the direction\n",
    "    individual_predX = (rate_data - decoder[:,3])*decoder[:,1]\n",
    "    individual_predY = (rate_data - decoder[:,3])*decoder[:,2]\n",
    "    pred_cosX = np.sum(individual_predX, axis=1)\n",
    "    pred_cosY = np.sum(individual_predY, axis=1)\n",
    "    pred_angle = np.arctan2(pred_cosY, pred_cosX)\n",
    "    pred_magnitude = np.sqrt(np.power(pred_cosX,2) + np.power(pred_cosY,2))\n",
    "    #Normalizes the cos and sin\n",
    "    pred_cosX = np.cos(pred_angle)\n",
    "    pred_cosY = np.sin(pred_angle)\n",
    "    \n",
    "    #Adds it to the dataset\n",
    "    dataset['pred_mag'] = pred_magnitude\n",
    "    dataset['pred_cosX'] = pred_cosX\n",
    "    dataset['pred_cosY'] = pred_cosY\n",
    "    dataset['pred_angle'] = pred_angle\n",
    "    dataset['spikes'] = rate_data\n",
    "    \n",
    "    #Builds the vectorM for comparition\n",
    "    trial_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    mask = np.concatenate(([True], trials_ids[1:] != trials_ids[:-1]))\n",
    "    diffs = trial_pos - np.roll(trial_pos, 1, axis=0)\n",
    "    diffs = np.where(mask[:, np.newaxis], np.zeros_like(diffs), diffs)\n",
    "    angles = np.arctan2(diffs[:, 1], diffs[:, 0])\n",
    "    cosines_matrix = np.column_stack((np.cos(angles), np.sin(angles)))\n",
    "    \n",
    "    dataset['mx'] = cosines_matrix[:,0]\n",
    "    dataset['my'] = cosines_matrix[:,1]\n",
    "    dataset['angle'] = angles\n",
    "    \n",
    "    #Predicts the position if needed\n",
    "    if(pred_pos):\n",
    "        pred_cosNX = np.roll(pred_cosX, 1)\n",
    "        pred_cosNY = np.roll(pred_cosY, 1)\n",
    "        pred_magnitude = np.roll(pred_magnitude, 1)\n",
    "        pred_x = np.where(mask, trial_pos[:,0], pred_cosNX * pred_magnitude * distance_coef)\n",
    "        pred_y = np.where(mask, trial_pos[:,1], pred_cosNY * pred_magnitude * distance_coef)\n",
    "    \n",
    "        for i in range(len(pred_x)):\n",
    "            if(not mask[i]): \n",
    "                pred_x[i] = pred_x[i-1] + pred_x[i]\n",
    "                pred_y[i] = pred_y[i-1] + pred_y[i]\n",
    "        \n",
    "        dataset['pred_X'] = pred_x\n",
    "        dataset['pred_Y'] = pred_y\n",
    "\n",
    "    return individual_predX, individual_predY\n",
    "\n",
    "'''\n",
    "Produces the tune curve data for a set number of angles for the observed firing rate and for the predicted firing rate\n",
    "'''\n",
    "\n",
    "def tune_neurons(dataset, decoder, nr_directions, neurons):\n",
    "    rate_data = np.asarray(dataset['spikes'], dtype='float64')\n",
    "    angle_data = np.asarray(dataset['angle'], dtype='float64')\n",
    "    target_angles = np.linspace(-np.pi, np.pi, nr_directions, endpoint=False)\n",
    "    \n",
    "    closest_angle_idxs = np.argmin(np.abs(target_angles[:, np.newaxis] - angle_data), axis=0)\n",
    "    angle_data = target_angles[closest_angle_idxs]\n",
    "    \n",
    "    #Gets the observed curve\n",
    "    mean_matrix = []\n",
    "    for target_angle in (target_angles):\n",
    "        mask = (angle_data == target_angle)\n",
    "        relevant_rates = rate_data[mask]\n",
    "        mean_rate = np.mean(relevant_rates, axis=0)\n",
    "        mean_matrix.append(mean_rate)\n",
    "    mean_matrix = np.asarray(mean_matrix, dtype='float64')\n",
    "    \n",
    "    #Gets the predicted curve\n",
    "    bias = decoder[:, 3]\n",
    "    k = decoder[:, 5]\n",
    "    angle = decoder[:, 4]\n",
    "    prediced_matrix = bias[:, np.newaxis] + k[:, np.newaxis] * np.cos(target_angles - angle[:, np.newaxis])\n",
    "    prediced_matrix = np.asarray(prediced_matrix, dtype='float64')\n",
    "    \n",
    "    return mean_matrix, prediced_matrix, target_angles\n",
    "\n",
    "'''\n",
    "Generates a decoder by fiting the training data to cosines (based in georgopolos et. al)\n",
    "'''\n",
    "def angle_decoder(dataset, window_size, neurons, delay=0, bin_size=5):\n",
    "    if(delay != 0): decoder = decoder_cosinesDelay(dataset, window_size, delay, bin_time=bin_size)\n",
    "    else: decoder = decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=bin_size)\n",
    "    return decoder\n",
    "\n",
    "'''\n",
    "Builds decoder by fiting the firing rate to cosines. This function does not support delay\n",
    "'''\n",
    "def decoder_cosinesNoDelay(dataset, window_size, neurons, bin_time=5):\n",
    "    #Change spikes to firing rates in ms (moving average) ----------- Can be Optimized\n",
    "    rate_data, trials_vector = firing_rate(dataset, window_size, bin_time=bin_time, panda=False)\n",
    "    train_size = len(rate_data)\n",
    "    nr_neurons = len(neurons)\n",
    "    \n",
    "    #Initiates the vectors for calculating the vector M\n",
    "    hand_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    cosines_matrix = np.zeros((train_size, 2))\n",
    "    angle = np.zeros(train_size)\n",
    "    \n",
    "    #Adds the column vectorM with the cosines components to the train_dataset\n",
    "    for i in range(train_size):\n",
    "        if (i == 0 or ((trials_vector[i] != trials_vector[i-1]))):\n",
    "            preX = 0\n",
    "            preY = 0\n",
    "        else:\n",
    "            preX = hand_pos[i-1,0]\n",
    "            preY = hand_pos[i-1,1]\n",
    "        x = hand_pos[i,0]\n",
    "        y = hand_pos[i,1]\n",
    "        angle_temp = np.arctan2((y - preY), (x - preX))\n",
    "        cosX = np.cos(angle_temp)\n",
    "        cosY = np.sin(angle_temp)\n",
    "        cosines_matrix[i,0] = cosX\n",
    "        cosines_matrix[i,1] = cosY\n",
    "        angle[i] = angle_temp\n",
    "    \n",
    "    #Trains the decoder fiting the data to a cos\n",
    "    decoder = []\n",
    "    for i in range(nr_neurons):\n",
    "        neuron_data = rate_data[:,i]\n",
    "        weights = linear_regression_model(cosines_matrix, neuron_data, loss='ridge')\n",
    "        k = np.sqrt(np.power(weights[1],2) + np.power(weights[2],2))\n",
    "        if(k == 0): \n",
    "            print(f'neuron {neurons[i]} did not fire')\n",
    "            cx = 1\n",
    "            cy = 0\n",
    "        else:\n",
    "            cx = weights[1]/k\n",
    "            cy = weights[2]/k\n",
    "        angleC = np.arctan2(cy, cx)\n",
    "        decoder.append([neurons[i], cx, cy, weights[0], angleC, k])\n",
    "    dataset['spikes'] = rate_data\n",
    "    dataset['angle'] = angle\n",
    "    return np.array(decoder)\n",
    "\n",
    "'''\n",
    "Linear regression with given loss function and given regularization that returns the weights\n",
    "'''\n",
    "def linear_regression_model(trainning_var, trainning_out, loss='sse', lambaRidge=2):\n",
    "    #Data preparation\n",
    "    rates = trainning_var\n",
    "    vel = trainning_out\n",
    "    \n",
    "    #Loss Function Selection\n",
    "    if(loss == 'sse'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        penrose = np.linalg.pinv(rates.astype(np.float32))\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "    elif(loss == 'ridge'):\n",
    "        rates = np.insert(rates, 0, 1, axis=1)\n",
    "        ratesT = np.transpose(rates)\n",
    "        prod = np.matmul(ratesT,rates)\n",
    "        identity = np.identity(len(prod))\n",
    "        helper = prod + lambaRidge * identity\n",
    "        inverse = np.linalg.inv(helper)\n",
    "        penrose = np.matmul(inverse,ratesT)\n",
    "        w = np.matmul(penrose,vel)\n",
    "        predicted = np.matmul(rates,w)\n",
    "\n",
    "    return w\n",
    "\n",
    "'''\n",
    "Plots the tuning curves\n",
    "'''\n",
    "def plot_tuning(dataset, decoder, nr_directions, neurons, full_neurons, nr_cols=5, ratio=4, threshold=0, save=False, save_name='plot.png'):\n",
    "    mean_matrix, predicted_matrix, target_angles = tune_neurons(dataset, decoder, nr_directions, neurons)\n",
    "    max_rate = np.max([np.max(mean_matrix), np.max(predicted_matrix)])+1\n",
    "    if(len(mean_matrix[0])%nr_cols == 0): nr_rows = len(neurons)//nr_cols\n",
    "    else: nr_rows = len(neurons)//nr_cols + 1\n",
    "    \n",
    "    # Create a figure and subplots\n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*ratio, nr_rows*ratio))\n",
    "    \n",
    "    # Flatten the axs array for easier iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    # Plot each array\n",
    "    for i, neuron in enumerate(neurons):\n",
    "        ax = axs[i]\n",
    "        observed = mean_matrix[:,neuron]\n",
    "        predicted = predicted_matrix[neuron]\n",
    "        max_angle = target_angles[np.argmax(predicted)]\n",
    "        height = round(np.max(predicted_matrix[neuron])-np.min(predicted_matrix[neuron]),3)\n",
    "        if(height > threshold):\n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve')\n",
    "            if(i==1):ax.plot(target_angles, predicted, label=f'Fitted Tuning Curve\\nPreferred Angle: {np.round(max_angle,2)} rads')\n",
    "            if(i==0):ax.set_title(f'Tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "            else:ax.set_title(f'Fitted tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "        else: \n",
    "            ax.plot(target_angles, observed, label='Observed Tuning Curve', alpha=0.3)\n",
    "            ax.plot(target_angles, predicted, label=f'Predicted Tuning Curve\\nPreferred Angle: {np.round(max_angle,2)} rads', alpha=0.3)\n",
    "            ax.set_title(f'Tuning curve of neuron {full_neurons[neuron]}', fontsize=10)\n",
    "        ax.set_xlabel('Angle (rads)')\n",
    "        ax.set_ylabel('Firing Rate (Hz)')\n",
    "        ax.set_ylim(0, max_rate)\n",
    "        ax.legend(loc='lower left')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    for ax in fig.get_axes():\n",
    "        ax.label_outer()\n",
    "        \n",
    "    #plt.suptitle(f'Tuning Curves\\n with {nr_directions} angles and threshold at {threshold}', y=1, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    print((nr_cols*ratio, nr_rows*ratio))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc56fc2-e1f7-46e7-83e4-e16e28d20baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predictions made with decoder trained with hand_pos covariate and cosines angles (paper)\n",
    "Moving Average Size: 35\n",
    "Time Required: 1 sec\n",
    "'''\n",
    "%time train_sets, val_sets, test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "train_dataset = train_sets[0]\n",
    "val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "plot_correlation = True\n",
    "\n",
    "#Generates a decoder for every neuron\n",
    "decoder = angle_decoder(train_dataset, 35, neurons)\n",
    "\n",
    "#Runs the tuning curve for every neuron\n",
    "_, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 360, neurons)\n",
    "\n",
    "#Removes unecessary neurons\n",
    "#decoder, neurons = remove_neurons(predicted_matrix, val_dataset, neurons, 0)\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(val_dataset, decoder, 35, neurons, distance_coef=0.02)\n",
    "\n",
    "#Plots Correlation\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "file_names = ['mean_pearson_all_cosx_standardDS_nonOptimized.png','mean_pearson_all_cosy_standardDS_nonOptimized.png',\n",
    "              'mean_pearson_all_angle_standardDS_nonOptimized.png']\n",
    "trials = trials_present(val_dataset)\n",
    "if(plot_correlation):\n",
    "    for i in range(len(metrics)):\n",
    "        plot_ccTogether(val_dataset, trials , metrics[i], focus_trial='mean', correlation='pearson', window_size=150, \n",
    "                        compare_axis=True, color_template='twilight', save=False, save_name=file_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e993ab0-2fd4-42b0-84d8-276a748223b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Population Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d148b61-3593-4ab3-9215-1d4e048533e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Window Size Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044b1c6-95fb-4d4b-92f5-70d044820c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizes the Population Vector for window size\n",
    "'''\n",
    "total_time = time.time()\n",
    "\n",
    "#|----------Gets the Datasets Info----------|\n",
    "print('Starting Dataset Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Preparing for Hacking----------|\n",
    "print('Starting Hacking Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "window_size = np.linspace(2, 50, 49, dtype=int)\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Hyperparametrization Hacking----------|\n",
    "print('Hacking Starting\\n')\n",
    "for size in window_size:\n",
    "    print(f'|----- Trying window size of {size} -----|')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Generates a decoder\n",
    "    train_dataset = o_train_dataset.copy()\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "    decoder = angle_decoder(train_dataset, size, neurons)\n",
    "\n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(val_dataset, decoder, size, neurons, pred_pos=False)\n",
    "\n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time,3)\n",
    "    print(f'It took {elapsed_time} sec\\n')\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "print(f'It took a total time of {round(time.time() - total_time,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf18b2d-61c3-4b63-809b-15d562035f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Window Size Hacking for Population Vector'\n",
    "label_name = 'Window Size (nº of bins)'\n",
    "save_name = 'ppv_window_size_hack_r2.png'\n",
    "eval='R2'\n",
    "compare_models(parameters, all_entries, window_size, eval=eval, title_name=title_name, label_name=label_name, save=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42773a1f-613a-40d1-a2f8-8cd6820999a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neuron Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414fc0b-55e7-4841-85cc-c40f248c4ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizes the Population Vector for angle resolution\n",
    "'''\n",
    "total_time = time.time()\n",
    "\n",
    "#|----------Gets the Datasets Info----------|\n",
    "print('Starting Dataset Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Preparing for Hacking----------|\n",
    "print('Starting Hacking Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "#Gets thresholds\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "#Generates a decoder\n",
    "o_neurons = standard_neurons\n",
    "o_decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "\n",
    "#Runs the tuning curve for every neuron\n",
    "observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, o_decoder, 360, o_neurons)\n",
    "diff = np.sqrt(np.sum(np.power(predicted_matrix - observed_matrix.T, 2), axis=1))\n",
    "threshold = np.linspace(np.max(diff), np.min(diff), 50, endpoint=False)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Hyperparametrization Hacking----------|\n",
    "print('Hacking Starting\\n')\n",
    "for i, thr in enumerate(threshold):\n",
    "    print(f'|----- Trying difference threshold of {round(thr, 2)} -----|')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Copies variables needed\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    decoder = o_decoder\n",
    "    \n",
    "    #Updates the neurons\n",
    "    decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, val_dataset, neurons, thr, metric='difference')\n",
    "    print(f'Nº of accepted neurons: {len(neurons)}')\n",
    "    print(f'Nº of removed neurons: {len(standard_neurons)-len(neurons)}')\n",
    "\n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(val_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "\n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        if(np.isnan([mean, std, r2_mean, r2_std]).any()): \n",
    "            threshold = threshold[:i]\n",
    "            print('NaN found -> Early Stop')\n",
    "            break\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "        \n",
    "    if(np.isnan([mean, std, r2_mean, r2_std]).any()): break\n",
    "        \n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time,3)\n",
    "    print(f'It took {elapsed_time} sec\\n')\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "print(f'It took a total time of {round(time.time() - total_time,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d5186-255c-474c-8e40-254bb7b35621",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Neuron Hacking for Population Vector'\n",
    "label_name = 'Difference Threshold'\n",
    "save_name = 'ppv_neuron_removal_hack_r2.png'\n",
    "eval='R2'\n",
    "compare_models(parameters, all_entries, np.round(threshold, 2), eval=eval, title_name=title_name, label_name=label_name, \n",
    "               save=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bfcb0-45d3-40e3-a168-007c10c120c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Angle Resolution Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9755bad-bf1f-4c12-960b-c0159097c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "'''\n",
    "Optimizes the Population Vector for angle resolution (With previous R2 optimizations)\n",
    "'''\n",
    "total_time = time.time()\n",
    "\n",
    "#|----------Gets the Datasets Info----------|\n",
    "print('Starting Dataset Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "o_neurons = standard_neurons\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Preparing for Hacking----------|\n",
    "print('Starting Hacking Preparation')\n",
    "start_time = time.time()\n",
    "\n",
    "thresholds = np.linspace(6, 360, 50, dtype=int)\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY'],['angle', 'pred_angle']]\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "o_decoder = angle_decoder(o_train_dataset, 31, o_neurons)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time,3)\n",
    "print(f'It took {elapsed_time} sec')\n",
    "\n",
    "#|----------Hyperparametrization Hacking----------|\n",
    "print('Hacking Starting\\n')\n",
    "for thr in thresholds:\n",
    "    print(f'|----- Angle Resolution of {thr} -----|')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Generates a decoder\n",
    "    train_dataset = o_train_dataset.copy()\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    decoder = o_decoder\n",
    "    \n",
    "    #Removes Neurons\n",
    "    observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, thr, neurons)\n",
    "    decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, val_dataset, neurons, 11.4, metric='difference')\n",
    "    \n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(val_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "\n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time,3)\n",
    "    print(f'It took {elapsed_time} sec\\n')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "print(f'It took a total time of {round(time.time() - total_time,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163c464-0c82-4638-a928-464ef17a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Angle Resolution Hacking for Population Vector'\n",
    "label_name = 'Resolution (nº of angles)'\n",
    "save_name = 'ppv_angle_res_hack_pearson.png'\n",
    "eval='Pearson'\n",
    "compare_models(parameters, all_entries, np.round(thresholds, 2), eval=eval, title_name=title_name, label_name=label_name, \n",
    "               save=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ee402-a3f2-4627-bf50-6a117d61bb0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241729e-8630-431a-acbc-0f31e311659b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Resolution Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa0ced-7d1e-4dc6-aba4-d836803e6160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple Bayes Decoder - Parametrization Hacking #1 (Velocity Resolution)\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(o_val_dataset, 35, panda=True) \n",
    "firing_rate(o_train_dataset, 35, panda=True) \n",
    "firing_rate(o_test_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Run Control\n",
    "parameters = []\n",
    "all_entries = []\n",
    "thresholds = np.linspace(10, 50, 9, dtype=int)\n",
    "thresholds = np.append(thresholds,100)\n",
    "\n",
    "print()\n",
    "print('-----------------------------')\n",
    "hack_time = time.time()\n",
    "#Removing Neurons with depth threshold\n",
    "print('Trying Velocity Resolution Hacking')\n",
    "for i, thr in enumerate(thresholds):\n",
    "    print()\n",
    "    print(f'Starting Model were velocity\\'s resolution used was {thr}')\n",
    "\n",
    "    #Start Counting Time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Neuron Removal\n",
    "    train_dataset = o_train_dataset.copy()\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "    test_dataset = o_test_dataset.copy()\n",
    "    \n",
    "    #datasets = [train_dataset, val_dataset, test_dataset]\n",
    "\n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, thr)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    '''\n",
    "    print('mean_matrix: ' + str(np.isnan(mean_matrix).any()))\n",
    "    print('found_pairs: ' + str(np.isnan(found_pairs).any()))\n",
    "    print('velocity_vector: ' + str(np.isnan(velocity_vector).any()))\n",
    "    print('grids: ' + str(np.isnan(grids).any()))\n",
    "    print('observed: ' + str(np.isnan(observed).any()))\n",
    "    print('predicted: ' + str(np.isnan(predicted).any()))\n",
    "    '''\n",
    "\n",
    "    \n",
    "    #acepted_neurons, acepted_predicted = remove_neurons(datasets, neurons, predicted, 'depth', min_depth=depth)\n",
    "    #print('Neurons Removed')\n",
    "    \n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "    '''\n",
    "    print('decoder 0: ' + str(np.isnan(decoder[0]).any()))\n",
    "    print('decoder 1: ' + str(np.isnan(decoder[1]).any()))\n",
    "    print('pred_vel: ' + str(np.isnan(pred_vel).any()))\n",
    "    print('max_prob: ' + str(np.isnan(max_prob).any()))\n",
    "    '''\n",
    "\n",
    "    \n",
    "    #Column Management\n",
    "    apply_2D_data(val_dataset, pred_vel)\n",
    "    apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "    \n",
    "    elapsed_depth = round(time.time() - start_time,3)\n",
    "    print(f'It took {elapsed_depth} sec')\n",
    "    os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print('Velocity Resolution Hacking DONE')\n",
    "elapsed_depth = round(time.time() - hack_time,3)\n",
    "print(f'It took {elapsed_depth} sec')\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da8cc6b-1111-474a-bf94-35a50a1213d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Velocity Resolution for Naïve Bayes'\n",
    "label_name = 'Resolution (nº of bins)'\n",
    "save_name = 'nb_vel_res_hack_pearson.png'\n",
    "eval='Pearson'\n",
    "compare_models(parameters, all_entries, thresholds, eval=eval, title_name=title_name, label_name=label_name, save=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b929887-8fc7-49db-aa2c-e890f48f70fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Time Resolution Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c28812-9034-4bc2-a3aa-7dbc2bff2f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple Bayes Decoder - Parametrization Hacking #1 (Time Resolution)\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Run Control\n",
    "parameters = []\n",
    "all_entries = []\n",
    "thresholds = np.linspace(1, 50, 50, dtype=int)\n",
    "\n",
    "print()\n",
    "print('-----------------------------')\n",
    "hack_time = time.time()\n",
    "#Removing Neurons with depth threshold\n",
    "print('Trying Velocity Resolution Hacking')\n",
    "for i, thr in enumerate(thresholds):\n",
    "    print()\n",
    "    print(f'Starting Model were time resolution used was {thr}')\n",
    "\n",
    "    #Start Counting Time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Neuron Removal\n",
    "    train_dataset = o_train_dataset.copy()\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "\n",
    "    #Firing Rate\n",
    "    firing_rate(val_dataset, thr, panda=True) \n",
    "    firing_rate(train_dataset, thr, panda=True) \n",
    "\n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 15)\n",
    "    print('All neurons have been fitted')\n",
    "    \n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, window_size=thr)\n",
    "    pred_vel, max_prob = SNB_apply(val_dataset, decoder, window_size=thr)\n",
    "    print('Decoder Applied')\n",
    "    \n",
    "    #Column Management\n",
    "    apply_2D_data(val_dataset, pred_vel)\n",
    "    apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "    \n",
    "    elapsed_depth = round(time.time() - start_time,3)\n",
    "    print(f'It took {elapsed_depth} sec')\n",
    "    os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "\n",
    "print('Velocity Resolution Hacking DONE')\n",
    "elapsed_depth = round(time.time() - hack_time,3)\n",
    "print(f'It took {elapsed_depth} sec')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b0077-4322-45aa-adad-47a5c5f9a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Time Resolution for Naïve Bayes'\n",
    "label_name = 'Window Size (nº of bins)'\n",
    "save_name = 'nb_time_res_hack_r2.png'\n",
    "eval='R2'\n",
    "compare_models(parameters, all_entries, thresholds, eval=eval, title_name=title_name, label_name=label_name, save=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09779d94-5a28-458a-acf5-ee5774ff7802",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neurons to Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5359e6-0074-4108-bfdd-6276cd3a5dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple Bayes Decoder - Parametrization Hacking #3 (Neurons)\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "neurons = standard_neurons\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(o_val_dataset, 35, panda=True) \n",
    "firing_rate(o_train_dataset, 35, panda=True) \n",
    "firing_rate(o_test_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Run Control\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(o_train_dataset, 15)\n",
    "print('All neurons have been fitted')\n",
    "dist = get_tune_distance(grids, observed, predicted)\n",
    "thresholds = np.linspace(np.max(dist), np.min(dist), 50, endpoint=False)\n",
    "\n",
    "print()\n",
    "print('-----------------------------')\n",
    "hack_time = time.time()\n",
    "#Removing Neurons with depth threshold\n",
    "print('Trying Neuron Hacking')\n",
    "for i, thr in enumerate(thresholds):\n",
    "    print()\n",
    "    print(f'Starting Model were Neuron Threshold Difference used was {thr}')\n",
    "\n",
    "    #Start Counting Time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Dataset Copy\n",
    "    train_dataset = o_train_dataset.copy()\n",
    "    val_dataset = o_val_dataset.copy()\n",
    "    test_dataset = o_test_dataset.copy()\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "\n",
    "    #Removes\n",
    "    acepted_neurons, acepted_predicted = nb_remove_neurons(datasets, neurons, predicted, observed, grids, 'diff', threshold=thr)\n",
    "    print('Neurons Removed')\n",
    "    print(f'Nº of accepted neurons: {len(acepted_neurons)}')\n",
    "    print(f'Nº of removed neurons: {len(neurons)-len(acepted_neurons)}')\n",
    "    \n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, acepted_predicted)\n",
    "    pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    \n",
    "    #Column Management\n",
    "    apply_2D_data(val_dataset, pred_vel)\n",
    "    apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "    \n",
    "    elapsed_depth = round(time.time() - start_time,3)\n",
    "    print(f'It took {elapsed_depth} sec')\n",
    "    os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print('Velocity Resolution Hacking DONE')\n",
    "elapsed_depth = round(time.time() - hack_time,3)\n",
    "print(f'It took {elapsed_depth} sec')\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30021900-3965-4d2b-ad0d-12443cbd47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name = 'Neuron Removal Hacking for Naïve Bayes'\n",
    "label_name = 'Distance Threhsold'\n",
    "save_name = 'nb_neuron_removal_hack_pearson.png'\n",
    "eval='Pearson'\n",
    "compare_models(parameters, all_entries, np.round(thresholds,2), eval=eval, title_name=title_name, label_name=label_name, save=False, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a440a-24d7-48ea-b716-ab02a788f79e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Smoothing Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b54d5-9079-4acf-8b85-2be7d146540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns all components needed for the Naive Bayses Decoder\n",
    "'''\n",
    "def NB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=None, bin_size=5, window_size=35):\n",
    "    #Unpacking data\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    probability_s = all_prob_output(found_pairs, velocity_vector)\n",
    "    rate_data = np.array(train_dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "    if(spike_range == None): spike_range = np.max(spike_data)\n",
    "    nr_neurons = len(spike_data[0])\n",
    "    \n",
    "    #Gets all neuron probability to all velocities for all spikes count in the train\n",
    "    all_conditional = []\n",
    "    for i, pair in enumerate(found_pairs):\n",
    "        fs = vect2_twoD_Gaussian(([found_pairs[i,0]], [found_pairs[i,1]]), predicted.T)\n",
    "        all_store = []\n",
    "        for j in range(nr_neurons):\n",
    "            f = fs[j] * tau\n",
    "            all_prob = []\n",
    "            for k in range(spike_range):\n",
    "                probability = (np.exp(-f)*np.power(f, k))/math.factorial(k)\n",
    "                all_prob.append(probability)\n",
    "            all_store.append(all_prob)\n",
    "        all_conditional.append(all_store)\n",
    "    all_conditional = np.array(all_conditional) #found_pair, neuron, spike_count\n",
    "\n",
    "    #Gets the distance probability matrix\n",
    "    std = get_train_std(train_dataset)\n",
    "    probability_dist = prob_distances(found_pairs, std*50)\n",
    "    return all_conditional, probability_s, probability_dist, found_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c5cb4-8960-4c71-aff2-b1b729ee07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fits a gaussian to the distances and visualizes them\n",
    "'''\n",
    "def prob_distances(found_pairs, std, plot=None, cmap='hot', dist_alpha=0, prob_alpha=1, save=False, save_name='plot.png'):\n",
    "    #Get the distances between each pair in a square matrix (found_pairs x found_pairs)\n",
    "    general_dists = squareform(pdist(found_pairs))\n",
    "    print('general_dists')\n",
    "    print(general_dists.shape)\n",
    "    print(type(general_dists))\n",
    "    print(general_dists)\n",
    "    print()\n",
    "    extra_prob = norm.pdf(general_dists, 0, std)\n",
    "    \n",
    "    #Ploting if not None\n",
    "    if(plot == '3D'):\n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        size = len(found_pairs)\n",
    "        grid_x, grid_y = np.mgrid[0:size:size*1j, 0:size:size*1j]\n",
    "        if(dist_alpha != 0):\n",
    "            ax.plot_surface(grid_x, grid_y, general_dists, cmap=cmap, edgecolor='none', alpha=dist_alpha)\n",
    "        ax.plot_surface(grid_x, grid_y, extra_prob, cmap=cmap, edgecolor='none', alpha=prob_alpha)\n",
    "        ax.set_xlabel('Found_Pairs')\n",
    "        ax.set_ylabel('Found_Pairs - 1')\n",
    "        ax.set_zlabel('Distances')\n",
    "        \n",
    "        if(view != None):\n",
    "            ax.view_init(view[0], view[1], view[2])\n",
    "    \n",
    "        plt.title('Distances', fontsize=20)\n",
    "        if(save): plt.savefig(save_name)\n",
    "        plt.show()\n",
    "        \n",
    "    elif(plot == 'heat'):\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot the first heatmap on the left subplot\n",
    "        im1 = axs[0].imshow(extra_prob.T, cmap=cmap, origin='lower')\n",
    "        cb_max1 = np.nanmax(extra_prob)\n",
    "        cb_min1 = np.nanmin(extra_prob)\n",
    "        cb_range1 = np.linspace(cb_min1, cb_max1, 10)\n",
    "        fig.colorbar(im1, ax=axs[0], ticks=cb_range1, label='P(s|s-1)')\n",
    "        axs[0].set_title('Heat Map of Distance Probability', fontsize=20)\n",
    "        axs[0].set_xlabel('Found Pairs')\n",
    "        axs[0].set_ylabel('Found Pairs - 1')\n",
    "        \n",
    "        # Plot the second heatmap on the right subplot\n",
    "        im2 = axs[1].imshow(general_dists.T, cmap=cmap, origin='lower', extent=(0,1790,0,1790))  \n",
    "        cb_max2 = np.nanmax(general_dists)  \n",
    "        cb_min2 = np.nanmin(general_dists)  \n",
    "        cb_range2 = np.linspace(cb_min2, cb_max2, 10)  \n",
    "        fig.colorbar(im2, ax=axs[1], ticks=cb_range2, label='Distance')  \n",
    "        axs[1].set_title('Heat Map of Distances', fontsize=20)\n",
    "        axs[1].set_xlabel('Found Pairs')\n",
    "        axs[1].set_ylabel('Found Pairs - 1')\n",
    "    \n",
    "        #Save plot\n",
    "        if save:\n",
    "            plt.savefig(save_name)\n",
    "        plt.show()\n",
    "    return extra_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad65ef0-0611-40ef-a332-a0af2b643f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets the distances (euclidean) between all training dataset velocities and their std\n",
    "'''\n",
    "def get_train_std(train_dataset, found_pairs, get_distance=False):\n",
    "    #Data extraction\n",
    "    velocities = np.array(train_dataset['hand_vel'])\n",
    "\n",
    "    #Get distance\n",
    "    for i in range(n-1):\n",
    "        dx[i]=np.sqrt((y_train[i+1,0]-y_train[i,0])**2+(y_train[i+1,1]-y_train[i,1])**2)\n",
    "    \n",
    "    delta = velocities[1:] - velocities[:-1]\n",
    "    distances = np.sqrt(np.sum(delta**2, axis=1))\n",
    "    std = np.std(distances)\n",
    "\n",
    "    if(get_distance): return std, distances\n",
    "    else: return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cffc01-a574-4df8-b48d-ddd9f425b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applies the Naive Bayes Decoder\n",
    "'''\n",
    "def NB_apply(dataset, decoder, bin_size=5, window_size=35):\n",
    "    #Data Extraction\n",
    "    all_conditional, probability_s, probability_dist, found_pairs = decoder\n",
    "    tau = bin_size * window_size * 0.001\n",
    "    rate_data = np.array(dataset['spikes'])\n",
    "    spike_data = np.asarray(rate_data * tau, dtype='int')\n",
    "\n",
    "    #Gets all trial changes boolean\n",
    "    _, first_occurrences = np.unique(trial_ids, return_index=True)\n",
    "    trial_change = np.zeros_like(trial_ids, dtype=bool)\n",
    "    trial_change[first_occurrences] = True\n",
    "    \n",
    "    #Loop Apply - Tries all s for each entry and returns the most likely -> Can be optimized (parallelization)\n",
    "    max_prob = []\n",
    "    directions = []\n",
    "    i_found_pairs = np.arange(len(found_pairs))\n",
    "    i_neurons = np.arange(len(spike_data[0]))\n",
    "    \n",
    "    for k in range(len(spike_data)):\n",
    "        spikes = spike_data[k]\n",
    "        values = all_conditional[i_found_pairs[:, np.newaxis], i_neurons, spikes]\n",
    "        conditional_vector = np.prod(values, axis=1)\n",
    "        if(trial_change[k]): probability_vector = conditional_vector #Firs entry of each trial should not check past\n",
    "        else: \n",
    "            previos_loc = directions[-1]\n",
    "            loc_index = np.where(np.all(found_pairs == directions[-1], axis=1))[0][0]\n",
    "            probability_vector = conditional_vector * probability_dist[loc_index]\n",
    "        direction_loc = np.argmax(probability_vector)\n",
    "        max_prob.append(probability_vector[direction_loc])\n",
    "        directions.append(found_pairs[direction_loc])\n",
    "    \n",
    "    max_prob = np.array(max_prob)\n",
    "    directions = np.array(directions)\n",
    "\n",
    "    return directions, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5b230-4f66-476b-a2e0-0928c1ec0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_distances(found_pairs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5442052-15e1-424d-81d3-b757a814fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "dx = np.zeros([n-1,1])\n",
    "noise = np.random.uniform(low=-0.0001, high=0.0001, size=dx.shape)\n",
    "dx = dx + noise\n",
    "np.sqrt(np.mean(dx**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7360312-d220-437f-8618-399f030df4fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = np.random.rand(100,2)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac6844-d785-47a7-8da3-8b02aa93c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=y_train.shape[0]\n",
    "dx=np.zeros([n-1,1])\n",
    "for i in range(n-1):\n",
    "    dx[i]=np.sqrt((y_train[i+1,0]-y_train[i,0])**2+(y_train[i+1,1]-y_train[i,1])**2) #Change in state across time steps\n",
    "std=np.sqrt(np.mean(dx**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab1256-f1f6-422d-aa4f-bdc3421e77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56ce2f-eb15-417a-aa15-7e05c1be9982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae5b38-bee2-4a3e-9ade-6b1d9e566906",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Surface Hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c62bb6-511d-4b66-97b2-967f06bae184",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cauchy Surface used to fit tuning curves\n",
    "'''\n",
    "def cauchy_surface(xy, amplitude, xo, yo, l, offset):\n",
    "    x, y = xy\n",
    "    up1 = l**2\n",
    "    down1 = (x-xo)**2+(y-yo)**2+up1\n",
    "    down2 = np.pi * l\n",
    "    return (up1/down1)*(1/down2)*amplitude + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177103fa-4173-47b8-bfb1-906a3b4d13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, 1)\n",
    "o_train_dataset = train_sets[0]\n",
    "o_val_dataset = val_sets[0]\n",
    "firing_rate(o_val_dataset, 35, panda=True) \n",
    "firing_rate(o_train_dataset, 35, panda=True) \n",
    "firing_rate(o_test_dataset, 35, panda=True)\n",
    "neurons = standard_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d42a33-474d-48ad-86c5-020cb9cf3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = o_train_dataset.copy()\n",
    "val_dataset = o_val_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7c77b-842c-4970-bf96-e036b41bdb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, model='Cauchy', resolution=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720207fd-6cb1-47b0-9e5b-d72dcc0a28f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_heatmap_all(mean_matrix, neurons, found_pairs, grids, observed, predicted, nr_cols=5, model='Cauchy', uniform=False, \n",
    "                 save=False, save_name='cauchy_tuning2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640082d-2b52-4a4a-a27a-839bcfd83ed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tune3D_single(mean_matrix, found_pairs, 1, grids, observed, predicted, model='Cauchy',\n",
    "                       view=None, fill_nan=False, view_points=False, observed_alpha=0.1, predicted_alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d6677-b798-4cbc-b46c-547c34b03b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, model='Cauchy')\n",
    "pred_vel, max_prob = SNB_apply(val_dataset, decoder)\n",
    "\n",
    "#Column Management\n",
    "apply_2D_data(val_dataset, pred_vel)\n",
    "apply_2D_data(val_dataset, np.array(val_dataset['hand_vel']), col_name='vel')\n",
    "print('Column Management Done')\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(val_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c77641-1ebf-4ec7-a99b-47683231751a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395756e-41c8-4eae-ab06-a1f08742520d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for population vector\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY']]\n",
    "\n",
    "print()\n",
    "print('--------------Population Vector---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_test_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "\n",
    "    #Build the decoder\n",
    "    decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "   \n",
    "    #Updates the neurons\n",
    "    observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "    decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, test_dataset, neurons, 11.4, metric='difference')\n",
    "    \n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(test_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "firing_rate(o_test_dataset, 35, panda=True)\n",
    "\n",
    "print()\n",
    "print('--------------Naive Bayes---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_test_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters = np.array(parameters)\n",
    "all_entries = np.array(all_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30d128-7906-4a21-8b99-7b0d2519aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Population Vector (PPV)', 'Naive Bayes (NB)', 'Golden Standard (MINT)']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Final Models Analysis'\n",
    "label_name='Final Models'\n",
    "save=False\n",
    "save_name='compare_ppv.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters[0:10]\n",
    "model_2_parameters = parameters[10:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(6, 6), layout='constrained')\n",
    "colors = ['tomato', 'dodgerblue', 'orange', 'dodgerblue', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for PPV: {round(m_mean[0],2)}')\n",
    "'''\n",
    "plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "             label=f'Mean {eval} for NB: {round(m_mean[1],2)}')\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names))\n",
    "plt.ylim(m_mean[0]-0.01, m_mean[0]+0.01)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8e596-5697-4191-96cd-f2cb1a1dfb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_pairs[2,0] - found_pairs[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da9c23-d9fa-499a-9cda-565f85042cf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## p_Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25531494-3191-4b7f-8a3c-7773d1280577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-sample t-test\n",
    "t_statistic, p_value = stats.ttest_1samp(f_model1, 0)\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.005:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of the data is significantly greater than 0.\")\n",
    "else:\n",
    "    print(\"The mean of the data is not significantly different from 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b7746-c980-4e94-9b62-b31d856394c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model2, f_model1, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model2 is significantly greater than the mean of f_model1.\")\n",
    "else:\n",
    "    print(\"The mean of f_model2 is not significantly greater than the mean of f_model1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf81d8-6789-468f-a407-7c8d8bf70ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, p_value = stats.ttest_1samp(f_model2, 0.85, alternative='less')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(f\"The value {0.85} is significantly greater than the mean of f_model2.\")\n",
    "else:\n",
    "    print(f\"The value {0.85} is not significantly greater than the mean of f_model2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a444240-5332-4a99-b8c5-9d320abcdce2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a14893-7461-470c-a1cd-6e6908208dff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Comparing Walls Vs No Walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c7dad-7f57-45f8-bbda-dfebc7a71727",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502eccd6-8ff2-4c10-948c-16eb8caa6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_sets[0].copy()\n",
    "test_dataset = o_test_dataset.copy()\n",
    "neurons = o_neurons\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY']]\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "direct_test_dataset = test_dataset[test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "indirect_test_dataset = test_dataset[~test_dataset['trial_id'].isin(trials_all_direct)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6dcf8-93e3-4dc8-a254-dfd242379124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the decoder\n",
    "decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "\n",
    "#Updates the neurons\n",
    "observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, direct_test_dataset, neurons, 11.4, metric='difference')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(direct_test_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(direct_test_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae371d-e1e4-4411-8a0a-0ae54f908bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the decoder\n",
    "decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "\n",
    "#Updates the neurons\n",
    "observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, indirect_test_dataset, neurons, 11.4, metric='difference')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(indirect_test_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(indirect_test_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c62721-b849-4855-baf4-35ba54f2a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(train_dataset, 35, panda=True) \n",
    "firing_rate(direct_test_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "print('All neurons have been fitted')\n",
    "\n",
    "\n",
    "#Decoding Moment\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "pred_vel, max_prob = SNB_apply(direct_test_dataset, decoder)\n",
    "print('Decoder Applied')\n",
    "\n",
    "#Column Management\n",
    "apply_2D_data(direct_test_dataset, pred_vel)\n",
    "apply_2D_data(direct_test_dataset, np.array(direct_test_dataset['hand_vel']), col_name='vel')\n",
    "print('Column Management Done')\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(direct_test_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29914c8-b89e-4ba7-81c3-1cb63fe58c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(train_dataset, 35, panda=True) \n",
    "firing_rate(indirect_test_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "print('All neurons have been fitted')\n",
    "\n",
    "\n",
    "#Decoding Moment\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "pred_vel, max_prob = SNB_apply(indirect_test_dataset, decoder)\n",
    "print('Decoder Applied')\n",
    "\n",
    "#Column Management\n",
    "apply_2D_data(indirect_test_dataset, pred_vel)\n",
    "apply_2D_data(indirect_test_dataset, np.array(indirect_test_dataset['hand_vel']), col_name='vel')\n",
    "print('Column Management Done')\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(indirect_test_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16579d6e-bd96-4754-ab97-f97879d7520c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Official Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b65ef0-b532-421e-b2d2-1a95941ee51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Population Vector\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for population vector\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "\n",
    "    #Build the decoder\n",
    "    decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "   \n",
    "    #Updates the neurons\n",
    "    observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "    decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, test_dataset, neurons, 11.4, metric='difference')\n",
    "    \n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(test_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "\n",
    "    #Build the decoder\n",
    "    decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "   \n",
    "    #Updates the neurons\n",
    "    observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "    decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, test_dataset, neurons, 11.4, metric='difference')\n",
    "    \n",
    "    #Predicts every angle according to a decoder\n",
    "    angle_predictor(test_dataset, decoder, 31, neurons, pred_pos=False)\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters = np.array(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fd78b-8f4e-4692-9476-703c080f208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Simple Movement (No Walls)', 'Complex Movement (Walls)']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Population Vector - Wall Presence Analysis'\n",
    "label_name='Models'\n",
    "save=True\n",
    "save_name='ppv_walls_compare_r2.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters[0:10]\n",
    "model_2_parameters = parameters[10:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(6, 6), layout='constrained')\n",
    "colors = ['tomato', 'dodgerblue', 'orange', 'dodgerblue', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Simple Movements: {round(m_mean[0],2)}')\n",
    "plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "             label=f'Mean {eval} for Complex Movements: {round(m_mean[1],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names))\n",
    "plt.ylim(m_mean[0]-0.01, m_mean[1]+0.01)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef35e43-2359-4e13-93f1-1cbf011caa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model2, f_model1, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model2 is significantly greater than the mean of f_model1.\")\n",
    "else:\n",
    "    print(\"The mean of f_model2 is not significantly greater than the mean of f_model1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe7c58-4e1c-4ea5-951a-3d67c031db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Naïve Bayes\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "firing_rate(o_test_dataset, 35, panda=True)\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters2 = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters2.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters2.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters2 = np.array(parameters2)\n",
    "all_entries = np.array(all_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945987e-c9e4-4c2b-ae62-7c820b70e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Simple Movement (No Walls)', 'Complex Movement (Walls)']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Naïve Bayes - Wall Presence Analysis'\n",
    "label_name='Models'\n",
    "save=False\n",
    "save_name='nb_walls_compare_r2.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters[0:10]\n",
    "model_2_parameters = parameters[10:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(6, 6), layout='constrained')\n",
    "colors = ['tomato', 'dodgerblue', 'orange', 'dodgerblue', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Simple Movements: {round(m_mean[0],2)}')\n",
    "plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "             label=f'Mean {eval} for Complex Movements: {round(m_mean[1],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names))\n",
    "plt.ylim(m_mean[0]-0.015, m_mean[1]+0.015)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36596e6e-ff6f-4ccb-bd67-b9a3c56c7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model2, f_model1, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model2 is significantly greater than the mean of f_model1.\")\n",
    "else:\n",
    "    print(\"The mean of f_model2 is not significantly greater than the mean of f_model1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebecba-b070-4d4a-adc7-77763b255d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Naïve Bayes\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "firing_rate(o_test_dataset, 35, panda=True)\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters3 = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters3.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters3.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters3.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, 35, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters3.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters3 = np.array(parameters3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3870df-a925-4624-ab24-571f2e354353",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Simple Movement (No Walls)', 'Simple Movement (No Walls) Modified', 'Complex Movement (Walls)', 'Complex Movement (Walls) Modified']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Naïve Bayes - Wall Presence Analysis'\n",
    "label_name='Models'\n",
    "save=True\n",
    "save_name='nb_walls_compare_r2_prior.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters3[0:10]\n",
    "model_2_parameters = parameters3[10:20]\n",
    "model_3_parameters = parameters3[20:30]\n",
    "model_4_parameters = parameters3[30:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3]), np.std(model_3_parameters[:,0,3]), np.std(model_4_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3]), np.std(model_3_parameters[:,1,3]), np.std(model_4_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2]), np.mean(model_3_parameters[:,0,2]), np.mean(model_4_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2]), np.mean(model_3_parameters[:,1,2]), np.mean(model_4_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "    f_model3 = np.mean((model_3_parameters[:,0,2], model_3_parameters[:,1,2]), axis=0)\n",
    "    f_model4 = np.mean((model_4_parameters[:,0,2], model_4_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(10, 10), layout='constrained')\n",
    "colors = ['tomato', 'orange', 'dodgerblue', 'darkturquoise', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "    plt.scatter(2, f_model3[i], color=colors[2], marker='x', alpha=0.5)\n",
    "    plt.scatter(3, f_model4[i], color=colors[3], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Simple Movements: {round(m_mean[0],2)}')\n",
    "plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "             label=f'Mean {eval} for Simple Movements Modified: {round(m_mean[1],2)}')\n",
    "plt.errorbar(2, m_mean[2], error[2], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[2],\n",
    "             label=f'Mean {eval} for Complex Movements: {round(m_mean[2],2)}')\n",
    "plt.errorbar(3, m_mean[3], error[3], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[3],\n",
    "             label=f'Mean {eval} for Complex Movements Modified: {round(m_mean[3],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names), fontsize=8)\n",
    "plt.ylim(m_mean[1]-0.015, m_mean[2]+0.015)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 3.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5a390-0c05-494d-a2f7-09837066d6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c01198-b502-4093-b653-922d0212852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Naïve Bayes\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "window_size = 10\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "firing_rate(o_test_dataset, window_size, panda=True)\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters4 = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters4.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters4.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters4.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters4.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters4 = np.array(parameters4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152e8f6-630e-4d7b-ac24-b7989a4df747",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Simple Movement (No Walls)', 'Simple Movement (No Walls) Modified', 'Complex Movement (Walls)', 'Complex Movement (Walls) Modified']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Naïve Bayes - Wall Presence Analysis\\nWindow Size of 10 bins'\n",
    "label_name='Models'\n",
    "save=False\n",
    "save_name='nb_walls_compare_r2_window10.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters4[0:10]\n",
    "model_2_parameters = parameters4[10:20]\n",
    "model_3_parameters = parameters4[20:30]\n",
    "model_4_parameters = parameters4[30:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3]), np.std(model_3_parameters[:,0,3]), np.std(model_4_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3]), np.std(model_3_parameters[:,1,3]), np.std(model_4_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2]), np.mean(model_3_parameters[:,0,2]), np.mean(model_4_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2]), np.mean(model_3_parameters[:,1,2]), np.mean(model_4_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "    f_model3 = np.mean((model_3_parameters[:,0,2], model_3_parameters[:,1,2]), axis=0)\n",
    "    f_model4 = np.mean((model_4_parameters[:,0,2], model_4_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(10, 10), layout='constrained')\n",
    "colors = ['tomato', 'orange', 'dodgerblue', 'darkturquoise', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    #plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model3[i], color=colors[2], marker='x', alpha=0.5)\n",
    "    #plt.scatter(3, f_model4[i], color=colors[3], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Simple Movements: {round(m_mean[0],2)}')\n",
    "#plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "#             label=f'Mean {eval} for Simple Movements Modified: {round(m_mean[1],2)}')\n",
    "plt.errorbar(1, m_mean[2], error[2], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[2],\n",
    "             label=f'Mean {eval} for Complex Movements: {round(m_mean[2],2)}')\n",
    "#plt.errorbar(3, m_mean[3], error[3], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[3],\n",
    "#             label=f'Mean {eval} for Complex Movements Modified: {round(m_mean[3],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names), fontsize=8)\n",
    "plt.ylim(m_mean[0]-0.05, m_mean[2]+0.05)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c28d1-46a7-4b3b-82c7-20db4a3fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model1, f_model2, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model1 is significantly greater than the mean of f_model2.\")\n",
    "else:\n",
    "    print(\"The mean of f_model1 is not significantly greater than the mean of f_model2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba9e60-c5e5-4e45-9f4e-15c54bd77e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model3, f_model4, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model3 is significantly greater than the mean of f_model4.\")\n",
    "else:\n",
    "    print(\"The mean of f_model3 is not significantly greater than the mean of f_model4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146610c9-be81-4583-a672-0d372a50507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model3, f_model1, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model3 is significantly greater than the mean of f_model1.\")\n",
    "else:\n",
    "    print(\"The mean of f_model3 is not significantly greater than the mean of f_model1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023b4f2-95ab-4903-8f84-f6841e8bb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Naïve Bayes\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 10\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "window_size = 3\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "firing_rate(o_test_dataset, window_size, panda=True)\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Prepare Arrays\n",
    "parameters5 = []\n",
    "all_entries = []\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters5.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_direct_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters5.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = SNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters5.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test (With out P(s))---------------')\n",
    "for i in range(folds):\n",
    "    print()\n",
    "    print(f'Starting fold {i+1}')\n",
    "    train_dataset = train_sets[i].copy()\n",
    "    test_dataset = o_indirect_dataset.copy()\n",
    "    neurons = o_neurons\n",
    "    \n",
    "    #Converting sets to firing rate -> Otimization Needed!!!\n",
    "    firing_rate(train_dataset, window_size, panda=True) \n",
    "    print('Dataset is now in rates and not spike count')\n",
    "    \n",
    "    #Fitting the Gaussian Tuning Curve\n",
    "    mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "    print('All neurons have been fitted')\n",
    "\n",
    "    #Decoding Moment\n",
    "    decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted)\n",
    "    pred_vel, max_prob = RNB_apply(test_dataset, decoder)\n",
    "    print('Decoder Applied')\n",
    "\n",
    "    #Column Management\n",
    "    apply_2D_data(test_dataset, pred_vel)\n",
    "    apply_2D_data(test_dataset, np.array(test_dataset['hand_vel']), col_name='vel')\n",
    "    print('Column Management Done')\n",
    "    \n",
    "    #Gets correlation values\n",
    "    data = []\n",
    "    entries = []\n",
    "    for metric in metrics:\n",
    "        mean, std, r2_mean, r2_std, entry = get_pearson(test_dataset, metric, extra_metrics=True)\n",
    "        data.append([mean, std, r2_mean, r2_std])\n",
    "        entries.append(entry)\n",
    "\n",
    "    data = np.array(data)\n",
    "    entries = np.array(entries)\n",
    "    parameters5.append(data)\n",
    "    all_entries.append(entries)\n",
    "    print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "    print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "parameters5 = np.array(parameters5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf82154-822d-4b87-a7e0-ab919f8c0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['Simple Movement (No Walls)', 'Simple Movement (No Walls) Modified', 'Complex Movement (Walls)', 'Complex Movement (Walls) Modified']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Naïve Bayes - Wall Presence Analysis\\nWindow Size of 3 bins'\n",
    "label_name='Models'\n",
    "save=True\n",
    "save_name='nb_walls_compare_r2_window3.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters5[0:10]\n",
    "model_2_parameters = parameters5[10:20]\n",
    "model_3_parameters = parameters5[20:30]\n",
    "model_4_parameters = parameters5[30:]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3]), np.std(model_3_parameters[:,0,3]), np.std(model_4_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3]), np.std(model_3_parameters[:,1,3]), np.std(model_4_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2]), np.mean(model_3_parameters[:,0,2]), np.mean(model_4_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2]), np.mean(model_3_parameters[:,1,2]), np.mean(model_4_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "    f_model3 = np.mean((model_3_parameters[:,0,2], model_3_parameters[:,1,2]), axis=0)\n",
    "    f_model4 = np.mean((model_4_parameters[:,0,2], model_4_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(10, 10), layout='constrained')\n",
    "colors = ['tomato', 'orange', 'dodgerblue', 'darkturquoise', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    #plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model3[i], color=colors[2], marker='x', alpha=0.5)\n",
    "    #plt.scatter(3, f_model4[i], color=colors[3], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Simple Movements: {round(m_mean[0],2)}')\n",
    "#plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "#             label=f'Mean {eval} for Simple Movements Modified: {round(m_mean[1],2)}')\n",
    "plt.errorbar(1, m_mean[2], error[2], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[2],\n",
    "             label=f'Mean {eval} for Complex Movements: {round(m_mean[2],2)}')\n",
    "#plt.errorbar(3, m_mean[3], error[3], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[3],\n",
    "#             label=f'Mean {eval} for Complex Movements Modified: {round(m_mean[3],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names), fontsize=8)\n",
    "plt.ylim(m_mean[0]-0.05, m_mean[2]+0.05)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "plt.grid(True)\n",
    "'''\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(10))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(4))  # 5 minor ticks between major ticks on y-axis\n",
    "'''\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7e407-814e-486b-8e48-e70d01c1fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(f_model3, f_model1, alternative='greater')\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < 0.05:  # assuming a significance level of 0.05\n",
    "    print(\"The mean of f_model3 is significantly greater than the mean of f_model1.\")\n",
    "else:\n",
    "    print(\"The mean of f_model3 is not significantly greater than the mean of f_model1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ef240-f731-491c-8ba1-72b818c15360",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters\n",
    "all_entries\n",
    "model_names = ['PPV (No Walls)', 'PPV (Walls)', 'NB (No Walls)', 'NB (Walls)']\n",
    "nr_folds = 10\n",
    "eval='R2'\n",
    "title_name='Wall Presence Analysis\\nBoth Models'\n",
    "label_name='Models'\n",
    "save=False\n",
    "save_name='both_models_walls_compare_r2.png'\n",
    "\n",
    "'''\n",
    "Compare final models\n",
    "'''\n",
    "\n",
    "# Data extraction --> Models\n",
    "model_1_parameters = parameters[0:10]\n",
    "model_2_parameters = parameters[10:]\n",
    "model_3_parameters = parameters3[0:10]\n",
    "model_4_parameters = parameters3[20:30]\n",
    "\n",
    "nr_trials = len(parameters)\n",
    "if(eval == 'Pearson'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,1]), np.std(model_2_parameters[:,0,1])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,1]), np.std(model_2_parameters[:,1,1])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,0]), np.mean(model_2_parameters[:,0,0])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,0]), np.mean(model_2_parameters[:,1,0])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,0], model_1_parameters[:,1,0]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,0], model_2_parameters[:,1,0]), axis=0)\n",
    "\n",
    "elif(eval == 'R2'):\n",
    "    x_std = np.array([np.std(model_1_parameters[:,0,3]), np.std(model_2_parameters[:,0,3]), np.std(model_3_parameters[:,0,3]), np.std(model_4_parameters[:,0,3])])\n",
    "    y_std = np.array([np.std(model_1_parameters[:,1,3]), np.std(model_2_parameters[:,1,3]), np.std(model_3_parameters[:,1,3]), np.std(model_4_parameters[:,1,3])])\n",
    "    x_mean = np.array([np.mean(model_1_parameters[:,0,2]), np.mean(model_2_parameters[:,0,2]), np.mean(model_3_parameters[:,0,2]), np.mean(model_4_parameters[:,0,2])])\n",
    "    y_mean = np.array([np.mean(model_1_parameters[:,1,2]), np.mean(model_2_parameters[:,1,2]), np.mean(model_3_parameters[:,1,2]), np.mean(model_4_parameters[:,1,2])])\n",
    "    f_model1 = np.mean((model_1_parameters[:,0,2], model_1_parameters[:,1,2]), axis=0)\n",
    "    f_model2 = np.mean((model_2_parameters[:,0,2], model_2_parameters[:,1,2]), axis=0)\n",
    "    f_model3 = np.mean((model_3_parameters[:,0,2], model_3_parameters[:,1,2]), axis=0)\n",
    "    f_model4 = np.mean((model_4_parameters[:,0,2], model_4_parameters[:,1,2]), axis=0)\n",
    "\n",
    "m_mean = np.mean((x_mean, y_mean), axis=0)\n",
    "m_std = np.mean((x_std, y_std), axis=0)\n",
    "\n",
    "# Extra Variables\n",
    "error = m_std / np.sqrt(nr_trials)\n",
    "\n",
    "#Begin Plotting\n",
    "plt.figure(figsize=(6, 6), layout='constrained')\n",
    "colors = ['tomato', 'orange', 'dodgerblue', 'darkturquoise', 'black']\n",
    "\n",
    "#Scatter plotting\n",
    "for i in range(nr_folds):\n",
    "    plt.scatter(0, f_model1[i], color=colors[0], marker='x', alpha=0.5)\n",
    "    plt.scatter(1, f_model2[i], color=colors[1], marker='x', alpha=0.5)\n",
    "    plt.scatter(2, f_model3[i], color=colors[2], marker='x', alpha=0.5)\n",
    "    plt.scatter(3, f_model4[i], color=colors[3], marker='x', alpha=0.5)\n",
    "\n",
    "#Error bae plotting\n",
    "plt.errorbar(0, m_mean[0], error[0], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[0], \n",
    "             label=f'Mean {eval} for Population Vector (Direct): {round(m_mean[0],2)}')\n",
    "plt.errorbar(1, m_mean[1], error[1], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[1],\n",
    "             label=f'Mean {eval} for Population Vector (Indirect): {round(m_mean[1],2)}')\n",
    "plt.errorbar(2, m_mean[2], error[2], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[2],\n",
    "             label=f'Mean {eval} for Naïve Bayes (Direct): {round(m_mean[2],2)}')\n",
    "plt.errorbar(3, m_mean[3], error[3], ls='none', elinewidth=3.5, capthick=3.5, capsize=10, color=colors[3],\n",
    "             label=f'Mean {eval} for Naïve Bayes (Indirect): {round(m_mean[3],2)}')\n",
    "'''\n",
    "#plt.errorbar(2, 0.85, error[1], ls='none', elinewidth=1, capthick=1, capsize=10, color=colors[2], label=f'Gold Standard {eval}: 0.85')\n",
    "plt.scatter(2, 0.85, color=colors[2], alpha=1, marker='x', linewidth=10, label=f'Gold Standard {eval}: 0.85')\n",
    "'''\n",
    "\n",
    "# Add labels, title, and custom x-axis tick labels for the left y-axis\n",
    "plt.ylabel(eval)\n",
    "plt.title(title_name)\n",
    "#ax1.set_xlabel(label_name)\n",
    "plt.xticks(np.arange(len(model_names)), labels=np.array(model_names))\n",
    "plt.ylim(m_mean[0]-0.05, m_mean[3]+0.05)\n",
    "#plt.ylim(0, 1)\n",
    "plt.xlim(-0.5, 3.5)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Customize the grid\n",
    "#plt.grid(True)\n",
    "plt.grid(which='major', color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(which='minor', color='k', linestyle='-', linewidth=0.5, alpha=0.25)\n",
    "\n",
    "# Enable minor ticks\n",
    "plt.gca().minorticks_on()\n",
    "\n",
    "# Customize the number of minor ticks\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(4))  # 4 minor ticks between major ticks on x-axis\n",
    "plt.gca().yaxis.set_minor_locator(AutoMinorLocator(2))  # 5 minor ticks between major ticks on y-axis\n",
    "\n",
    "# Save figure if required\n",
    "if save: plt.savefig(save_name)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64342aeb-060b-4d56-8339-cd995273cd74",
   "metadata": {},
   "source": [
    "## Predicetd Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034807a5-c954-4009-abcc-c7dcca17a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plots all trajectories separate (with trial duration in ms)\n",
    "'''\n",
    "def plot_allTrajectoriesS(dataset, original_dataset, nr_cols=4, save=False, save_name='plot.png'):\n",
    "    #Gets the hand positions (trua and pred)\n",
    "    cursor_pos = np.asarray(dataset['cursor_pos'], dtype='float64')\n",
    "    true_pos = np.asarray(dataset['hand_pos'], dtype='float64')\n",
    "    predicted_x = np.asarray(dataset['pred_X'], dtype='float64')\n",
    "    predicted_y = np.asarray(dataset['pred_Y'], dtype='float64')\n",
    "    true_x = true_pos[:,0]\n",
    "    true_y = true_pos[:,1]\n",
    "    cursor_x = cursor_pos[:,0]\n",
    "    cursor_y = cursor_pos[:,1]\n",
    "    \n",
    "    #Gets the trials and time\n",
    "    trial_ids = np.asarray(dataset['trial_id'], dtype='int64')\n",
    "    mask = np.concatenate(([True], trial_ids[1:] != trial_ids[:-1]))\n",
    "    split_indices = np.where(mask)[0]\n",
    "    align_time = np.asarray(dataset['align_time'])\n",
    "    \n",
    "    #Gets the target positions per trial\n",
    "    trials = np.unique(trial_ids)\n",
    "    target_pos = np.asarray(original_dataset.trial_info[['active_pos_x', 'active_pos_y']], dtype='int64')[trials]\n",
    "    barrier_pos = np.asarray(original_dataset.trial_info['barrier_pos'])[trials]\n",
    "    barrier_lengths = np.array([len(inner_array) for inner_array in barrier_pos])\n",
    "    \n",
    "    \n",
    "    #Splits all arrays acording to the trials\n",
    "    predicted_x = np.split(predicted_x, split_indices)[1:]\n",
    "    predicted_y = np.split(predicted_y, split_indices)[1:]\n",
    "    align_time = np.split(align_time, split_indices)[1:]\n",
    "    true_x = np.split(true_x, split_indices)[1:]\n",
    "    true_y = np.split(true_y, split_indices)[1:]\n",
    "    cursor_x = np.split(cursor_x, split_indices)[1:]\n",
    "    cursor_y = np.split(cursor_y, split_indices)[1:]\n",
    "    \n",
    "    nr_trials = len(trials)\n",
    "    #The ploting starts\n",
    "    if(nr_trials%nr_cols == 0): nr_rows = nr_trials//nr_cols\n",
    "    else: nr_rows = nr_trials//nr_cols + 1\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=nr_rows, ncols=nr_cols, figsize=(nr_cols*4, nr_rows*4.5))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    #Plotting\n",
    "    for i in range(nr_trials):\n",
    "        ax = axs[i]\n",
    "        x_coords = true_x[i]\n",
    "        y_coords = true_y[i]\n",
    "        x_pred = predicted_x[i]\n",
    "        y_pred = predicted_y[i]\n",
    "        x_cursor = cursor_x[i]\n",
    "        y_cursor = cursor_y[i]\n",
    "        target_x = target_pos[i,0]\n",
    "        target_y = target_pos[i,1]\n",
    "        duration = int(align_time[i][-1])/1000000000\n",
    "        distance = round(np.sqrt(np.power(x_pred[-1]-x_coords[-1],2) + np.power(y_pred[-1]-y_coords[-1],2)),3)\n",
    "        barriers = barrier_pos[i]\n",
    "    \n",
    "        #Gets the limit\n",
    "        x_lim = np.max([np.max(x_coords), np.max(x_pred), np.max(x_cursor), np.abs(np.min(x_coords)), np.abs(np.min(x_pred)), \n",
    "                        np.abs(np.min(x_cursor))])\n",
    "        y_lim = np.max([np.max(y_coords), np.max(y_pred), np.max(y_cursor), np.abs(np.min(y_coords)), np.abs(np.min(y_pred)), \n",
    "                        np.abs(np.min(y_cursor))])\n",
    "        lim_value = np.max([x_lim,y_lim])+15\n",
    "        \n",
    "        #Plots the barriers\n",
    "        for j in range(barrier_lengths[i]):\n",
    "            x, y, half_width, half_height = barrier_pos[i][j]\n",
    "            left = x - half_width\n",
    "            bottom = y - half_height\n",
    "            width = 2 * half_width\n",
    "            height = 2 * half_height\n",
    "            ax.add_patch(plt.Rectangle((left, bottom), width, height, facecolor='gray'))\n",
    "    \n",
    "        ax.plot(x_cursor, y_cursor, label='Cursor Path')\n",
    "        ax.plot(x_pred, y_pred, label='Predicted Path')\n",
    "        #ax.plot(x_coords, y_coords, label='True Path')\n",
    "        ax.scatter(target_x, target_y, label='Target Position', color='red')\n",
    "        ax.scatter(x_pred[0], y_pred[0], label='Start Point', color='black')\n",
    "        ax.set_title(f'Trial {trials[i]}\\nLasted {duration} seconds\\nDistance Final: {distance}')\n",
    "        ax.set_xlim(-lim_value, lim_value)\n",
    "        ax.set_ylim(-lim_value, lim_value)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    #plt.suptitle(f'Trial Trajectories', y=1, fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    if(save): plt.savefig(save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d07691-4be6-455d-9d7e-0c8f706260f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Naïve Bayes\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 1\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "firing_rate(o_test_dataset, 35, panda=True)\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Runs the 10 fold for Naive Bayes\n",
    "metrics = [['vel_X', 'pred_vel_X'], ['vel_Y', 'pred_vel_Y']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "print()\n",
    "train_dataset = train_sets[0].copy()\n",
    "direct_dataset = o_direct_dataset.copy()\n",
    "neurons = o_neurons\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(train_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "print('All neurons have been fitted')\n",
    "\n",
    "#Decoding Moment\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "pred_vel, max_prob = SNB_apply(direct_dataset, decoder)\n",
    "print('Decoder Applied')\n",
    "\n",
    "#Column Management\n",
    "apply_2D_data(direct_dataset, pred_vel)\n",
    "apply_2D_data(direct_dataset, np.array(direct_dataset['hand_vel']), col_name='vel')\n",
    "print('Column Management Done')\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(direct_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "\n",
    "print()\n",
    "train_dataset = train_sets[0].copy()\n",
    "indirect_dataset = o_indirect_dataset.copy()\n",
    "neurons = o_neurons\n",
    "\n",
    "#Converting sets to firing rate -> Otimization Needed!!!\n",
    "firing_rate(train_dataset, 35, panda=True) \n",
    "print('Dataset is now in rates and not spike count')\n",
    "\n",
    "#Fitting the Gaussian Tuning Curve\n",
    "mean_matrix, found_pairs, velocity_vector, grids, observed, predicted = tune_neurons3D(train_dataset, 'Gaussian', 15)\n",
    "print('All neurons have been fitted')\n",
    "\n",
    "#Decoding Moment\n",
    "decoder = SNB_decoder(train_dataset, found_pairs, velocity_vector, predicted, spike_range=28)\n",
    "pred_vel, max_prob = SNB_apply(indirect_dataset, decoder)\n",
    "print('Decoder Applied')\n",
    "\n",
    "#Column Management\n",
    "apply_2D_data(indirect_dataset, pred_vel)\n",
    "apply_2D_data(indirect_dataset, np.array(indirect_dataset['hand_vel']), col_name='vel')\n",
    "print('Column Management Done')\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(indirect_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6b545-04f5-47c4-b986-ab5da77f46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_vel2pos(direct_dataset)\n",
    "convert_vel2pos(indirect_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab3041-abba-4aa6-a896-58f7ce236e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(direct_dataset, standardDS, nr_cols=5, save=False, save_name='nb_direct_traj_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1640f-eaa3-40be-ada6-90f994c285f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = trials_present(direct_dataset)\n",
    "desired_trials = [227, 302, 560, 344, 486]\n",
    "\n",
    "desired_direct_dataset = direct_dataset[direct_dataset['trial_id'].isin(desired_trials)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9303b69-845d-4a14-a4be-f062e621d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(desired_direct_dataset, standardDS, nr_cols=5, save=False, save_name='direct_nb_traj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c1f88-5285-47a7-96a0-6970e0dc854b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(indirect_dataset, standardDS, nr_cols=5, save=False, save_name='nb_indirect_traj_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1849b1-1dcc-4fdd-8b00-2b43233a91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = trials_present(indirect_dataset)\n",
    "desired_trials = [233, 423, 381, 294, 159]\n",
    "\n",
    "desired_indirect_dataset = indirect_dataset[indirect_dataset['trial_id'].isin(desired_trials)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79210ec0-c35c-41bc-8282-9ac211c0efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(desired_indirect_dataset, standardDS, nr_cols=5, save=False, save_name='indirect_nb_traj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a1241-11df-492d-b5d8-6cab1cb9845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run 10 fold comparison model of No Walls Vs Walls Population Vector\n",
    "'''\n",
    "\n",
    "#Separates the dataset into 3 sets\n",
    "folds = 1\n",
    "train_sets, val_sets, o_test_dataset = generate_sets(standardDS, 0.1, folds) \n",
    "o_neurons = standard_neurons\n",
    "\n",
    "#Gets the test set with direct and indirect separation\n",
    "trials = trials_present(o_test_dataset)\n",
    "trials_all_direct = np.array(standardDS.trial_info[standardDS.trial_info['num_barriers'] == 0]['trial_id'])\n",
    "trials_test_direct = trials[np.isin(trials, trials_all_direct)]\n",
    "\n",
    "#Gets the separated test set\n",
    "o_direct_dataset = o_test_dataset[o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "o_indirect_dataset = o_test_dataset[~o_test_dataset['trial_id'].isin(trials_all_direct)].copy()\n",
    "\n",
    "print('Dataset is now divided')\n",
    "\n",
    "#Runs the 10 fold for population vector\n",
    "metrics = [['mx', 'pred_cosX'],['my', 'pred_cosY']]\n",
    "\n",
    "print()\n",
    "print('--------------Direct Test---------------')\n",
    "print()\n",
    "print(f'Starting fold {1}')\n",
    "train_dataset = train_sets[0].copy()\n",
    "direct_ppv_dataset = o_direct_dataset.copy()\n",
    "neurons = o_neurons\n",
    "\n",
    "#Build the decoder\n",
    "decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "\n",
    "#Updates the neurons\n",
    "observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, direct_ppv_dataset, neurons, 11.4, metric='difference')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(direct_ppv_dataset, decoder, 31, neurons, pred_pos=True)\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(direct_ppv_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')\n",
    "\n",
    "print()\n",
    "print('--------------Indirect Test---------------')\n",
    "print()\n",
    "print(f'Starting fold {1}')\n",
    "train_dataset = train_sets[0].copy()\n",
    "indirect_ppv_dataset = o_indirect_dataset.copy()\n",
    "neurons = o_neurons\n",
    "\n",
    "#Build the decoder\n",
    "decoder = angle_decoder(train_dataset, 31, neurons)\n",
    "\n",
    "#Updates the neurons\n",
    "observed_matrix, predicted_matrix,_ = tune_neurons(train_dataset, decoder, 345, neurons)\n",
    "decoder, neurons = remove_neurons(predicted_matrix, observed_matrix, indirect_ppv_dataset, neurons, 11.4, metric='difference')\n",
    "\n",
    "#Predicts every angle according to a decoder\n",
    "angle_predictor(indirect_ppv_dataset, decoder, 31, neurons, pred_pos=True)\n",
    "\n",
    "#Gets correlation values\n",
    "data = []\n",
    "entries = []\n",
    "for metric in metrics:\n",
    "    mean, std, r2_mean, r2_std, entry = get_pearson(indirect_ppv_dataset, metric, extra_metrics=True)\n",
    "    data.append([mean, std, r2_mean, r2_std])\n",
    "    entries.append(entry)\n",
    "\n",
    "data = np.array(data)\n",
    "entries = np.array(entries)\n",
    "print(f'X_Corr: {data[0,0]} ± {data[0,1]}\\nY_Corr: {data[1,0]} ± {data[1,1]}')\n",
    "print(f'X_R2: {data[0,2]} ± {data[0,3]}\\nY_R2: {data[1,2]} ± {data[1,3]}')\n",
    "\n",
    "os.system('afplay /System/Library/Sounds/Glass.aiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8be9c-5b3b-4020-957f-02958863b775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(direct_ppv_dataset, standardDS, nr_cols=5, save=False, save_name='ppv_direct_traj_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501fe60-fa2a-4b25-be4b-9d09a885a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = trials_present(direct_ppv_dataset)\n",
    "desired_trials = [4, 33, 72, 178, 459]\n",
    "\n",
    "desired_direct_dataset_ppv = direct_ppv_dataset[direct_ppv_dataset['trial_id'].isin(desired_trials)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0e50-b2cd-4ef4-9f80-3d70b63f4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(desired_direct_dataset_ppv, standardDS, nr_cols=5, save=True, save_name='direct_ppv_traj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f3b70-3ab7-4aa2-9372-84cde5f74322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(indirect_ppv_dataset, standardDS, nr_cols=5, save=False, save_name='ppv_indirect_traj_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c9d57-5945-4c1d-95c1-b427b416c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = trials_present(indirect_ppv_dataset)\n",
    "desired_trials = [1912, 1947, 2118, 2197, 2244]\n",
    "\n",
    "desired_indirect_dataset_ppv = indirect_ppv_dataset[indirect_ppv_dataset['trial_id'].isin(desired_trials)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a3c1ef-0228-4517-b30c-ee79fbe4d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_allTrajectoriesS(desired_indirect_dataset_ppv, standardDS, nr_cols=5, save=True, save_name='indirect_ppv_traj.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77421e4-bad9-4ba8-9d3f-01a1e1727c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
